<!DOCTYPE html><html class=bg-base-300 data-theme=winter data-theme-type=light lang=en><head><meta content=true name=astro-view-transitions-enabled><meta content=animate name=astro-view-transitions-fallback><script type=module src=/_astro/ClientRouter.astro_astro_type_script_index_0_lang.waUwC-PJ.js></script><script src=/_astro/vanilla.ChgcMPTJ.js data-astro-rerun></script><script type=module src=/_astro/PointerOnNavigation.astro_astro_type_script_index_0_lang.Dwfd1XXs.js></script><meta charset=utf-8><meta content="width=device-width,initial-scale=1" name=viewport><link href=/favicon.svg rel=icon type=image><meta content="Astro v5.13.7" name=generator><meta content=从零构建大语言模型第3章:本章讲解注意力机制 name=description><meta content="实现注意力机制 | LLM" property=og:title><meta content=从零构建大语言模型第3章:本章讲解注意力机制 property=og:description><meta content=https://tankimzeg.top/og/llm/ch3_attention-machine.png property=og:image><link href=https://tankimzeg.top/blog/llm/ch3_attention-machine/ rel=canonical><title>实现注意力机制 | LLM - TanKimzeg</title><script>!function(){const e="winter",t="dracula";!function(){const a=localStorage.getItem("theme"),m=window.matchMedia("(prefers-color-scheme: dark)").matches;let c;a?c=a:(c=m?t:e,localStorage.setItem("theme",c)),document.documentElement.setAttribute("data-theme",c);const o=c===t?"dark":"light";document.documentElement.setAttribute("data-theme-type",o),window.matchMedia("(prefers-color-scheme: dark)").addEventListener("change",(a=>{if(!localStorage.getItem("theme")){const m=a.matches?t:e;document.documentElement.setAttribute("data-theme",m);const c=a.matches?"dark":"light";document.documentElement.setAttribute("data-theme-type",c),localStorage.setItem("theme",m)}}))}()}()</script><link href=/_astro/about.BB1pbI9A.css rel=stylesheet><style>.modal-backdrop[data-astro-cid-zux26muy] button[data-astro-cid-zux26muy]{cursor:default;width:100%;height:100%;opacity:0}.card-body .btn-category,.card-body .btn-tag{text-decoration:none}</style></head><body class="flex flex-col min-h-screen" data-pagefind-body=true><nav class="transition-transform bg-base-100 ease-in-out shadow-lg transform w-full delay-300 duration-500 fixed md:hidden navbar px-2 text-center z-50" id=navbar><div class=navbar-start><label class="btn btn-circle bg-base-100 btn-md swap swap-rotate"><span class=sr-only>Toggle menu</span> <input type=checkbox id=menu-toggle> <svg height=1em width=1em data-icon=lucide:menu class="h-5 w-5 swap-off"><symbol id=ai:lucide:menu viewBox="0 0 24 24"><path d="M4 5h16M4 12h16M4 19h16" fill=none stroke=currentColor stroke-linecap=round stroke-linejoin=round stroke-width=2 /></symbol><use href=#ai:lucide:menu></use></svg> <svg height=1em width=1em data-icon=lucide:x class="h-5 w-5 swap-on"><symbol id=ai:lucide:x viewBox="0 0 24 24"><path d="M18 6L6 18M6 6l12 12" fill=none stroke=currentColor stroke-linecap=round stroke-linejoin=round stroke-width=2 /></symbol><use href=#ai:lucide:x></use></svg></label></div><div class=navbar-center><a href=/ class="transition-transform btn btn-ghost hover:scale-105 duration-300 text-xl">TanKimzeg&#39;s Blog</a></div><div class=navbar-end><button title="Theme Toggle" aria-label="Toggle theme" class="btn btn-circle hover:scale-110 bg-base-100 btn-md shadow-sm md:border-base-content/20 navbar-theme" data-astro-cid-mjqc4hpp data-theme-toggle id=theme-toggle-nx14pc8v7><svg height=1em width=1em data-icon=lucide:sun class="h-5 w-5 theme-toggle-icon sun-icon" data-astro-cid-mjqc4hpp=true><symbol id=ai:lucide:sun viewBox="0 0 24 24"><g fill=none stroke=currentColor stroke-linecap=round stroke-linejoin=round stroke-width=2><circle cx=12 cy=12 r=4 /><path d="M12 2v2m0 16v2M4.93 4.93l1.41 1.41m11.32 11.32l1.41 1.41M2 12h2m16 0h2M6.34 17.66l-1.41 1.41M19.07 4.93l-1.41 1.41"/></g></symbol><use href=#ai:lucide:sun></use></svg> <svg height=1em width=1em data-icon=lucide:moon class="h-5 w-5 theme-toggle-icon hidden moon-icon" data-astro-cid-mjqc4hpp=true><symbol id=ai:lucide:moon viewBox="0 0 24 24"><path d="M20.985 12.486a9 9 0 1 1-9.473-9.472c.405-.022.617.46.402.803a6 6 0 0 0 8.268 8.268c.344-.215.825-.004.803.401" fill=none stroke=currentColor stroke-linecap=round stroke-linejoin=round stroke-width=2 /></symbol><use href=#ai:lucide:moon></use></svg></button><script>!function(){const e="winter",t="dracula",n=(e,t)=>{if(!e)return;const n=e.querySelector(".sun-icon"),d=e.querySelector(".moon-icon");n&&d&&(t?(n.classList.remove("hidden"),d.classList.add("hidden")):(n.classList.add("hidden"),d.classList.remove("hidden")))};document.addEventListener("astro:page-load",(()=>{const d=document.getElementById("theme-toggle-nx14pc8v7");if(!d)return;const o=document.documentElement.getAttribute("data-theme");n(d,o===t),d.addEventListener("click",(()=>{const o=document.documentElement.getAttribute("data-theme")===e?t:e;d.classList.add("animate-spin-once"),document.documentElement.setAttribute("data-theme",o);const c=o===t?"dark":"light";document.documentElement.setAttribute("data-theme-type",c),localStorage.setItem("theme",o);document.querySelectorAll("[data-theme-toggle]").forEach((e=>{n(e,o===t)})),setTimeout((()=>{d.classList.remove("animate-spin-once")}),300)}))}))}()</script></div><div class="transition-transform bg-base-100 ease-in-out shadow-lg transform w-full -translate-x-full absolute duration-300 left-0 top-full" id=header-menu><ul class="w-full bg-transparent menu p-2"><li class="relative w-full group"><a href=/ class="flex items-center rounded-lg hover:bg-base-200 font-bold p-3 text-lg" target=_self><svg height=1em width=1em data-icon=material-symbols:home-outline-rounded class="h-5 w-5 mr-2"><symbol id=ai:material-symbols:home-outline-rounded viewBox="0 0 24 24"><path d="M6 19h3v-5q0-.425.288-.712T10 13h4q.425 0 .713.288T15 14v5h3v-9l-6-4.5L6 10zm-2 0v-9q0-.475.213-.9t.587-.7l6-4.5q.525-.4 1.2-.4t1.2.4l6 4.5q.375.275.588.7T20 10v9q0 .825-.588 1.413T18 21h-4q-.425 0-.712-.288T13 20v-5h-2v5q0 .425-.288.713T10 21H6q-.825 0-1.412-.587T4 19m8-6.75" fill=currentColor /></symbol><use href=#ai:material-symbols:home-outline-rounded></use></svg> <span>Home</span></a></li><li class="relative w-full group"><a href=/about class="flex items-center rounded-lg hover:bg-base-200 font-bold p-3 text-lg" target=_self><svg height=1em width=1em data-icon=material-symbols:info-outline-rounded class="h-5 w-5 mr-2"><symbol id=ai:material-symbols:info-outline-rounded viewBox="0 0 24 24"><path d="M12 17q.425 0 .713-.288T13 16v-4q0-.425-.288-.712T12 11t-.712.288T11 12v4q0 .425.288.713T12 17m0-8q.425 0 .713-.288T13 8t-.288-.712T12 7t-.712.288T11 8t.288.713T12 9m0 13q-2.075 0-3.9-.788t-3.175-2.137T2.788 15.9T2 12t.788-3.9t2.137-3.175T8.1 2.788T12 2t3.9.788t3.175 2.137T21.213 8.1T22 12t-.788 3.9t-2.137 3.175t-3.175 2.138T12 22m0-2q3.35 0 5.675-2.325T20 12t-2.325-5.675T12 4T6.325 6.325T4 12t2.325 5.675T12 20m0-8" fill=currentColor /></symbol><use href=#ai:material-symbols:info-outline-rounded></use></svg> <span>About</span></a></li><li class="relative w-full group"><details open><summary class="flex items-center rounded-lg hover:bg-base-200 font-bold p-3 text-lg"><svg height=1em width=1em data-icon=material-symbols:book-2-outline-rounded class="h-5 w-5 mr-2"><symbol id=ai:material-symbols:book-2-outline-rounded viewBox="0 0 24 24"><path d="M6 15.325q.35-.175.725-.25T7.5 15H8V4h-.5q-.625 0-1.062.438T6 5.5zM10 15h8V4h-8zm-4 .325V4zM7.5 22q-1.45 0-2.475-1.025T4 18.5v-13q0-1.45 1.025-2.475T7.5 2H18q.825 0 1.413.587T20 4v12.525q0 .2-.162.363t-.588.362q-.35.175-.55.5t-.2.75t.2.763t.55.487t.55.413t.2.562v.25q0 .425-.288.725T19 22zm0-2h9.325q-.15-.35-.237-.712T16.5 18.5q0-.4.075-.775t.25-.725H7.5q-.65 0-1.075.438T6 18.5q0 .65.425 1.075T7.5 20" fill=currentColor /></symbol><use href=#ai:material-symbols:book-2-outline-rounded></use></svg> <span>Blogs</span></summary><ul class=pl-4><li><a href=/blog class="flex items-center rounded-lg hover:bg-base-200 p-2" target=_self><svg height=1em width=1em data-icon=material-symbols:ink-pen-outline-rounded class="h-4 w-4 mr-2"><symbol id=ai:material-symbols:ink-pen-outline-rounded viewBox="0 0 24 24"><path d="m12.25 10.825l.925.925L18.6 6.325l-.925-.925zM5 19h.925l5.825-5.825l-.925-.925L5 18.075zm8.875-5.125l-3.75-3.75L14.3 5.95l-.725-.725L8.8 10q-.3.3-.7.3t-.7-.3t-.3-.712t.3-.713l4.75-4.75q.6-.6 1.413-.6t1.412.6l.725.725l1.25-1.25q.3-.3.713-.3t.712.3L20.7 5.625q.3.3.3.712t-.3.713zM4 21q-.425 0-.712-.288T3 20v-1.925q0-.4.15-.763t.425-.637l6.55-6.55l3.75 3.75l-6.55 6.55q-.275.275-.638.425t-.762.15z" fill=currentColor /></symbol><use href=#ai:material-symbols:ink-pen-outline-rounded></use></svg> <span>All blogs</span></a></li><li><a href=/blog/category/tech class="flex items-center rounded-lg hover:bg-base-200 p-2" target=_self><svg height=1em width=1em data-icon=material-symbols:deployed-code-outline class="h-4 w-4 mr-2"><symbol id=ai:material-symbols:deployed-code-outline viewBox="0 0 24 24"><path d="M11 19.425v-6.85L5 9.1v6.85zm2 0l6-3.475V9.1l-6 3.475zm-1-8.575l5.925-3.425L12 4L6.075 7.425zM4 17.7q-.475-.275-.737-.725t-.263-1v-7.95q0-.55.263-1T4 6.3l7-4.025Q11.475 2 12 2t1 .275L20 6.3q.475.275.738.725t.262 1v7.95q0 .55-.262 1T20 17.7l-7 4.025Q12.525 22 12 22t-1-.275zm8-5.7" fill=currentColor /></symbol><use href=#ai:material-symbols:deployed-code-outline></use></svg> <span>Tech Blogs</span></a></li><li><a href=/blog/category/life class="flex items-center rounded-lg hover:bg-base-200 p-2" target=_self><svg height=1em width=1em data-icon=material-symbols:earthquake-rounded class="h-4 w-4 mr-2"><symbol id=ai:material-symbols:earthquake-rounded viewBox="0 0 24 24"><path d="M9.025 22q-.35 0-.612-.187T8.05 21.3L5.5 13H3q-.425 0-.712-.288T2 12t.288-.712T3 11h3.25q.325 0 .588.188t.362.512l1.65 5.375L12.025 2.8q.075-.35.35-.575T13 2t.625.213t.35.562l2.175 9.4l1.4-4.475q.1-.325.362-.512T18.5 7t.575.175t.375.475L20.7 11h.3q.425 0 .713.288T22 12t-.288.713T21 13h-1q-.325 0-.575-.175t-.375-.475l-.475-1.275L16.95 16.3q-.1.325-.375.525T15.95 17t-.6-.238t-.325-.537L13 7.525l-3.025 13.7q-.075.35-.337.55T9.025 22" fill=currentColor /></symbol><use href=#ai:material-symbols:earthquake-rounded></use></svg> <span>Life Blogs</span></a></li></ul></details></li><li class="relative w-full group"><a href=/project class="flex items-center rounded-lg hover:bg-base-200 font-bold p-3 text-lg" target=_self><svg height=1em width=1em data-icon=material-symbols:code-blocks-outline class="h-5 w-5 mr-2"><symbol id=ai:material-symbols:code-blocks-outline viewBox="0 0 24 24"><path d="m9.6 15.6l1.4-1.425L8.825 12L11 9.825L9.6 8.4L6 12zm4.8 0L18 12l-3.6-3.6L13 9.825L15.175 12L13 14.175zM5 21q-.825 0-1.412-.587T3 19V5q0-.825.588-1.412T5 3h14q.825 0 1.413.588T21 5v14q0 .825-.587 1.413T19 21zm0-2h14V5H5zM5 5v14z" fill=currentColor /></symbol><use href=#ai:material-symbols:code-blocks-outline></use></svg> <span>Project</span></a></li><li class="relative w-full group"><a href=/friend class="flex items-center rounded-lg hover:bg-base-200 font-bold p-3 text-lg" target=_self><svg height=1em width=1em data-icon=material-symbols:supervisor-account-outline-rounded class="h-5 w-5 mr-2"><symbol id=ai:material-symbols:supervisor-account-outline-rounded viewBox="0 0 24 24"><path d="M17 15q-1.05 0-1.775-.725T14.5 12.5t.725-1.775T17 10t1.775.725t.725 1.775t-.725 1.775T17 15m-5 4v-.4q0-.6.313-1.112t.887-.738q.9-.375 1.863-.562T17 16t1.938.188t1.862.562q.575.225.888.738T22 18.6v.4q0 .425-.288.713T21 20h-8q-.425 0-.712-.288T12 19m-2-7q-1.65 0-2.825-1.175T6 8t1.175-2.825T10 4t2.825 1.175T14 8t-1.175 2.825T10 12m-8 5.2q0-.85.425-1.562T3.6 14.55q1.5-.75 3.113-1.15T10 13q.875 0 1.75.15t1.75.35l-.85.85l-.85.85q-.45-.125-.9-.162T10 15q-1.45 0-2.838.35t-2.662 1q-.25.125-.375.35T4 17.2v.8h6v.975q0 .325.125.588t.35.437H4q-.825 0-1.412-.587T2 18zm8-7.2q.825 0 1.413-.587T12 8t-.587-1.412T10 6t-1.412.588T8 8t.588 1.413T10 10" fill=currentColor /></symbol><use href=#ai:material-symbols:supervisor-account-outline-rounded></use></svg> <span>Friend</span></a></li></ul></div></nav><script type=module>document.addEventListener("astro:page-load",(()=>{let e=window.scrollY;const t=document.getElementById("navbar"),s=document.getElementById("header-menu"),a=document.getElementById("menu-toggle");if(!t||!s||!a)return;a.addEventListener("change",(()=>{a.checked?(s.classList.remove("-translate-x-full"),s.classList.add("translate-x-0")):(s.classList.add("-translate-x-full"),s.classList.remove("translate-x-0"))}));let l=!1;window.addEventListener("scroll",(()=>{l||(window.requestAnimationFrame((()=>{!t||!s||!a||(window.scrollY>e&&window.scrollY>50?(t.classList.add("-translate-y-full","duration-500"),t.classList.remove("translate-y-0"),a.checked&&(a.checked=!1,s.classList.add("-translate-x-full"),s.classList.remove("translate-x-0"))):window.scrollY<e&&(t.classList.remove("-translate-y-full"),t.classList.add("translate-y-0","duration-300")),e=window.scrollY),l=!1})),l=!0)})),document.addEventListener("click",(e=>{if(!t||!s||!a)return;const l=e.target;!t.contains(l)&&s.classList.contains("translate-x-0")&&(a.checked=!1,s.classList.add("-translate-x-full"),s.classList.remove("translate-x-0"))})),document.querySelectorAll("#header-menu a").forEach((e=>{e.addEventListener("click",(()=>{!s||!a||a.checked&&(a.checked=!1,s.classList.add("-translate-x-full"),s.classList.remove("translate-x-0"))}))}))}))</script><div class="w-full flex-grow max-w-6xl mx-auto"><div class="gap-4 grid lg:grid-cols-4 grid-cols-1 h-full md:grid-cols-5 p-4"><main class="flex flex-col gap-4 bg-transparent col-span-1 lg:col-span-3 md:col-span-4 md:mt-0 md:order-2 mt-16 order-1"><div class="flex flex-col gap-4 flex-grow"><div class="w-full bg-base-100 shadow-lg rounded-xl overflow-hidden"><div class="pt-6 px-6"><h1 class="lg:text-4xl font-bold sm:text-2xl text-xl">实现注意力机制 | LLM</h1><p class="text-sm mt-2 text-base-content/80">从零构建大语言模型第3章:本章讲解注意力机制</p></div><div class="sm:p-6 p-4"><div class="flex flex-col gap-y-2 mb-4 opacity-75 sm:flex-row sm:justify-between sm:text-sm text-xs"><div class="flex items-center flex-wrap gap-x-4 gap-y-2"><span class="flex items-center gap-1"><svg height=1em width=1em data-icon=lucide:calendar class="h-4 w-4 flex-shrink-0"><symbol id=ai:lucide:calendar viewBox="0 0 24 24"><g fill=none stroke=currentColor stroke-linecap=round stroke-linejoin=round stroke-width=2><path d="M8 2v4m8-4v4"/><rect height=18 rx=2 width=18 x=3 y=4 /><path d="M3 10h18"/></g></symbol><use href=#ai:lucide:calendar></use></svg> <span class=truncate>Mon Sep 22 2025</span></span></div><div class="flex items-center gap-1 flex-wrap"><svg height=1em width=1em data-icon=lucide:book-open class="h-4 w-4 flex-shrink-0"><symbol id=ai:lucide:book-open viewBox="0 0 24 24"><path d="M12 7v14m-9-3a1 1 0 0 1-1-1V4a1 1 0 0 1 1-1h5a4 4 0 0 1 4 4a4 4 0 0 1 4-4h5a1 1 0 0 1 1 1v13a1 1 0 0 1-1 1h-6a3 3 0 0 0-3 3a3 3 0 0 0-3-3z" fill=none stroke=currentColor stroke-linecap=round stroke-linejoin=round stroke-width=2 /></symbol><use href=#ai:lucide:book-open></use></svg> <span class=truncate>3182 words · 12 minutes</span></div></div><div class=mb-6><div class="flex items-center flex-wrap gap-2"><a href=/blog/category/tech class="btn btn-xs btn-category"><svg height=1em width=1em data-icon=lucide:folder class="h-4 w-4" viewBox="0 0 24 24"><use href=#ai:lucide:folder></use></svg> <span>tech</span> </a><a href=/blog/tag/llm class="btn btn-xs btn-tag"><svg height=1em width=1em data-icon=lucide:tag class="h-4 w-4" viewBox="0 0 24 24"><use href=#ai:lucide:tag></use></svg> <span>llm</span></a></div></div><div class=mt-8><div class="max-w-none prose prose-headings:scroll-mt-20 prose-img:mx-auto prose-img:rounded-xl" id=content><p><img alt="" src=https://skindhu.github.io/Build-A-Large-Language-Model-CN/Image/chapter3/figure3.1.png></p><p>本章我们将实现四种不同的注意力机制.</p><p><img alt="" src=https://skindhu.github.io/Build-A-Large-Language-Model-CN/Image/chapter3/figure3.2.png></p><h2 id=长序列建模的问题>长序列建模的问题</h2><h3 id=循环神经网络rnn>循环神经网络(RNN)</h3><p>在编码器-解码器架构的 RNN 网络中，输入文本被输入到编码器中，编码器按顺序处理文本内容。在每个步骤中，编码器会更新其隐状态（即隐藏层的内部值），试图在最终的隐状态中捕捉整个输入句子的含义，如图所示。随后，解码器使用该最终隐状态来开始逐词生成翻译句子。解码器在每一步也会更新其隐状态，用于携带生成下一个词所需的上下文信息。</p><p><img alt="" src=https://skindhu.github.io/Build-A-Large-Language-Model-CN/Image/chapter3/figure3.4.png></p><p>这里的关键思想在于，编码器部分将整个输入文本处理为==一个==隐藏状态（记忆单元）。解码器随后使用该隐藏状态生成输出.可以将这个隐藏状态视为一个嵌入向量.</p><p>编码器-解码器架构的 RNN 的一个重大问题和限制在于，<strong>在解码阶段 RNN 无法直接访问编码器的早期隐藏状态</strong>。因此，它只能依赖当前隐藏状态来封装所有相关信息。这种设计可能导致上下文信息的丢失，特别是在依赖关系较长的复杂句子中，这一问题尤为突出。</p><blockquote><p>RNN用最后一个隐藏层的信息进入解码器,它聚合了以前的所有编码信息,所有可能导致上下文信息的丢失,这点很好理解.</p></blockquote><p>正是这一缺点促成了注意力机制的设计.</p><h3 id=注意力机制的解决方法>注意力机制的解决方法</h3><p>它的关键思想是<strong>在处理每个词时，不仅依赖于最后的隐藏状态，而是允许模型直接关注序列中的所有词</strong>。这样，即使是较远的词也能在模型计算当前词的语义时直接参与。</p><h2 id=通过注意力机制捕捉数据依赖关系>通过注意力机制捕捉数据依赖关系</h2><p><img alt="" src=https://skindhu.github.io/Build-A-Large-Language-Model-CN/Image/chapter3/figure3.6.png></p><h2 id=通过自注意力机制关注输入的不同部分>通过自注意力机制关注输入的不同部分</h2><h3 id=一种不含可训练权重的简化自注意力机制>一种不含可训练权重的简化自注意力机制</h3><p><img alt="" src=https://skindhu.github.io/Build-A-Large-Language-Model-CN/Image/chapter3/figure3.7.png></p><p>在自注意力机制中，上下文向量起着关键作用。它们的目的是通过整合序列中所有其他元素的信息（如同一个句子中的其他词），为输入序列中的每个元素创建丰富的表示.</p><p><img alt="" src=https://skindhu.github.io/Build-A-Large-Language-Model-CN/Image/chapter3/figure3.8.png></p><p>实现自注意力机制的第一步是计算中间值 <strong>ω</strong>，即注意力得分.</p><p>每个输入token会先通过权重矩阵<span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mi>W</mi></mrow><annotation encoding=application/x-tex>W</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:.6833em></span><span class="mord mathnormal" style=margin-right:.13889em>W</span></span></span></span> 分别计算出它的Q,K,V三个向量:</p><ul><li><strong>Q向量（查询向量）</strong>：查询向量代表了这个词在寻找相关信息时提出的问题</li><li><strong>K向量（键向量）</strong>：键向量代表了一个单词的特征，或者说是这个单词如何”展示”自己，以便其它单词可以与它进行匹配</li><li><strong>V向量（值向量）</strong>：值向量携带的是这个单词的具体信息，也就是当一个单词被”注意到”时，它提供给关注者的内容</li></ul><p>具体生成Q、K、V向量的方式主要通过线性变换：</p><div class=expressive-code><link href=/_astro/ec.oj3b7.css rel=stylesheet><script type=module src=/_astro/ec.p1z7b.js></script><figure class=frame><figcaption class=header></figcaption><pre data-language=plaintext><code><div class=ec-line><div class=gutter><div class=ln aria-hidden=true>1</div></div><div class=code><span style=--0:#e1e4e8>Q1 = W_Q * (E1 + Pos1)</span></div></div><div class=ec-line><div class=gutter><div class=ln aria-hidden=true>2</div></div><div class=code><span style=--0:#e1e4e8>K1 = W_K * (E1 + Pos1)</span></div></div><div class=ec-line><div class=gutter><div class=ln aria-hidden=true>3</div></div><div class=code><span style=--0:#e1e4e8>V1 = W_V * (E1 + Pos1)</span></div></div></code></pre><div class=copy><button title="Copy to clipboard" data-code="Q1 = W_Q * (E1 + Pos1)K1 = W_K * (E1 + Pos1)V1 = W_V * (E1 + Pos1)" data-copied=Copied!><div></div></button></div></figure></div><p>依次类推，为所有token生成<code>Q</code>，<code>K</code>，<code>V</code>向量，其中<code>W_Q</code>，<code>W_K</code>和<code>W_V</code>是Transformer训练出的权重（每一层不同）</p><p>针对每一个目标token，Transformer会计算它的 <code>Q向量</code> 与其它所有的token的 <code>K向量</code> 的==点积==，以确定每个词对当前词的重要性（即注意力分数）</p><blockquote><p>为什么是K与Q呢?V做什么?</p></blockquote><p>接下来，我们对先前计算的每个注意力分数进行归一化.</p><p><img alt="" src=https://skindhu.github.io/Build-A-Large-Language-Model-CN/Image/chapter3/figure3.9.png></p><blockquote><p>我知道,点积可以衡量相似度,但最好先标准化(?我忘记准确的名词了,就是使其长度为1,上过高中都知道).这里算出来诸多注意力分数后再归一化.</p></blockquote><p>归一化采用的是<code>softmax</code>函数:</p><p><span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mi>s</mi><mi>o</mi><mi>f</mi><mi>t</mi><mi>m</mi><mi>a</mi><mi>x</mi><mo stretchy=false>(</mo><msub><mi>z</mi><mi>i</mi></msub><mo stretchy=false>)</mo><mo>=</mo><mfrac><msup><mi>e</mi><msub><mi>z</mi><mi>i</mi></msub></msup><mrow><msub><mo>∑</mo><mi>j</mi></msub><msup><mi>e</mi><msub><mi>z</mi><mi>j</mi></msub></msup></mrow></mfrac></mrow><annotation encoding=application/x-tex>softmax(z_{i}) = \frac{e^{z_{i}}}{\sum_{j}e^{z_{j}}}</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:1em;vertical-align:-.25em></span><span class="mord mathnormal">so</span><span class="mord mathnormal" style=margin-right:.10764em>f</span><span class="mord mathnormal">t</span><span class="mord mathnormal">ma</span><span class="mord mathnormal">x</span><span class=mopen>(</span><span class=mord><span class="mord mathnormal" style=margin-right:.04398em>z</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:.3117em><span style=top:-2.55em;margin-left:-.044em;margin-right:.05em><span class=pstrut style=height:2.7em></span><span class="mtight sizing reset-size6 size3"><span class="mtight mord"><span class="mtight mord mathnormal">i</span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:.15em><span></span></span></span></span></span></span><span class=mclose>)</span><span class=mspace style=margin-right:.2778em></span><span class=mrel>=</span><span class=mspace style=margin-right:.2778em></span></span><span class=base><span class=strut style=height:1.5885em;vertical-align:-.6775em></span><span class=mord><span class="mopen nulldelimiter"></span><span class=mfrac><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:.911em><span style=top:-2.6447em><span class=pstrut style=height:3em></span><span class="mtight sizing reset-size6 size3"><span class="mtight mord"><span class="mtight mop"><span class="mtight mop op-symbol small-op" style=position:relative;top:0>∑</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:.1496em><span style=top:-2.1786em;margin-left:0;margin-right:.0714em><span class=pstrut style=height:2.5em></span><span class="mtight sizing reset-size3 size1"><span class="mtight mord"><span class="mtight mord mathnormal" style=margin-right:.05724em>j</span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:.4603em><span></span></span></span></span></span></span><span class="mtight mspace" style=margin-right:.1952em></span><span class="mtight mord"><span class="mtight mord mathnormal">e</span><span class=msupsub><span class=vlist-t><span class=vlist-r><span class=vlist style=height:.779em><span style=top:-2.9714em;margin-right:.0714em><span class=pstrut style=height:2.5em></span><span class="mtight sizing reset-size3 size1"><span class="mtight mord"><span class="mtight mord"><span class="mtight mord mathnormal" style=margin-right:.04398em>z</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:.3448em><span style=top:-2.3448em;margin-left:-.044em;margin-right:.1em><span class=pstrut style=height:2.6595em></span><span class="mtight mord"><span class="mtight mord mathnormal" style=margin-right:.05724em>j</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:.5092em><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span style=top:-3.23em><span class=pstrut style=height:3em></span><span class=frac-line style=border-bottom-width:.04em></span></span><span style=top:-3.394em><span class=pstrut style=height:3em></span><span class="mtight sizing reset-size6 size3"><span class="mtight mord"><span class="mtight mord"><span class="mtight mord mathnormal">e</span><span class=msupsub><span class=vlist-t><span class=vlist-r><span class=vlist style=height:.7385em><span style=top:-2.931em;margin-right:.0714em><span class=pstrut style=height:2.5em></span><span class="mtight sizing reset-size3 size1"><span class="mtight mord"><span class="mtight mord"><span class="mtight mord mathnormal" style=margin-right:.04398em>z</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:.3448em><span style=top:-2.3448em;margin-left:-.044em;margin-right:.1em><span class=pstrut style=height:2.6595em></span><span class="mtight mord"><span class="mtight mord mathnormal">i</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:.3147em><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:.6775em><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></p><blockquote><p><code>softmax</code>是神经网络常用的激活函数,有一些优点</p></blockquote><p>最后,计算上下文向量的方法是每个输入向量与对应的注意力权重的加权和.</p><h3 id=为所有输入的token计算注意力权重>为所有输入的token计算注意力权重</h3><div class=expressive-code><figure class=frame><figcaption class=header></figcaption><pre data-language=python><code><div class=ec-line><div class=gutter><div class=ln aria-hidden=true>1</div></div><div class=code><span style=--0:#E1E4E8>attn_scores </span><span style=--0:#F97583>=</span><span style=--0:#E1E4E8> inputs </span><span style=--0:#F97583>@</span><span style=--0:#E1E4E8> inputs.T</span></div></div><div class=ec-line><div class=gutter><div class=ln aria-hidden=true>2</div></div><div class=code><span style=--0:#79B8FF>print</span><span style=--0:#E1E4E8>(attn_scores)</span></div></div></code></pre><div class=copy><button title="Copy to clipboard" data-code="attn_scores = inputs @ inputs.Tprint(attn_scores)" data-copied=Copied!><div></div></button></div></figure></div><p><code>inputs</code>是[token_num, embedding_dim],结果的形状是[token_num, token_num].即所有输入对的注意力得分.</p><p>进一步计算注意力权重:</p><div class=expressive-code><figure class=frame><figcaption class=header></figcaption><pre data-language=python><code><div class=ec-line><div class=gutter><div class=ln aria-hidden=true>1</div></div><div class=code><span style=--0:#E1E4E8>attn_weights </span><span style=--0:#F97583>=</span><span style=--0:#E1E4E8> torch.softmax(attn_scores, </span><span style=--0:#FFAB70>dim</span><span style=--0:#F97583>=-</span><span style=--0:#79B8FF>1</span><span style=--0:#E1E4E8>)</span></div></div><div class=ec-line><div class=gutter><div class=ln aria-hidden=true>2</div></div><div class=code><span style=--0:#79B8FF>print</span><span style=--0:#E1E4E8>(attn_weights)</span></div></div></code></pre><div class=copy><button title="Copy to clipboard" data-code="attn_weights = torch.softmax(attn_scores, dim=-1)print(attn_weights)" data-copied=Copied!><div></div></button></div></figure></div><p>在使用 PyTorch 时，像 <code>torch.softmax</code> 这样的函数中的 <code>dim</code> 参数指定了将在输入张量中的哪个维度上进行归一化计算。 <code>dim=-1</code>表示表示沿着最后一个维度进行归一化操作.</p><h2 id=实现带有可训练权重的自注意力机制>实现带有可训练权重的自注意力机制</h2><p><img alt="" src=https://skindhu.github.io/Build-A-Large-Language-Model-CN/Image/chapter3/figure3.13.png></p><blockquote><p>我发现按以上方法没有任何可训练权重.原始Transformer架构通过设置可训练的<code>W_Q</code>/<code>W_K</code>/<code>W_V</code>矩阵来实现这一点.</p></blockquote><p>这种自注意力机制也被称为放缩点积注意力.</p><h3 id=逐步计算注意力权重>逐步计算注意力权重</h3><p>我们通过引入三个可训练的权重矩阵：Wq、Wk 和 Wv 来逐步实现自注意力机制。这三个矩阵用于将嵌入后的输入 token <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><msup><mi>x</mi><mrow><mo stretchy=false>(</mo><mi>i</mi><mo stretchy=false>)</mo></mrow></msup></mrow><annotation encoding=application/x-tex>x^{(i)}</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:.888em></span><span class=mord><span class="mord mathnormal">x</span><span class=msupsub><span class=vlist-t><span class=vlist-r><span class=vlist style=height:.888em><span style=top:-3.063em;margin-right:.05em><span class=pstrut style=height:2.7em></span><span class="mtight sizing reset-size6 size3"><span class="mtight mord"><span class="mtight mopen">(</span><span class="mtight mord mathnormal">i</span><span class="mtight mclose">)</span></span></span></span></span></span></span></span></span></span></span></span> 映射为查询向量、键向量和值向量.</p><p><img alt="" src=https://skindhu.github.io/Build-A-Large-Language-Model-CN/Image/chapter3/figure3.14.png></p><div class=expressive-code><figure class=frame><figcaption class=header></figcaption><pre data-language=python><code><div class=ec-line><div class=gutter><div class=ln aria-hidden=true>1</div></div><div class=code><span style=--0:#E1E4E8>torch.manual_seed(</span><span style=--0:#79B8FF>123</span><span style=--0:#E1E4E8>)</span></div></div><div class=ec-line><div class=gutter><div class=ln aria-hidden=true>2</div></div><div class=code><span style=--0:#E1E4E8>W_query </span><span style=--0:#F97583>=</span><span style=--0:#E1E4E8> torch.nn.Parameter(torch.rand(d_in, d_out))</span></div></div><div class=ec-line><div class=gutter><div class=ln aria-hidden=true>3</div></div><div class=code><span style=--0:#E1E4E8>W_key   </span><span style=--0:#F97583>=</span><span style=--0:#E1E4E8> torch.nn.Parameter(torch.rand(d_in, d_out))</span></div></div><div class=ec-line><div class=gutter><div class=ln aria-hidden=true>4</div></div><div class=code><span style=--0:#E1E4E8>W_value </span><span style=--0:#F97583>=</span><span style=--0:#E1E4E8> torch.nn.Parameter(torch.rand(d_in, d_out))</span></div></div></code></pre><div class=copy><button title="Copy to clipboard" data-code="torch.manual_seed(123)W_query = torch.nn.Parameter(torch.rand(d_in, d_out))W_key   = torch.nn.Parameter(torch.rand(d_in, d_out))W_value = torch.nn.Parameter(torch.rand(d_in, d_out))" data-copied=Copied!><div></div></button></div></figure></div><p>在 GPT 类模型中，输入维度和输出维度通常是相同的。</p><p>我们可以通过矩阵乘法获取所有元素的key和value向量：</p><div class=expressive-code><figure class=frame><figcaption class=header></figcaption><pre data-language=python><code><div class=ec-line><div class=gutter><div class=ln aria-hidden=true>1</div></div><div class=code><span style=--0:#E1E4E8>keys </span><span style=--0:#F97583>=</span><span style=--0:#E1E4E8> inputs </span><span style=--0:#F97583>@</span><span style=--0:#E1E4E8> W_key</span></div></div><div class=ec-line><div class=gutter><div class=ln aria-hidden=true>2</div></div><div class=code><span style=--0:#E1E4E8>values </span><span style=--0:#F97583>=</span><span style=--0:#E1E4E8> inputs </span><span style=--0:#F97583>@</span><span style=--0:#E1E4E8> W_value</span></div></div></code></pre><div class=copy><button title="Copy to clipboard" data-code="keys = inputs @ W_keyvalues = inputs @ W_value" data-copied=Copied!><div></div></button></div></figure></div><blockquote><p>在标准的自注意力机制中，W、K、V向量都是固定的，然而，由于 GPT 模型是由多层自注意力模块堆叠而成，每一层都会根据当前输入和上下文信息，动态调整查询、键和值向量的<strong>权重矩阵</strong>。因此，即使初始的词嵌入和权重矩阵是固定的，经过多层处理后，模型能够生成与当前上下文相关的 Q、K、V 向量权重矩阵，最终计算出的Q、K、V 向量也就能反映出上下文的语义了。</p></blockquote><p><img alt="" src=https://skindhu.github.io/Build-A-Large-Language-Model-CN/Image/chapter3/figure3.16.png> 接下来的注意力权重计算,使用点积缩放注意力机制,将注意力得分除以嵌入维度的平方根来进行缩放:</p><div class=expressive-code><figure class=frame><figcaption class=header></figcaption><pre data-language=python><code><div class=ec-line><div class=gutter><div class=ln aria-hidden=true>1</div></div><div class=code><span style=--0:#E1E4E8>d_k </span><span style=--0:#F97583>=</span><span style=--0:#E1E4E8> keys.shape[</span><span style=--0:#F97583>-</span><span style=--0:#79B8FF>1</span><span style=--0:#E1E4E8>]</span></div></div><div class=ec-line><div class=gutter><div class=ln aria-hidden=true>2</div></div><div class=code><span style=--0:#E1E4E8>attn_weights_2 </span><span style=--0:#F97583>=</span><span style=--0:#E1E4E8> torch.softmax(attn_scores_2 </span><span style=--0:#F97583>/</span><span style=--0:#E1E4E8> d_k</span><span style=--0:#F97583>**</span><span style=--0:#79B8FF>0.5</span><span style=--0:#E1E4E8>, </span><span style=--0:#FFAB70>dim</span><span style=--0:#F97583>=-</span><span style=--0:#79B8FF>1</span><span style=--0:#E1E4E8>)</span></div></div><div class=ec-line><div class=gutter><div class=ln aria-hidden=true>3</div></div><div class=code><span style=--0:#79B8FF>print</span><span style=--0:#E1E4E8>(attn_weights_2)</span></div></div></code></pre><div class=copy><button title="Copy to clipboard" data-code="d_k = keys.shape[-1]attn_weights_2 = torch.softmax(attn_scores_2 / d_k**0.5, dim=-1)print(attn_weights_2)" data-copied=Copied!><div></div></button></div></figure></div><blockquote><p>在自注意力机制中，查询向量（Query）与键向量（Key）之间的点积用于计算注意力权重。然而，当嵌入维度较大时，点积的结果可能会非常大。那么大的点积对接下来的计算有哪些具体影响呢？</p><ul><li><strong>Softmax函数的特性</strong>：在计算注意力权重时，点积结果会通过Softmax函数转换为概率分布。而Softmax函数对输入值的差异非常敏感，当输入值较大时，Softmax的输出会趋近于0或1，表现得类似于阶跃函数（step function）。</li><li><strong>梯度消失问题</strong>：当Softmax的输出接近0或1时，其梯度会非常小，接近于零（可以通过3.3.1小节中提到的Softmax公式推断）。这意味着在反向传播过程中，梯度更新幅度会很小，导致模型学习速度减慢，甚至训练停滞。 为了解决上述问题，在计算点积后，将结果除以嵌入维度的平方根。这样可以将点积结果缩放到适当的范围，避免Softmax函数进入梯度平缓区，从而保持梯度的有效性，促进模型的正常训练。</li></ul></blockquote><p><img alt="" src=https://skindhu.github.io/Build-A-Large-Language-Model-CN/Image/chapter3/figure3.17.png></p><p>现在我们通过值向量的加权和来计算上下文向量。这里，注意力权重作为加权因子，用于衡量每个值向量的重要性。</p><div class=expressive-code><figure class=frame><figcaption class=header></figcaption><pre data-language=python><code><div class=ec-line><div class=gutter><div class=ln aria-hidden=true>1</div></div><div class=code><span style=--0:#E1E4E8>context_vec_2 </span><span style=--0:#F97583>=</span><span style=--0:#E1E4E8> attn_weights_2 </span><span style=--0:#F97583>@</span><span style=--0:#E1E4E8> values</span></div></div><div class=ec-line><div class=gutter><div class=ln aria-hidden=true>2</div></div><div class=code><span style=--0:#79B8FF>print</span><span style=--0:#E1E4E8>(context_vec_2)</span></div></div></code></pre><div class=copy><button title="Copy to clipboard" data-code="context_vec_2 = attn_weights_2 @ valuesprint(context_vec_2)" data-copied=Copied!><div></div></button></div></figure></div><p>这就是一个上下文向量<span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><msup><mi>z</mi><mrow><mo stretchy=false>(</mo><mn>2</mn><mo stretchy=false>)</mo></mrow></msup></mrow><annotation encoding=application/x-tex>z^{(2)}</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:.888em></span><span class=mord><span class="mord mathnormal" style=margin-right:.04398em>z</span><span class=msupsub><span class=vlist-t><span class=vlist-r><span class=vlist style=height:.888em><span style=top:-3.063em;margin-right:.05em><span class=pstrut style=height:2.7em></span><span class="mtight sizing reset-size6 size3"><span class="mtight mord"><span class="mtight mopen">(</span><span class="mtight mord">2</span><span class="mtight mclose">)</span></span></span></span></span></span></span></span></span></span></span></span>.我们将计算输入序列中的所有上下文向量,从<span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><msup><mi>z</mi><mrow><mo stretchy=false>(</mo><mn>1</mn><mo stretchy=false>)</mo></mrow></msup></mrow><annotation encoding=application/x-tex>z^{(1)}</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:.888em></span><span class=mord><span class="mord mathnormal" style=margin-right:.04398em>z</span><span class=msupsub><span class=vlist-t><span class=vlist-r><span class=vlist style=height:.888em><span style=top:-3.063em;margin-right:.05em><span class=pstrut style=height:2.7em></span><span class="mtight sizing reset-size6 size3"><span class="mtight mord"><span class="mtight mopen">(</span><span class="mtight mord">1</span><span class="mtight mclose">)</span></span></span></span></span></span></span></span></span></span></span></span>到<span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><msup><mi>z</mi><mrow><mo stretchy=false>(</mo><mi>T</mi><mo stretchy=false>)</mo></mrow></msup></mrow><annotation encoding=application/x-tex>z^{(T)}</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:.888em></span><span class=mord><span class="mord mathnormal" style=margin-right:.04398em>z</span><span class=msupsub><span class=vlist-t><span class=vlist-r><span class=vlist style=height:.888em><span style=top:-3.063em;margin-right:.05em><span class=pstrut style=height:2.7em></span><span class="mtight sizing reset-size6 size3"><span class="mtight mord"><span class="mtight mopen">(</span><span class="mtight mord mathnormal" style=margin-right:.13889em>T</span><span class="mtight mclose">)</span></span></span></span></span></span></span></span></span></span></span></span>.</p><h3 id=实现一个简洁的子注意力机制python类>实现一个简洁的子注意力机制Python类</h3><p><code>torch.nn.Module</code>是PyTorch模型的基础组件,提供了创建和管理模型层所需的必要功能.</p><p><img alt="" src=https://skindhu.github.io/Build-A-Large-Language-Model-CN/Image/chapter3/figure3.18.png></p><h2 id=使用因果注意力机制来屏蔽后续词>使用因果注意力机制来屏蔽后续词</h2><p>屏蔽注意力是一种特殊的自注意力形式.它限制模型在处理任何给定的 token 时，只能考虑序列中的前一个和当前输入，而不能看到后续的内容。这与标准的自注意力机制形成对比，后者允许模型同时访问整个输入序列。</p><p><img alt="" src=https://skindhu.github.io/Build-A-Large-Language-Model-CN/Image/chapter3/figure3.19.png></p><h3 id=应用因果注意力掩码>应用因果注意力掩码</h3><p><img alt="" src=https://skindhu.github.io/Build-A-Large-Language-Model-CN/Image/chapter3/figure3.20.png></p><p>我们可以使用 PyTorch 的 <code>tril</code> 函数生成一个掩码矩阵，使对角线以上的值为零：</p><div class=expressive-code><figure class=frame><figcaption class=header></figcaption><pre data-language=python><code><div class=ec-line><div class=gutter><div class=ln aria-hidden=true>1</div></div><div class=code><span style=--0:#E1E4E8>context_length </span><span style=--0:#F97583>=</span><span style=--0:#E1E4E8> attn_scores.shape[</span><span style=--0:#79B8FF>0</span><span style=--0:#E1E4E8>]</span></div></div><div class=ec-line><div class=gutter><div class=ln aria-hidden=true>2</div></div><div class=code><span style=--0:#E1E4E8>mask_simple </span><span style=--0:#F97583>=</span><span style=--0:#E1E4E8> torch.tril(torch.ones(context_length, context_length))</span></div></div><div class=ec-line><div class=gutter><div class=ln aria-hidden=true>3</div></div><div class=code><span style=--0:#79B8FF>print</span><span style=--0:#E1E4E8>(mask_simple)</span></div></div></code></pre><div class=copy><button title="Copy to clipboard" data-code="context_length = attn_scores.shape[0]mask_simple = torch.tril(torch.ones(context_length, context_length))print(mask_simple)" data-copied=Copied!><div></div></button></div></figure></div><p>生成的掩码如下所示：</p><div class=expressive-code><figure class=frame><figcaption class=header></figcaption><pre data-language=python><code><div class=ec-line><div class=gutter><div class=ln aria-hidden=true>1</div></div><div class=code><span style=--0:#E1E4E8>tensor([[</span><span style=--0:#79B8FF>1</span><span style=--0:#E1E4E8>., </span><span style=--0:#79B8FF>0</span><span style=--0:#E1E4E8>., </span><span style=--0:#79B8FF>0</span><span style=--0:#E1E4E8>., </span><span style=--0:#79B8FF>0</span><span style=--0:#E1E4E8>., </span><span style=--0:#79B8FF>0</span><span style=--0:#E1E4E8>., </span><span style=--0:#79B8FF>0</span><span style=--0:#E1E4E8>.],</span></div></div><div class=ec-line><div class=gutter><div class=ln aria-hidden=true>2</div></div><div class=code><span class=indent><span style=--0:#E1E4E8>        </span></span><span style=--0:#E1E4E8>[</span><span style=--0:#79B8FF>1</span><span style=--0:#E1E4E8>., </span><span style=--0:#79B8FF>1</span><span style=--0:#E1E4E8>., </span><span style=--0:#79B8FF>0</span><span style=--0:#E1E4E8>., </span><span style=--0:#79B8FF>0</span><span style=--0:#E1E4E8>., </span><span style=--0:#79B8FF>0</span><span style=--0:#E1E4E8>., </span><span style=--0:#79B8FF>0</span><span style=--0:#E1E4E8>.],</span></div></div><div class=ec-line><div class=gutter><div class=ln aria-hidden=true>3</div></div><div class=code><span class=indent><span style=--0:#E1E4E8>        </span></span><span style=--0:#E1E4E8>[</span><span style=--0:#79B8FF>1</span><span style=--0:#E1E4E8>., </span><span style=--0:#79B8FF>1</span><span style=--0:#E1E4E8>., </span><span style=--0:#79B8FF>1</span><span style=--0:#E1E4E8>., </span><span style=--0:#79B8FF>0</span><span style=--0:#E1E4E8>., </span><span style=--0:#79B8FF>0</span><span style=--0:#E1E4E8>., </span><span style=--0:#79B8FF>0</span><span style=--0:#E1E4E8>.],</span></div></div><div class=ec-line><div class=gutter><div class=ln aria-hidden=true>4</div></div><div class=code><span class=indent><span style=--0:#E1E4E8>        </span></span><span style=--0:#E1E4E8>[</span><span style=--0:#79B8FF>1</span><span style=--0:#E1E4E8>., </span><span style=--0:#79B8FF>1</span><span style=--0:#E1E4E8>., </span><span style=--0:#79B8FF>1</span><span style=--0:#E1E4E8>., </span><span style=--0:#79B8FF>1</span><span style=--0:#E1E4E8>., </span><span style=--0:#79B8FF>0</span><span style=--0:#E1E4E8>., </span><span style=--0:#79B8FF>0</span><span style=--0:#E1E4E8>.],</span></div></div><div class=ec-line><div class=gutter><div class=ln aria-hidden=true>5</div></div><div class=code><span class=indent><span style=--0:#E1E4E8>        </span></span><span style=--0:#E1E4E8>[</span><span style=--0:#79B8FF>1</span><span style=--0:#E1E4E8>., </span><span style=--0:#79B8FF>1</span><span style=--0:#E1E4E8>., </span><span style=--0:#79B8FF>1</span><span style=--0:#E1E4E8>., </span><span style=--0:#79B8FF>1</span><span style=--0:#E1E4E8>., </span><span style=--0:#79B8FF>1</span><span style=--0:#E1E4E8>., </span><span style=--0:#79B8FF>0</span><span style=--0:#E1E4E8>.],</span></div></div><div class=ec-line><div class=gutter><div class=ln aria-hidden=true>6</div></div><div class=code><span class=indent><span style=--0:#E1E4E8>        </span></span><span style=--0:#E1E4E8>[</span><span style=--0:#79B8FF>1</span><span style=--0:#E1E4E8>., </span><span style=--0:#79B8FF>1</span><span style=--0:#E1E4E8>., </span><span style=--0:#79B8FF>1</span><span style=--0:#E1E4E8>., </span><span style=--0:#79B8FF>1</span><span style=--0:#E1E4E8>., </span><span style=--0:#79B8FF>1</span><span style=--0:#E1E4E8>., </span><span style=--0:#79B8FF>1</span><span style=--0:#E1E4E8>.]])</span></div></div></code></pre><div class=copy><button title="Copy to clipboard" data-code="tensor([[1., 0., 0., 0., 0., 0.],        [1., 1., 0., 0., 0., 0.],        [1., 1., 1., 0., 0., 0.],        [1., 1., 1., 1., 0., 0.],        [1., 1., 1., 1., 1., 0.],        [1., 1., 1., 1., 1., 1.]])" data-copied=Copied!><div></div></button></div></figure></div><p>现在，我们可以将这个掩码矩阵与注意力权重相乘，从而将对角线以上的值置零。</p><div class=expressive-code><figure class=frame><figcaption class=header></figcaption><pre data-language=python><code><div class=ec-line><div class=gutter><div class=ln aria-hidden=true>1</div></div><div class=code><span style=--0:#E1E4E8>masked_simple </span><span style=--0:#F97583>=</span><span style=--0:#E1E4E8> attn_weights</span><span style=--0:#F97583>*</span><span style=--0:#E1E4E8>mask_simple</span></div></div></code></pre><div class=copy><button title="Copy to clipboard" data-code="masked_simple = attn_weights*mask_simple" data-copied=Copied!><div></div></button></div></figure></div><p>第三步是将注意力权重重新归一化,使得每一行的权重和再次等于1.</p><blockquote><p>我一开始觉得,两次softmax混入了后面的信息,实则不然,证明如下: 第一次softmax: <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mfrac><msup><mi>e</mi><msub><mi>z</mi><mn>1</mn></msub></msup><mrow><mo>∑</mo><msup><mi>e</mi><msub><mi>z</mi><mi>j</mi></msub></msup></mrow></mfrac><mo separator=true>,</mo><mfrac><msup><mi>e</mi><msub><mi>z</mi><mn>2</mn></msub></msup><mrow><mo>∑</mo><msup><mi>e</mi><msub><mi>z</mi><mi>j</mi></msub></msup></mrow></mfrac><mo separator=true>,</mo><mfrac><msup><mi>e</mi><msub><mi>z</mi><mn>3</mn></msub></msup><mrow><mo>∑</mo><msup><mi>e</mi><msub><mi>z</mi><mi>j</mi></msub></msup></mrow></mfrac><mo separator=true>,</mo><mo>…</mo></mrow><annotation encoding=application/x-tex>\frac{e^{z_{1}}}{\sum e^{z_{j}}}, \frac{e^{z_{2}}}{\sum e^{z_{j}}},\frac{e^{z_{3}}}{\sum e^{z_{j}}},\dots</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:1.4413em;vertical-align:-.5303em></span><span class=mord><span class="mopen nulldelimiter"></span><span class=mfrac><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:.911em><span style=top:-2.6447em><span class=pstrut style=height:3em></span><span class="mtight sizing reset-size6 size3"><span class="mtight mord"><span class="mtight mop op-symbol small-op" style=position:relative;top:0>∑</span><span class="mtight mspace" style=margin-right:.1952em></span><span class="mtight mord"><span class="mtight mord mathnormal">e</span><span class=msupsub><span class=vlist-t><span class=vlist-r><span class=vlist style=height:.779em><span style=top:-2.9714em;margin-right:.0714em><span class=pstrut style=height:2.5em></span><span class="mtight sizing reset-size3 size1"><span class="mtight mord"><span class="mtight mord"><span class="mtight mord mathnormal" style=margin-right:.04398em>z</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:.3448em><span style=top:-2.3448em;margin-left:-.044em;margin-right:.1em><span class=pstrut style=height:2.6595em></span><span class="mtight mord"><span class="mtight mord mathnormal" style=margin-right:.05724em>j</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:.5092em><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span style=top:-3.23em><span class=pstrut style=height:3em></span><span class=frac-line style=border-bottom-width:.04em></span></span><span style=top:-3.394em><span class=pstrut style=height:3em></span><span class="mtight sizing reset-size6 size3"><span class="mtight mord"><span class="mtight mord"><span class="mtight mord mathnormal">e</span><span class=msupsub><span class=vlist-t><span class=vlist-r><span class=vlist style=height:.7385em><span style=top:-2.931em;margin-right:.0714em><span class=pstrut style=height:2.5em></span><span class="mtight sizing reset-size3 size1"><span class="mtight mord"><span class="mtight mord"><span class="mtight mord mathnormal" style=margin-right:.04398em>z</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:.3448em><span style=top:-2.3448em;margin-left:-.044em;margin-right:.1em><span class=pstrut style=height:2.6444em></span><span class="mtight mord"><span class="mtight mord">1</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:.2996em><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:.5303em><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class=mpunct>,</span><span class=mspace style=margin-right:.1667em></span><span class=mord><span class="mopen nulldelimiter"></span><span class=mfrac><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:.911em><span style=top:-2.6447em><span class=pstrut style=height:3em></span><span class="mtight sizing reset-size6 size3"><span class="mtight mord"><span class="mtight mop op-symbol small-op" style=position:relative;top:0>∑</span><span class="mtight mspace" style=margin-right:.1952em></span><span class="mtight mord"><span class="mtight mord mathnormal">e</span><span class=msupsub><span class=vlist-t><span class=vlist-r><span class=vlist style=height:.779em><span style=top:-2.9714em;margin-right:.0714em><span class=pstrut style=height:2.5em></span><span class="mtight sizing reset-size3 size1"><span class="mtight mord"><span class="mtight mord"><span class="mtight mord mathnormal" style=margin-right:.04398em>z</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:.3448em><span style=top:-2.3448em;margin-left:-.044em;margin-right:.1em><span class=pstrut style=height:2.6595em></span><span class="mtight mord"><span class="mtight mord mathnormal" style=margin-right:.05724em>j</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:.5092em><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span style=top:-3.23em><span class=pstrut style=height:3em></span><span class=frac-line style=border-bottom-width:.04em></span></span><span style=top:-3.394em><span class=pstrut style=height:3em></span><span class="mtight sizing reset-size6 size3"><span class="mtight mord"><span class="mtight mord"><span class="mtight mord mathnormal">e</span><span class=msupsub><span class=vlist-t><span class=vlist-r><span class=vlist style=height:.7385em><span style=top:-2.931em;margin-right:.0714em><span class=pstrut style=height:2.5em></span><span class="mtight sizing reset-size3 size1"><span class="mtight mord"><span class="mtight mord"><span class="mtight mord mathnormal" style=margin-right:.04398em>z</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:.3448em><span style=top:-2.3448em;margin-left:-.044em;margin-right:.1em><span class=pstrut style=height:2.6444em></span><span class="mtight mord"><span class="mtight mord">2</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:.2996em><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:.5303em><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class=mpunct>,</span><span class=mspace style=margin-right:.1667em></span><span class=mord><span class="mopen nulldelimiter"></span><span class=mfrac><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:.911em><span style=top:-2.6447em><span class=pstrut style=height:3em></span><span class="mtight sizing reset-size6 size3"><span class="mtight mord"><span class="mtight mop op-symbol small-op" style=position:relative;top:0>∑</span><span class="mtight mspace" style=margin-right:.1952em></span><span class="mtight mord"><span class="mtight mord mathnormal">e</span><span class=msupsub><span class=vlist-t><span class=vlist-r><span class=vlist style=height:.779em><span style=top:-2.9714em;margin-right:.0714em><span class=pstrut style=height:2.5em></span><span class="mtight sizing reset-size3 size1"><span class="mtight mord"><span class="mtight mord"><span class="mtight mord mathnormal" style=margin-right:.04398em>z</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:.3448em><span style=top:-2.3448em;margin-left:-.044em;margin-right:.1em><span class=pstrut style=height:2.6595em></span><span class="mtight mord"><span class="mtight mord mathnormal" style=margin-right:.05724em>j</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:.5092em><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span style=top:-3.23em><span class=pstrut style=height:3em></span><span class=frac-line style=border-bottom-width:.04em></span></span><span style=top:-3.394em><span class=pstrut style=height:3em></span><span class="mtight sizing reset-size6 size3"><span class="mtight mord"><span class="mtight mord"><span class="mtight mord mathnormal">e</span><span class=msupsub><span class=vlist-t><span class=vlist-r><span class=vlist style=height:.7385em><span style=top:-2.931em;margin-right:.0714em><span class=pstrut style=height:2.5em></span><span class="mtight sizing reset-size3 size1"><span class="mtight mord"><span class="mtight mord"><span class="mtight mord mathnormal" style=margin-right:.04398em>z</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:.3448em><span style=top:-2.3448em;margin-left:-.044em;margin-right:.1em><span class=pstrut style=height:2.6444em></span><span class="mtight mord"><span class="mtight mord">3</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:.2996em><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:.5303em><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class=mpunct>,</span><span class=mspace style=margin-right:.1667em></span><span class=minner>…</span></span></span></span> 第二次softmax: <span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mfrac><msup><mi>e</mi><mrow><mo stretchy=false>(</mo><mn>1</mn><mo stretchy=false>)</mo></mrow></msup><mrow><msup><mi>e</mi><mrow><mo stretchy=false>(</mo><mn>1</mn><mo stretchy=false>)</mo></mrow></msup><mo>+</mo><msup><mi>e</mi><mrow><mo stretchy=false>(</mo><mn>2</mn><mo stretchy=false>)</mo></mrow></msup><mo>+</mo><msup><mi>e</mi><mrow><mo stretchy=false>(</mo><mn>3</mn><mo stretchy=false>)</mo></mrow></msup></mrow></mfrac><mo separator=true>,</mo><mfrac><msup><mi>e</mi><mrow><mo stretchy=false>(</mo><mn>2</mn><mo stretchy=false>)</mo></mrow></msup><mrow><msup><mi>e</mi><mrow><mo stretchy=false>(</mo><mn>1</mn><mo stretchy=false>)</mo></mrow></msup><mo>+</mo><msup><mi>e</mi><mrow><mo stretchy=false>(</mo><mn>2</mn><mo stretchy=false>)</mo></mrow></msup><mo>+</mo><msup><mi>e</mi><mrow><mo stretchy=false>(</mo><mn>3</mn><mo stretchy=false>)</mo></mrow></msup></mrow></mfrac><mo separator=true>,</mo><mfrac><msup><mi>e</mi><mrow><mo stretchy=false>(</mo><mn>3</mn><mo stretchy=false>)</mo></mrow></msup><mrow><msup><mi>e</mi><mrow><mo stretchy=false>(</mo><mn>1</mn><mo stretchy=false>)</mo></mrow></msup><mo>+</mo><msup><mi>e</mi><mrow><mo stretchy=false>(</mo><mn>2</mn><mo stretchy=false>)</mo></mrow></msup><mo>+</mo><msup><mi>e</mi><mrow><mo stretchy=false>(</mo><mn>3</mn><mo stretchy=false>)</mo></mrow></msup></mrow></mfrac></mrow><annotation encoding=application/x-tex>\frac{e^{(1)}}{e^{(1)}+e^{(2)}+e^{(3)}},\frac{e^{(2)}}{e^{(1)}+e^{(2)}+e^{(3)}},\frac{e^{(3)}}{e^{(1)}+e^{(2)}+e^{(3)}}</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:1.5145em;vertical-align:-.4438em></span><span class=mord><span class="mopen nulldelimiter"></span><span class=mfrac><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:1.0707em><span style=top:-2.6146em><span class=pstrut style=height:3em></span><span class="mtight sizing reset-size6 size3"><span class="mtight mord"><span class="mtight mord"><span class="mtight mord mathnormal">e</span><span class=msupsub><span class=vlist-t><span class=vlist-r><span class=vlist style=height:.822em><span style=top:-2.822em;margin-right:.0714em><span class=pstrut style=height:2.5357em></span><span class="mtight sizing reset-size3 size1"><span class="mtight mord"><span class="mtight mopen">(</span><span class="mtight mord">1</span><span class="mtight mclose">)</span></span></span></span></span></span></span></span></span><span class="mtight mbin">+</span><span class="mtight mord"><span class="mtight mord mathnormal">e</span><span class=msupsub><span class=vlist-t><span class=vlist-r><span class=vlist style=height:.822em><span style=top:-2.822em;margin-right:.0714em><span class=pstrut style=height:2.5357em></span><span class="mtight sizing reset-size3 size1"><span class="mtight mord"><span class="mtight mopen">(</span><span class="mtight mord">2</span><span class="mtight mclose">)</span></span></span></span></span></span></span></span></span><span class="mtight mbin">+</span><span class="mtight mord"><span class="mtight mord mathnormal">e</span><span class=msupsub><span class=vlist-t><span class=vlist-r><span class=vlist style=height:.822em><span style=top:-2.822em;margin-right:.0714em><span class=pstrut style=height:2.5357em></span><span class="mtight sizing reset-size3 size1"><span class="mtight mord"><span class="mtight mopen">(</span><span class="mtight mord">3</span><span class="mtight mclose">)</span></span></span></span></span></span></span></span></span></span></span></span><span style=top:-3.23em><span class=pstrut style=height:3em></span><span class=frac-line style=border-bottom-width:.04em></span></span><span style=top:-3.394em><span class=pstrut style=height:3em></span><span class="mtight sizing reset-size6 size3"><span class="mtight mord"><span class="mtight mord"><span class="mtight mord mathnormal">e</span><span class=msupsub><span class=vlist-t><span class=vlist-r><span class=vlist style=height:.9667em><span style=top:-2.9667em;margin-right:.0714em><span class=pstrut style=height:2.5357em></span><span class="mtight sizing reset-size3 size1"><span class="mtight mord"><span class="mtight mopen">(</span><span class="mtight mord">1</span><span class="mtight mclose">)</span></span></span></span></span></span></span></span></span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:.4438em><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class=mpunct>,</span><span class=mspace style=margin-right:.1667em></span><span class=mord><span class="mopen nulldelimiter"></span><span class=mfrac><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:1.0707em><span style=top:-2.6146em><span class=pstrut style=height:3em></span><span class="mtight sizing reset-size6 size3"><span class="mtight mord"><span class="mtight mord"><span class="mtight mord mathnormal">e</span><span class=msupsub><span class=vlist-t><span class=vlist-r><span class=vlist style=height:.822em><span style=top:-2.822em;margin-right:.0714em><span class=pstrut style=height:2.5357em></span><span class="mtight sizing reset-size3 size1"><span class="mtight mord"><span class="mtight mopen">(</span><span class="mtight mord">1</span><span class="mtight mclose">)</span></span></span></span></span></span></span></span></span><span class="mtight mbin">+</span><span class="mtight mord"><span class="mtight mord mathnormal">e</span><span class=msupsub><span class=vlist-t><span class=vlist-r><span class=vlist style=height:.822em><span style=top:-2.822em;margin-right:.0714em><span class=pstrut style=height:2.5357em></span><span class="mtight sizing reset-size3 size1"><span class="mtight mord"><span class="mtight mopen">(</span><span class="mtight mord">2</span><span class="mtight mclose">)</span></span></span></span></span></span></span></span></span><span class="mtight mbin">+</span><span class="mtight mord"><span class="mtight mord mathnormal">e</span><span class=msupsub><span class=vlist-t><span class=vlist-r><span class=vlist style=height:.822em><span style=top:-2.822em;margin-right:.0714em><span class=pstrut style=height:2.5357em></span><span class="mtight sizing reset-size3 size1"><span class="mtight mord"><span class="mtight mopen">(</span><span class="mtight mord">3</span><span class="mtight mclose">)</span></span></span></span></span></span></span></span></span></span></span></span><span style=top:-3.23em><span class=pstrut style=height:3em></span><span class=frac-line style=border-bottom-width:.04em></span></span><span style=top:-3.394em><span class=pstrut style=height:3em></span><span class="mtight sizing reset-size6 size3"><span class="mtight mord"><span class="mtight mord"><span class="mtight mord mathnormal">e</span><span class=msupsub><span class=vlist-t><span class=vlist-r><span class=vlist style=height:.9667em><span style=top:-2.9667em;margin-right:.0714em><span class=pstrut style=height:2.5357em></span><span class="mtight sizing reset-size3 size1"><span class="mtight mord"><span class="mtight mopen">(</span><span class="mtight mord">2</span><span class="mtight mclose">)</span></span></span></span></span></span></span></span></span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:.4438em><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class=mpunct>,</span><span class=mspace style=margin-right:.1667em></span><span class=mord><span class="mopen nulldelimiter"></span><span class=mfrac><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:1.0707em><span style=top:-2.6146em><span class=pstrut style=height:3em></span><span class="mtight sizing reset-size6 size3"><span class="mtight mord"><span class="mtight mord"><span class="mtight mord mathnormal">e</span><span class=msupsub><span class=vlist-t><span class=vlist-r><span class=vlist style=height:.822em><span style=top:-2.822em;margin-right:.0714em><span class=pstrut style=height:2.5357em></span><span class="mtight sizing reset-size3 size1"><span class="mtight mord"><span class="mtight mopen">(</span><span class="mtight mord">1</span><span class="mtight mclose">)</span></span></span></span></span></span></span></span></span><span class="mtight mbin">+</span><span class="mtight mord"><span class="mtight mord mathnormal">e</span><span class=msupsub><span class=vlist-t><span class=vlist-r><span class=vlist style=height:.822em><span style=top:-2.822em;margin-right:.0714em><span class=pstrut style=height:2.5357em></span><span class="mtight sizing reset-size3 size1"><span class="mtight mord"><span class="mtight mopen">(</span><span class="mtight mord">2</span><span class="mtight mclose">)</span></span></span></span></span></span></span></span></span><span class="mtight mbin">+</span><span class="mtight mord"><span class="mtight mord mathnormal">e</span><span class=msupsub><span class=vlist-t><span class=vlist-r><span class=vlist style=height:.822em><span style=top:-2.822em;margin-right:.0714em><span class=pstrut style=height:2.5357em></span><span class="mtight sizing reset-size3 size1"><span class="mtight mord"><span class="mtight mopen">(</span><span class="mtight mord">3</span><span class="mtight mclose">)</span></span></span></span></span></span></span></span></span></span></span></span><span style=top:-3.23em><span class=pstrut style=height:3em></span><span class=frac-line style=border-bottom-width:.04em></span></span><span style=top:-3.394em><span class=pstrut style=height:3em></span><span class="mtight sizing reset-size6 size3"><span class="mtight mord"><span class="mtight mord"><span class="mtight mord mathnormal">e</span><span class=msupsub><span class=vlist-t><span class=vlist-r><span class=vlist style=height:.9667em><span style=top:-2.9667em;margin-right:.0714em><span class=pstrut style=height:2.5357em></span><span class="mtight sizing reset-size3 size1"><span class="mtight mord"><span class="mtight mopen">(</span><span class="mtight mord">3</span><span class="mtight mclose">)</span></span></span></span></span></span></span></span></span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:.4438em><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span> 化简得<span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mfrac><msup><mi>e</mi><msub><mi>z</mi><mn>1</mn></msub></msup><mrow><msup><mi>e</mi><msub><mi>z</mi><mn>1</mn></msub></msup><mo>+</mo><msup><mi>e</mi><msub><mi>z</mi><mn>2</mn></msub></msup><mo>+</mo><msup><mi>e</mi><msub><mi>z</mi><mn>3</mn></msub></msup></mrow></mfrac><mo separator=true>,</mo><mfrac><msup><mi>e</mi><msub><mi>z</mi><mn>2</mn></msub></msup><mrow><msup><mi>e</mi><msub><mi>z</mi><mn>1</mn></msub></msup><mo>+</mo><msup><mi>e</mi><msub><mi>z</mi><mn>2</mn></msub></msup><mo>+</mo><msup><mi>e</mi><msub><mi>z</mi><mn>3</mn></msub></msup></mrow></mfrac><mo separator=true>,</mo><mfrac><msup><mi>e</mi><msub><mi>z</mi><mn>3</mn></msub></msup><mrow><msup><mi>e</mi><msub><mi>z</mi><mn>1</mn></msub></msup><mo>+</mo><msup><mi>e</mi><msub><mi>z</mi><mn>2</mn></msub></msup><mo>+</mo><msup><mi>e</mi><msub><mi>z</mi><mn>3</mn></msub></msup></mrow></mfrac></mrow><annotation encoding=application/x-tex>\frac{e^{z_{1}}}{e^{z_{1}}+e^{z_{2}}+e^{z_{3}}},\frac{e^{z_{2}}}{e^{z_{1}}+e^{z_{2}}+e^{z_{3}}},\frac{e^{z_{3}}}{e^{z_{1}}+e^{z_{2}}+e^{z_{3}}}</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:1.3143em;vertical-align:-.4033em></span><span class=mord><span class="mopen nulldelimiter"></span><span class=mfrac><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:.911em><span style=top:-2.655em><span class=pstrut style=height:3em></span><span class="mtight sizing reset-size6 size3"><span class="mtight mord"><span class="mtight mord"><span class="mtight mord mathnormal">e</span><span class=msupsub><span class=vlist-t><span class=vlist-r><span class=vlist style=height:.6293em><span style=top:-2.8218em;margin-right:.0714em><span class=pstrut style=height:2.5em></span><span class="mtight sizing reset-size3 size1"><span class="mtight mord"><span class="mtight mord"><span class="mtight mord mathnormal" style=margin-right:.04398em>z</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:.3448em><span style=top:-2.3448em;margin-left:-.044em;margin-right:.1em><span class=pstrut style=height:2.6444em></span><span class="mtight mord"><span class="mtight mord">1</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:.2996em><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span class="mtight mbin">+</span><span class="mtight mord"><span class="mtight mord mathnormal">e</span><span class=msupsub><span class=vlist-t><span class=vlist-r><span class=vlist style=height:.6293em><span style=top:-2.8218em;margin-right:.0714em><span class=pstrut style=height:2.5em></span><span class="mtight sizing reset-size3 size1"><span class="mtight mord"><span class="mtight mord"><span class="mtight mord mathnormal" style=margin-right:.04398em>z</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:.3448em><span style=top:-2.3448em;margin-left:-.044em;margin-right:.1em><span class=pstrut style=height:2.6444em></span><span class="mtight mord"><span class="mtight mord">2</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:.2996em><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span class="mtight mbin">+</span><span class="mtight mord"><span class="mtight mord mathnormal">e</span><span class=msupsub><span class=vlist-t><span class=vlist-r><span class=vlist style=height:.6293em><span style=top:-2.8218em;margin-right:.0714em><span class=pstrut style=height:2.5em></span><span class="mtight sizing reset-size3 size1"><span class="mtight mord"><span class="mtight mord"><span class="mtight mord mathnormal" style=margin-right:.04398em>z</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:.3448em><span style=top:-2.3448em;margin-left:-.044em;margin-right:.1em><span class=pstrut style=height:2.6444em></span><span class="mtight mord"><span class="mtight mord">3</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:.2996em><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span style=top:-3.23em><span class=pstrut style=height:3em></span><span class=frac-line style=border-bottom-width:.04em></span></span><span style=top:-3.394em><span class=pstrut style=height:3em></span><span class="mtight sizing reset-size6 size3"><span class="mtight mord"><span class="mtight mord"><span class="mtight mord mathnormal">e</span><span class=msupsub><span class=vlist-t><span class=vlist-r><span class=vlist style=height:.7385em><span style=top:-2.931em;margin-right:.0714em><span class=pstrut style=height:2.5em></span><span class="mtight sizing reset-size3 size1"><span class="mtight mord"><span class="mtight mord"><span class="mtight mord mathnormal" style=margin-right:.04398em>z</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:.3448em><span style=top:-2.3448em;margin-left:-.044em;margin-right:.1em><span class=pstrut style=height:2.6444em></span><span class="mtight mord"><span class="mtight mord">1</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:.2996em><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:.4033em><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class=mpunct>,</span><span class=mspace style=margin-right:.1667em></span><span class=mord><span class="mopen nulldelimiter"></span><span class=mfrac><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:.911em><span style=top:-2.655em><span class=pstrut style=height:3em></span><span class="mtight sizing reset-size6 size3"><span class="mtight mord"><span class="mtight mord"><span class="mtight mord mathnormal">e</span><span class=msupsub><span class=vlist-t><span class=vlist-r><span class=vlist style=height:.6293em><span style=top:-2.8218em;margin-right:.0714em><span class=pstrut style=height:2.5em></span><span class="mtight sizing reset-size3 size1"><span class="mtight mord"><span class="mtight mord"><span class="mtight mord mathnormal" style=margin-right:.04398em>z</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:.3448em><span style=top:-2.3448em;margin-left:-.044em;margin-right:.1em><span class=pstrut style=height:2.6444em></span><span class="mtight mord"><span class="mtight mord">1</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:.2996em><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span class="mtight mbin">+</span><span class="mtight mord"><span class="mtight mord mathnormal">e</span><span class=msupsub><span class=vlist-t><span class=vlist-r><span class=vlist style=height:.6293em><span style=top:-2.8218em;margin-right:.0714em><span class=pstrut style=height:2.5em></span><span class="mtight sizing reset-size3 size1"><span class="mtight mord"><span class="mtight mord"><span class="mtight mord mathnormal" style=margin-right:.04398em>z</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:.3448em><span style=top:-2.3448em;margin-left:-.044em;margin-right:.1em><span class=pstrut style=height:2.6444em></span><span class="mtight mord"><span class="mtight mord">2</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:.2996em><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span class="mtight mbin">+</span><span class="mtight mord"><span class="mtight mord mathnormal">e</span><span class=msupsub><span class=vlist-t><span class=vlist-r><span class=vlist style=height:.6293em><span style=top:-2.8218em;margin-right:.0714em><span class=pstrut style=height:2.5em></span><span class="mtight sizing reset-size3 size1"><span class="mtight mord"><span class="mtight mord"><span class="mtight mord mathnormal" style=margin-right:.04398em>z</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:.3448em><span style=top:-2.3448em;margin-left:-.044em;margin-right:.1em><span class=pstrut style=height:2.6444em></span><span class="mtight mord"><span class="mtight mord">3</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:.2996em><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span style=top:-3.23em><span class=pstrut style=height:3em></span><span class=frac-line style=border-bottom-width:.04em></span></span><span style=top:-3.394em><span class=pstrut style=height:3em></span><span class="mtight sizing reset-size6 size3"><span class="mtight mord"><span class="mtight mord"><span class="mtight mord mathnormal">e</span><span class=msupsub><span class=vlist-t><span class=vlist-r><span class=vlist style=height:.7385em><span style=top:-2.931em;margin-right:.0714em><span class=pstrut style=height:2.5em></span><span class="mtight sizing reset-size3 size1"><span class="mtight mord"><span class="mtight mord"><span class="mtight mord mathnormal" style=margin-right:.04398em>z</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:.3448em><span style=top:-2.3448em;margin-left:-.044em;margin-right:.1em><span class=pstrut style=height:2.6444em></span><span class="mtight mord"><span class="mtight mord">2</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:.2996em><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:.4033em><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class=mpunct>,</span><span class=mspace style=margin-right:.1667em></span><span class=mord><span class="mopen nulldelimiter"></span><span class=mfrac><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:.911em><span style=top:-2.655em><span class=pstrut style=height:3em></span><span class="mtight sizing reset-size6 size3"><span class="mtight mord"><span class="mtight mord"><span class="mtight mord mathnormal">e</span><span class=msupsub><span class=vlist-t><span class=vlist-r><span class=vlist style=height:.6293em><span style=top:-2.8218em;margin-right:.0714em><span class=pstrut style=height:2.5em></span><span class="mtight sizing reset-size3 size1"><span class="mtight mord"><span class="mtight mord"><span class="mtight mord mathnormal" style=margin-right:.04398em>z</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:.3448em><span style=top:-2.3448em;margin-left:-.044em;margin-right:.1em><span class=pstrut style=height:2.6444em></span><span class="mtight mord"><span class="mtight mord">1</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:.2996em><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span class="mtight mbin">+</span><span class="mtight mord"><span class="mtight mord mathnormal">e</span><span class=msupsub><span class=vlist-t><span class=vlist-r><span class=vlist style=height:.6293em><span style=top:-2.8218em;margin-right:.0714em><span class=pstrut style=height:2.5em></span><span class="mtight sizing reset-size3 size1"><span class="mtight mord"><span class="mtight mord"><span class="mtight mord mathnormal" style=margin-right:.04398em>z</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:.3448em><span style=top:-2.3448em;margin-left:-.044em;margin-right:.1em><span class=pstrut style=height:2.6444em></span><span class="mtight mord"><span class="mtight mord">2</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:.2996em><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span class="mtight mbin">+</span><span class="mtight mord"><span class="mtight mord mathnormal">e</span><span class=msupsub><span class=vlist-t><span class=vlist-r><span class=vlist style=height:.6293em><span style=top:-2.8218em;margin-right:.0714em><span class=pstrut style=height:2.5em></span><span class="mtight sizing reset-size3 size1"><span class="mtight mord"><span class="mtight mord"><span class="mtight mord mathnormal" style=margin-right:.04398em>z</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:.3448em><span style=top:-2.3448em;margin-left:-.044em;margin-right:.1em><span class=pstrut style=height:2.6444em></span><span class="mtight mord"><span class="mtight mord">3</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:.2996em><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span style=top:-3.23em><span class=pstrut style=height:3em></span><span class=frac-line style=border-bottom-width:.04em></span></span><span style=top:-3.394em><span class=pstrut style=height:3em></span><span class="mtight sizing reset-size6 size3"><span class="mtight mord"><span class="mtight mord"><span class="mtight mord mathnormal">e</span><span class=msupsub><span class=vlist-t><span class=vlist-r><span class=vlist style=height:.7385em><span style=top:-2.931em;margin-right:.0714em><span class=pstrut style=height:2.5em></span><span class="mtight sizing reset-size3 size1"><span class="mtight mord"><span class="mtight mord"><span class="mtight mord mathnormal" style=margin-right:.04398em>z</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:.3448em><span style=top:-2.3448em;margin-left:-.044em;margin-right:.1em><span class=pstrut style=height:2.6444em></span><span class="mtight mord"><span class="mtight mord">3</span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:.2996em><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:.4033em><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span> 不包含后面的信息</p></blockquote><p><img alt="" src=https://skindhu.github.io/Build-A-Large-Language-Model-CN/Image/chapter3/figure3.21.png></p><div class=expressive-code><figure class=frame><figcaption class=header></figcaption><pre data-language=python><code><div class=ec-line><div class=gutter><div class=ln aria-hidden=true>1</div></div><div class=code><span style=--0:#E1E4E8>mask </span><span style=--0:#F97583>=</span><span style=--0:#E1E4E8> torch.triu(torch.ones(context_length, context_length), </span><span style=--0:#FFAB70>diagonal</span><span style=--0:#F97583>=</span><span style=--0:#79B8FF>1</span><span style=--0:#E1E4E8>)</span></div></div><div class=ec-line><div class=gutter><div class=ln aria-hidden=true>2</div></div><div class=code><span style=--0:#E1E4E8>masked </span><span style=--0:#F97583>=</span><span style=--0:#E1E4E8> attn_scores.masked_fill(mask.bool(), </span><span style=--0:#F97583>-</span><span style=--0:#E1E4E8>torch.inf)</span></div></div><div class=ec-line><div class=gutter><div class=ln aria-hidden=true>3</div></div><div class=code><span style=--0:#79B8FF>print</span><span style=--0:#E1E4E8>(masked)</span></div></div><div class=ec-line><div class=gutter><div class=ln aria-hidden=true>4</div></div><div class=code><span style=--0:#E1E4E8>attn_weights </span><span style=--0:#F97583>=</span><span style=--0:#E1E4E8> torch.softmax(masked </span><span style=--0:#F97583>/</span><span style=--0:#E1E4E8> keys.shape[</span><span style=--0:#F97583>-</span><span style=--0:#79B8FF>1</span><span style=--0:#E1E4E8>]</span><span style=--0:#F97583>**</span><span style=--0:#79B8FF>0.5</span><span style=--0:#E1E4E8>, </span><span style=--0:#FFAB70>dim</span><span style=--0:#F97583>=</span><span style=--0:#79B8FF>1</span><span style=--0:#E1E4E8>) </span><span style=--0:#79B8FF>print</span><span style=--0:#E1E4E8>(attn_weights)</span></div></div></code></pre><div class=copy><button title="Copy to clipboard" data-code="mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)masked = attn_scores.masked_fill(mask.bool(), -torch.inf)print(masked)attn_weights = torch.softmax(masked / keys.shape[-1]**0.5, dim=1) print(attn_weights)" data-copied=Copied!><div></div></button></div></figure></div><h3 id=使用dropout遮掩额外的注意力权重>使用<code>dropout</code>遮掩额外的注意力权重</h3><p>Dropout 在深度学习中是一种技术，即在训练过程中随机忽略一些隐藏层单元，实际上将它们“丢弃”。这种方法有助于防止过拟合，确保模型不会过于依赖任何特定的隐藏层单元组合。</p><p><img alt="" src=https://skindhu.github.io/Build-A-Large-Language-Model-CN/Image/chapter3/figure3.22.png></p><p>当对注意力权重矩阵应用 50% 的 dropout 时，矩阵中一半的元素会被随机设置为零。为了补偿有效元素的减少，矩阵中剩余元素的值会被放大 1/0.5 = 2 倍。这个缩放操作至关重要，可以在训练和推理阶段保持注意力机制的整体权重平衡，确保注意力机制在这两个阶段的平均影响保持一致。</p><h3 id=实现一个简洁的因果注意力类>实现一个简洁的因果注意力类</h3><p><code>CausalAttention</code>类添加了<code>dropout</code>和因果掩码组件.</p><h2 id=从单头注意力拓展到多头注意力>从单头注意力拓展到多头注意力</h2><p>多头’一词指的是将注意力机制划分为多个‘头’，每个头独立运作。在这种情况下，单个因果注意力模块可以视为单头注意力，即只有一组注意力权重用于按顺序处理输入。</p><h3 id=堆叠多层单头注意力>堆叠多层单头注意力</h3><p>每个实例都具有独立的权重，然后将它们的输出合并。 它对于识别复杂模式至关重要，这是基于 Transformer 的大语言模型所擅长的能力之一。</p><p><img alt="" src=https://skindhu.github.io/Build-A-Large-Language-Model-CN/Image/chapter3/figure3.24.png></p><p><img alt="" src=https://skindhu.github.io/Build-A-Large-Language-Model-CN/Image/chapter3/figure3.25.png></p><h3 id=通过权重分割实现多头注意力机制>通过权重分割实现多头注意力机制</h3><p><img alt="" src=https://skindhu.github.io/Build-A-Large-Language-Model-CN/Image/chapter3/figure3.26.png></p><p>Q,K,V张量的拆分是通过张量的重塑和转置操作实现的.</p><p>关键操作是将 <code>d_out</code> 维度拆分成 <code>num_heads</code> 和 <code>head_dim</code>，其中 <code>head_dim = d_out / num_heads</code>。这种拆分通过 <code>.view</code> 方法实现：将形状为 <code>(b, num_tokens, d_out)</code> 的张量重塑为 <code>(b, num_tokens, num_heads, head_dim)</code>。</p><p>接下来对张量进行转置操作，将 <code>num_heads</code> 维度移动到 <code>num_tokens</code> 维度之前，使其形状变为 <code>(b, num_heads, num_tokens, head_dim)</code>。这种转置对于在不同注意力头之间正确对齐查询（queries）、键（keys）和值（values），并高效执行批量矩阵乘法至关重要。</p><p>在多头注意力机制中，计算完注意力权重和上下文向量之后，将所有头的上下文向量转置回形状 <code>(b, num_tokens, num_heads, head_dim)</code>。然后将这些向量重新塑形（展平）为 <code>(b, num_tokens, d_out)</code> 的形状，从而有效地将所有头的输出组合在一起。</p><p>此外，我们在多头注意力机制中添加了一个称为输出投影层（self.out_proj）的模块，用于在组合多个头的输出后进行投影。而在因果注意力类中并没有这个投影层。这个输出投影层并非绝对必要（详见附录 B 的参考部分），但由于它在许多 LLM 架构中被广泛使用，因此我们在这里加上以保持完整性。</p><blockquote><p>这个线性层的作用是什么?</p></blockquote><p>最小的 GPT-2 模型（1.17 亿参数）具有 12 个注意力头和 768 的上下文向量嵌入大小。而最大的 GPT-2 模型（15 亿参数）则具有 25 个注意力头和 1600 的上下文向量嵌入大小。请注意，在 GPT 模型中，token 输入的嵌入大小与上下文嵌入大小是相同的（<code>d_in = d_out</code>）。</p><h2 id=本章摘要>本章摘要</h2><blockquote><p>本章的注意力机制是整本书中最重要的内容</p></blockquote><hr data-astro-cid-zux26muy><div class="container p-0" data-astro-cid-zux26muy><div class="text-sm italic mb-2 text-base-content/60 text-right" data-astro-cid-zux26muy><svg height=1em width=1em data-icon=ri:heart-line class="h-4 w-4 align-text-bottom inline-block text-error" data-astro-cid-zux26muy=true><symbol id=ai:ri:heart-line viewBox="0 0 24 24"><path d="M12.001 4.529a6 6 0 0 1 8.242.228a6 6 0 0 1 .236 8.236l-8.48 8.492l-8.478-8.492a6 6 0 0 1 8.48-8.464m6.826 1.641a4 4 0 0 0-5.49-.153l-1.335 1.198l-1.336-1.197a4 4 0 0 0-5.686 5.605L12 18.654l7.02-7.03a4 4 0 0 0-.193-5.454" fill=currentColor /></symbol><use href=#ai:ri:heart-line></use></svg> Thanks for reading!</div><div class="bg-base-200 card overflow-visible" data-astro-cid-zux26muy><div class="relative p-4 card-body lg:p-8 sm:p-6" data-astro-cid-zux26muy><div class="-top-8 absolute left-8" data-astro-cid-zux26muy><div class="flex items-center justify-center bg-primary h-16 rounded-full shadow-lg w-16" data-astro-cid-zux26muy><svg height=1em width=1em data-icon=ri:creative-commons-line class="h-10 text-primary-content w-10" data-astro-cid-zux26muy=true><symbol id=ai:ri:creative-commons-line viewBox="0 0 24 24"><path d="M9 8c1.104 0 2.105.448 2.829 1.173l-1.414 1.413a2 2 0 1 0 0 2.828l1.413 1.414A4.001 4.001 0 0 1 5 12c0-2.208 1.792-4 4-4m9.829 1.173A4.001 4.001 0 0 0 12 12a4.001 4.001 0 0 0 6.828 2.828l-1.414-1.414a2 2 0 1 1 0-2.828zM2 12C2 6.477 6.477 2 12 2s10 4.477 10 10s-4.477 10-10 10S2 17.523 2 12m10-8a8 8 0 1 0 0 16a8 8 0 0 0 0-16" fill=currentColor /></symbol><use href=#ai:ri:creative-commons-line></use></svg></div></div><div class=space-y-4 data-astro-cid-zux26muy><h3 class="sm:text-xl font-bold text-lg lg:text-2xl" data-astro-cid-zux26muy>实现注意力机制 | LLM</h3><div class="flex flex-wrap gap-2 opacity-75 sm:gap-4 sm:text-sm text-xs" data-astro-cid-zux26muy><span class="flex items-center gap-1" data-astro-cid-zux26muy><svg height=1em width=1em data-icon=lucide:calendar class="h-4 w-4" viewBox="0 0 24 24" data-astro-cid-zux26muy=true><use href=#ai:lucide:calendar></use></svg> Mon Sep 22 2025</span><div class="flex items-center gap-1" data-astro-cid-zux26muy><svg height=1em width=1em data-icon=lucide:book-open class="h-4 w-4" viewBox="0 0 24 24" data-astro-cid-zux26muy=true><use href=#ai:lucide:book-open></use></svg> <span data-astro-cid-zux26muy>3182 words · 12 minutes</span></div><a href=https://tankimzeg.top/llm/ch3_attention-machine class="flex items-center gap-1 hover:text-primary transition-colors" data-astro-cid-zux26muy><svg height=1em width=1em data-icon=ri:links-line class="h-4 w-4" data-astro-cid-zux26muy=true><symbol id=ai:ri:links-line viewBox="0 0 24 24"><path d="m13.06 8.111l1.415 1.414a7 7 0 0 1 0 9.9l-.354.353a7 7 0 1 1-9.9-9.9l1.415 1.415a5 5 0 1 0 7.071 7.071l.354-.354a5 5 0 0 0 0-7.07l-1.415-1.415zm6.718 6.01l-1.414-1.414a5 5 0 0 0-7.071-7.07l-.354.353a5 5 0 0 0 0 7.07l1.415 1.415l-1.415 1.414l-1.414-1.414a7 7 0 0 1 0-9.9l.354-.353a7 7 0 1 1 9.9 9.9" fill=currentColor /></symbol><use href=#ai:ri:links-line></use></svg></a></div><div class=mt-4 data-astro-cid-zux26muy><div class="flex items-center flex-wrap gap-2"><a href=/blog/category/tech class="btn btn-xs btn-category"><svg height=1em width=1em data-icon=lucide:folder class="h-4 w-4" viewBox="0 0 24 24"><use href=#ai:lucide:folder></use></svg> <span>tech</span> </a><a href=/blog/tag/llm class="btn btn-xs btn-tag"><svg height=1em width=1em data-icon=lucide:tag class="h-4 w-4" viewBox="0 0 24 24"><use href=#ai:lucide:tag></use></svg> <span>llm</span></a></div><hr data-astro-cid-zux26muy class=my-4><p class="text-sm opacity-75" data-astro-cid-zux26muy></p>© TanKimzeg | <a href=https://creativecommons.org/licenses/by-nc-sa/4.0/deed class="hover:text-primary transition-colors" target=_blank data-astro-cid-zux26muy rel="noopener noreferrer">CC BY-NC-SA 4.0</a></div></div><div class="flex justify-end mt-4" data-astro-cid-zux26muy><button class="btn btn-outline btn-primary" data-astro-cid-zux26muy onclick=share_modal.showModal()>Share <svg height=1em width=1em data-icon=ri:share-line class="h-5 w-5" data-astro-cid-zux26muy=true><symbol id=ai:ri:share-line viewBox="0 0 24 24"><path d="m13.12 17.023l-4.199-2.29a4 4 0 1 1 0-5.465l4.2-2.29a4 4 0 1 1 .958 1.755l-4.2 2.29a4 4 0 0 1 0 1.954l4.2 2.29a4 4 0 1 1-.959 1.755M6 14a2 2 0 1 0 0-4a2 2 0 0 0 0 4m11-6a2 2 0 1 0 0-4a2 2 0 0 0 0 4m0 12a2 2 0 1 0 0-4a2 2 0 0 0 0 4" fill=currentColor /></symbol><use href=#ai:ri:share-line></use></svg></button></div></div></div><dialog class="modal modal-bottom sm:modal-middle" id=share_modal data-astro-cid-zux26muy><div class="modal-box max-w-2xl rounded-none sm:rounded-xl" data-astro-cid-zux26muy><h3 class="mb-6 font-bold sm:text-xl text-center text-lg" data-astro-cid-zux26muy>Share this article</h3><div class="flex justify-center flex-wrap gap-4" data-astro-cid-zux26muy><a href="https://twitter.com/intent/tweet/?text=实现注意力机制 | LLM&url=https://tankimzeg.top/llm/ch3_attention-machine" class="transition-transform btn btn-circle btn-lg hover:scale-110 bg-black hover:bg-gray-800" target=_blank data-astro-cid-zux26muy rel="noopener noreferrer" title="X (Twitter)"><span class=sr-only data-astro-cid-zux26muy>X (Twitter)</span> <svg height=1em width=1em data-icon=ri:twitter-x-line class="h-6 text-white w-6" data-astro-cid-zux26muy=true><symbol id=ai:ri:twitter-x-line viewBox="0 0 24 24"><path d="M10.488 14.651L15.25 21h7l-7.858-10.478L20.93 3h-2.65l-5.117 5.886L8.75 3h-7l7.51 10.015L2.32 21h2.65zM16.25 19L5.75 5h2l10.5 14z" fill=currentColor /></symbol><use href=#ai:ri:twitter-x-line></use></svg> </a><a href="https://telegram.me/share/url?text=实现注意力机制 | LLM&url=https://tankimzeg.top/llm/ch3_attention-machine" class="transition-transform btn btn-circle btn-lg hover:scale-110 bg-[#26a5e4] hover:bg-[#1e96d1]" target=_blank data-astro-cid-zux26muy rel="noopener noreferrer" title=Telegram><span class=sr-only data-astro-cid-zux26muy>Telegram</span> <svg height=1em width=1em data-icon=ri:telegram-line class="h-6 text-white w-6" data-astro-cid-zux26muy=true><symbol id=ai:ri:telegram-line viewBox="0 0 24 24"><path d="M20 12a8 8 0 1 1-16 0a8 8 0 0 1 16 0m-8 10c5.523 0 10-4.477 10-10S17.523 2 12 2S2 6.477 2 12s4.477 10 10 10m.358-12.618q-1.458.607-5.831 2.513q-.711.282-.744.552c-.038.304.343.424.862.587l.218.07c.51.166 1.198.36 1.555.368q.486.01 1.084-.4q4.086-2.76 4.218-2.789c.063-.014.149-.032.207.02c.059.052.053.15.047.177c-.038.161-1.534 1.552-2.308 2.271q-.344.324-.683.653c-.474.457-.83.8.02 1.36c.861.568 1.73 1.134 2.57 1.733c.414.296.786.56 1.246.519c.267-.025.543-.276.683-1.026c.332-1.77.983-5.608 1.133-7.19a1.8 1.8 0 0 0-.017-.393a.42.42 0 0 0-.142-.27c-.12-.098-.305-.118-.387-.117c-.376.007-.953.207-3.73 1.362" fill=currentColor /></symbol><use href=#ai:ri:telegram-line></use></svg> </a><a href="https://reddit.com/submit/?url=https://tankimzeg.top/llm/ch3_attention-machine&title=实现注意力机制 | LLM" class="transition-transform btn btn-circle btn-lg hover:scale-110 bg-[#ff4500] hover:bg-[#e63e00]" target=_blank data-astro-cid-zux26muy rel="noopener noreferrer" title=Reddit><span class=sr-only data-astro-cid-zux26muy>Reddit</span> <svg height=1em width=1em data-icon=ri:reddit-line class="h-6 text-white w-6" data-astro-cid-zux26muy=true><symbol id=ai:ri:reddit-line viewBox="0 0 24 24"><path d="m11.053 7.815l.751-3.536a2 2 0 0 1 2.372-1.54l3.196.68a2 2 0 1 1-.415 1.956l-3.197-.68l-.666 3.135c1.785.137 3.558.73 5.164 1.7A3.192 3.192 0 0 1 23 12.203v.021a3.2 3.2 0 0 1-1.207 2.55l-.008.123c0 3.998-4.45 7.03-9.799 7.03c-5.333 0-9.708-3.024-9.705-6.953l-.01-.181a3.193 3.193 0 0 1 3.454-5.35a11.45 11.45 0 0 1 5.329-1.628m9.285 5.526a1.19 1.19 0 0 0 .662-1.075a1.192 1.192 0 0 0-2.016-.806l-.585.56l-.67-.455c-1.615-1.098-3.452-1.725-5.23-1.764h-1.006c-1.875.028-3.652.6-5.237 1.675l-.664.45l-.583-.55a1.192 1.192 0 1 0-1.315 1.952l.633.29l-.053.695a4 4 0 0 0 .003.584c0 2.71 3.356 5.03 7.708 5.03c4.371 0 7.799-2.336 7.802-5.107a3 3 0 0 0 0-.507l-.052-.672zM6.951 13.5a1.5 1.5 0 1 1 3 0a1.5 1.5 0 0 1-3 0m7 0a1.5 1.5 0 1 1 3 0a1.5 1.5 0 0 1-3 0m-1.985 5.103c-1.397 0-2.766-.37-3.881-1.21a.424.424 0 0 1 .597-.597c.945.693 2.123.99 3.269.99s2.33-.275 3.284-.959a.44.44 0 0 1 .732.206a.47.47 0 0 1-.12.423c-.683.797-2.483 1.147-3.88 1.147" fill=currentColor /></symbol><use href=#ai:ri:reddit-line></use></svg> </a><a href="https://facebook.com/sharer/sharer.php?u=https://tankimzeg.top/llm/ch3_attention-machine" class="transition-transform btn btn-circle btn-lg hover:scale-110 bg-[#0866ff] hover:bg-[#0755d6]" target=_blank data-astro-cid-zux26muy rel="noopener noreferrer" title=Facebook><span class=sr-only data-astro-cid-zux26muy>Facebook</span> <svg height=1em width=1em data-icon=ri:facebook-circle-line class="h-6 text-white w-6" data-astro-cid-zux26muy=true><symbol id=ai:ri:facebook-circle-line viewBox="0 0 24 24"><path d="M13.001 19.938a8.001 8.001 0 0 0-1-15.938a8 8 0 0 0-1 15.938V14h-2v-2h2v-1.654c0-1.337.14-1.822.4-2.311A2.73 2.73 0 0 1 12.537 6.9c.382-.205.857-.328 1.687-.381q.494-.032 1.278.08v1.9h-.5c-.917 0-1.296.043-1.522.164a.73.73 0 0 0-.314.314c-.12.226-.164.45-.164 1.368V12h2.5l-.5 2h-2zm-1 2.062c-5.523 0-10-4.477-10-10s4.477-10 10-10s10 4.477 10 10s-4.477 10-10 10" fill=currentColor /></symbol><use href=#ai:ri:facebook-circle-line></use></svg> </a><a href="mailto:?subject=实现注意力机制 | LLM&#38;body=https://tankimzeg.top/llm/ch3_attention-machine" class="transition-transform btn btn-circle btn-lg hover:scale-110 bg-gray-600 hover:bg-gray-700" target=_blank data-astro-cid-zux26muy rel="noopener noreferrer" title=Email><span class=sr-only data-astro-cid-zux26muy>Email</span> <svg height=1em width=1em data-icon=ri:mail-line class="h-6 text-white w-6" data-astro-cid-zux26muy=true><symbol id=ai:ri:mail-line viewBox="0 0 24 24"><path d="M3 3h18a1 1 0 0 1 1 1v16a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V4a1 1 0 0 1 1-1m17 4.238l-7.928 7.1L4 7.216V19h16zM4.511 5l7.55 6.662L19.502 5z" fill=currentColor /></symbol><use href=#ai:ri:mail-line></use></svg></a></div><div class=modal-action data-astro-cid-zux26muy><form method=dialog data-astro-cid-zux26muy><button class="transition-transform btn btn-ghost hover:scale-105" data-astro-cid-zux26muy>Close</button></form></div></div><form method=dialog class=modal-backdrop data-astro-cid-zux26muy><button data-astro-cid-zux26muy>Close</button></form></dialog></div></div></div></div></div><script type=module>document.addEventListener("astro:page-load",(()=>{const t=document.querySelectorAll(".dropdown label"),e=document.querySelectorAll(".dropdown-content");t.forEach(((t,a)=>{t.addEventListener("click",(()=>{const t=e[a];"closed"===t.getAttribute("data-state")?(t.setAttribute("data-state","open"),t.classList.add("scale-100","opacity-100"),t.classList.remove("scale-90","opacity-0")):(t.setAttribute("data-state","closed"),t.classList.add("scale-90","opacity-0"),t.classList.remove("scale-100","opacity-100"))}))}))}))</script></div><footer class="rounded-xl bg-base-100 shadow-lg footer p-10"><aside><svg height=50 width=50 class=fill-current viewBox="0 0 24 24" clip-rule=evenodd fill-rule=evenodd xmlns=http://www.w3.org/2000/svg><path d="M22.672 15.226l-2.432.811.841 2.515c.33 1.019-.209 2.127-1.23 2.456-1.15.325-2.148-.321-2.463-1.226l-.84-2.518-5.013 1.677.84 2.517c.391 1.203-.434 2.542-1.831 2.542-.88 0-1.601-.564-1.86-1.314l-.842-2.516-2.431.809c-1.135.328-2.145-.317-2.463-1.229-.329-1.018.211-2.127 1.231-2.456l2.432-.809-1.621-4.823-2.432.808c-1.355.384-2.558-.59-2.558-1.839 0-.817.509-1.582 1.327-1.846l2.433-.809-.842-2.515c-.33-1.02.211-2.129 1.232-2.458 1.02-.329 2.13.209 2.461 1.229l.842 2.515 5.011-1.677-.839-2.517c-.403-1.238.484-2.553 1.843-2.553.819 0 1.585.509 1.85 1.326l.841 2.517 2.431-.81c1.02-.33 2.131.211 2.461 1.229.332 1.018-.21 2.126-1.23 2.456l-2.433.809 1.622 4.823 2.433-.809c1.242-.401 2.557.484 2.557 1.838 0 .819-.51 1.583-1.328 1.847m-8.992-6.428l-5.01 1.675 1.619 4.828 5.011-1.674-1.62-4.829z"></path></svg><p>Powered by <a href=https://github.com/EveSunMaple/Frosti class=font-bold target=_blank>Frosti Template</a><br>Copyright © <a href=https://tankimzeg.top class=font-bold target=_blank>TanKimzeg</a> 2025 - All right reserved</p></aside><nav><span class=footer-title>Social</span><div class="gap-4 grid grid-flow-col"><div class=tooltip data-tip=Github><a href=https://github.com/TanKimzeg aria-label=Github tabindex=0><svg height=1em width=1em data-icon=ri:github-line class="md:text-xl text-2xl"><symbol id=ai:ri:github-line viewBox="0 0 24 24"><path d="M5.884 18.653c-.3-.2-.558-.455-.86-.816a51 51 0 0 1-.466-.579c-.463-.575-.755-.841-1.056-.95a1 1 0 1 1 .675-1.882c.752.27 1.261.735 1.947 1.588c-.094-.117.34.427.433.539c.19.227.33.365.44.438c.204.137.588.196 1.15.14c.024-.382.094-.753.202-1.095c-2.968-.726-4.648-2.64-4.648-6.396c0-1.24.37-2.356 1.058-3.292c-.218-.894-.185-1.975.302-3.192a1 1 0 0 1 .63-.582c.081-.024.127-.035.208-.047c.803-.124 1.937.17 3.415 1.096a11.7 11.7 0 0 1 2.687-.308c.912 0 1.819.104 2.684.308c1.477-.933 2.614-1.227 3.422-1.096q.128.02.218.05a1 1 0 0 1 .616.58c.487 1.216.52 2.296.302 3.19c.691.936 1.058 2.045 1.058 3.293c0 3.757-1.674 5.665-4.642 6.392c.125.415.19.878.19 1.38c0 .665-.002 1.299-.007 2.01c0 .19-.002.394-.005.706a1 1 0 0 1-.018 1.958c-1.14.227-1.984-.532-1.984-1.525l.002-.447l.005-.705c.005-.707.008-1.337.008-1.997c0-.697-.184-1.152-.426-1.361c-.661-.57-.326-1.654.541-1.751c2.966-.333 4.336-1.482 4.336-4.66c0-.955-.312-1.744-.913-2.404A1 1 0 0 1 17.2 6.19c.166-.414.236-.957.095-1.614l-.01.003c-.491.139-1.11.44-1.858.949a1 1 0 0 1-.833.135a9.6 9.6 0 0 0-2.592-.349c-.89 0-1.772.118-2.592.35a1 1 0 0 1-.829-.134c-.753-.507-1.374-.807-1.87-.947c-.143.653-.072 1.194.093 1.607a1 1 0 0 1-.189 1.045c-.597.655-.913 1.458-.913 2.404c0 3.172 1.371 4.328 4.322 4.66c.865.097 1.202 1.177.545 1.748c-.193.168-.43.732-.43 1.364v3.15c0 .985-.834 1.725-1.96 1.528a1 1 0 0 1-.04-1.962v-.99c-.91.061-1.661-.088-2.254-.485" fill=currentColor /></symbol><use href=#ai:ri:github-line></use></svg></a></div><div class=tooltip data-tip=Zhihu><a href=https://www.zhihu.com/people/Dalekov aria-label=Zhihu tabindex=0><svg height=1em width=1em data-icon=ri:zhihu-line class="md:text-xl text-2xl"><symbol id=ai:ri:zhihu-line viewBox="0 0 24 24"><path d="m12.345 17.963l-1.688 1.074l-2.132-3.35c-.44 1.402-1.171 2.665-2.138 3.825c-.402.483-.82.918-1.301 1.376c-.155.146-.775.716-.878.82l-1.414-1.415c.139-.139.787-.735.914-.856c.43-.408.796-.79 1.143-1.205C6.117 16.712 6.88 15.02 6.988 13H3v-2h4V7h-.868c-.689 1.266-1.558 2.222-2.618 2.858L2.486 8.143c1.396-.838 2.426-2.603 3.039-5.36l1.952.434q-.21.95-.489 1.783h4.513v2H9v4h2.5v2H9.186zm3.838-.07L17.3 17h1.702V7h-4v10h.736zM13.001 5h8v14h-3l-2.5 2l-1-2H13z" fill=currentColor /></symbol><use href=#ai:ri:zhihu-line></use></svg></a></div><div class=tooltip data-tip=BiliBili><a href=https://space.bilibili.com/3493121543375815 aria-label=BiliBili tabindex=0><svg height=1em width=1em data-icon=ri:bilibili-line class="md:text-xl text-2xl"><symbol id=ai:ri:bilibili-line viewBox="0 0 24 24"><path d="M7.172 2.757L10.414 6h3.171l3.243-3.242a1 1 0 1 1 1.415 1.415L16.414 6H18.5A3.5 3.5 0 0 1 22 9.5v8a3.5 3.5 0 0 1-3.5 3.5h-13A3.5 3.5 0 0 1 2 17.5v-8A3.5 3.5 0 0 1 5.5 6h2.085L5.757 4.171a1 1 0 0 1 1.415-1.415M18.5 8h-13a1.5 1.5 0 0 0-1.493 1.356L4 9.5v8a1.5 1.5 0 0 0 1.356 1.493L5.5 19h13a1.5 1.5 0 0 0 1.493-1.355L20 17.5v-8A1.5 1.5 0 0 0 18.5 8M8 11a1 1 0 0 1 1 1v2a1 1 0 1 1-2 0v-2a1 1 0 0 1 1-1m8 0a1 1 0 0 1 1 1v2a1 1 0 1 1-2 0v-2a1 1 0 0 1 1-1" fill=currentColor /></symbol><use href=#ai:ri:bilibili-line></use></svg></a></div><div class=tooltip data-tip="RSS Feed"><a href=/rss.xml aria-label="RSS Feed" tabindex=0><svg height=1em width=1em data-icon=ri:rss-line class="md:text-xl text-2xl"><symbol id=ai:ri:rss-line viewBox="0 0 24 24"><path d="M3 17a4 4 0 0 1 4 4H3zm0-7c6.075 0 11 4.925 11 11h-2a9 9 0 0 0-9-9zm0-7c9.941 0 18 8.059 18 18h-2c0-8.837-7.163-16-16-16z" fill=currentColor /></symbol><use href=#ai:ri:rss-line></use></svg></a></div></div></nav></footer></main><aside class="md:top-4 bg-transparent col-span-1 md:order-1 order-2"><div class="relative mb-4"><div class="w-full bg-base-100 shadow-lg rounded-xl"><div class="flex relative flex-col p-4"><div class="flex justify-center p-2"><img alt=Profile src=/profile.jpg class="mask mask-circle" decoding=async fetchpriority=auto height=250 loading=eager width=250></div><ul class="items-center bg-transparent flex-col hidden lg:items-start m-0 md:flex menu p-0 w-full"><li class="relative w-full group"><a href=/ class="flex items-center rounded-lg hover:bg-base-200 font-bold justify-center lg:justify-start lg:text-xl md:text-3xl p-4 text-center text-xl" target=_self aria-label=Home tabindex=0 id=home><svg height=1em width=1em data-icon=material-symbols:home-outline-rounded viewBox="0 0 24 24"><use href=#ai:material-symbols:home-outline-rounded></use></svg> <span class="hidden lg:inline ml-2">Home</span></a></li><li class="relative w-full group"><a href=/about class="flex items-center rounded-lg hover:bg-base-200 font-bold justify-center lg:justify-start lg:text-xl md:text-3xl p-4 text-center text-xl" target=_self aria-label=About tabindex=0 id=about><svg height=1em width=1em data-icon=material-symbols:info-outline-rounded viewBox="0 0 24 24"><use href=#ai:material-symbols:info-outline-rounded></use></svg> <span class="hidden lg:inline ml-2">About</span></a></li><li class="relative w-full group"><details open class="w-full menu-item" data-submenu-id=blog><summary class="items-center rounded-lg hover:bg-base-200 font-bold justify-center lg:justify-start lg:text-xl md:text-3xl p-4 text-center text-xl"><svg height=1em width=1em data-icon=material-symbols:book-2-outline-rounded viewBox="0 0 24 24"><use href=#ai:material-symbols:book-2-outline-rounded></use></svg> <span class="hidden lg:inline ml-2">Blogs</span></summary><ul class=rounded-lg><li class=relative><a href=/blog class="p-2 hover:bg-base-200 rounded-lg font-bold lg:text-base md:text-2xl menu-item text-base" target=_self aria-label="All blogs" tabindex=0 id=header-all><svg height=1em width=1em data-icon=material-symbols:ink-pen-outline-rounded viewBox="0 0 24 24"><use href=#ai:material-symbols:ink-pen-outline-rounded></use></svg> <span class="hidden lg:inline ml-2">All blogs</span></a></li><li class=relative><a href=/blog/category/tech class="p-2 hover:bg-base-200 rounded-lg font-bold lg:text-base md:text-2xl menu-item text-base" target=_self aria-label="Tech Blogs" tabindex=0 id=header-tech><svg height=1em width=1em data-icon=material-symbols:deployed-code-outline viewBox="0 0 24 24"><use href=#ai:material-symbols:deployed-code-outline></use></svg> <span class="hidden lg:inline ml-2">Tech Blogs</span></a></li><li class=relative><a href=/blog/category/life class="p-2 hover:bg-base-200 rounded-lg font-bold lg:text-base md:text-2xl menu-item text-base" target=_self aria-label="Life Blogs" tabindex=0 id=header-life><svg height=1em width=1em data-icon=material-symbols:earthquake-rounded viewBox="0 0 24 24"><use href=#ai:material-symbols:earthquake-rounded></use></svg> <span class="hidden lg:inline ml-2">Life Blogs</span></a></li></ul></details></li><li class="relative w-full group"><a href=/project class="flex items-center rounded-lg hover:bg-base-200 font-bold justify-center lg:justify-start lg:text-xl md:text-3xl p-4 text-center text-xl" target=_self aria-label=Project tabindex=0 id=project><svg height=1em width=1em data-icon=material-symbols:code-blocks-outline viewBox="0 0 24 24"><use href=#ai:material-symbols:code-blocks-outline></use></svg> <span class="hidden lg:inline ml-2">Project</span></a></li><li class="relative w-full group"><a href=/friend class="flex items-center rounded-lg hover:bg-base-200 font-bold justify-center lg:justify-start lg:text-xl md:text-3xl p-4 text-center text-xl" target=_self aria-label=Friend tabindex=0 id=friend><svg height=1em width=1em data-icon=material-symbols:supervisor-account-outline-rounded viewBox="0 0 24 24"><use href=#ai:material-symbols:supervisor-account-outline-rounded></use></svg> <span class="hidden lg:inline ml-2">Friend</span></a></li></ul><div class="mt-4 border-base-content/20 border-t pt-3"><div class="justify-items-center gap-2 grid grid-cols-4 lg:grid-cols-4 md:grid-cols-2"><div class="tooltip tooltip-bottom" data-tip=Github><a href=https://github.com/TanKimzeg class="btn btn-circle btn-ghost" target=_blank aria-label=Github tabindex=0><svg height=1em width=1em data-icon=ri:github-line class=text-xl viewBox="0 0 24 24"><use href=#ai:ri:github-line></use></svg></a></div><div class="tooltip tooltip-bottom" data-tip=Zhihu><a href=https://www.zhihu.com/people/Dalekov class="btn btn-circle btn-ghost" target=_blank aria-label=Zhihu tabindex=0><svg height=1em width=1em data-icon=ri:zhihu-line class=text-xl viewBox="0 0 24 24"><use href=#ai:ri:zhihu-line></use></svg></a></div><div class="tooltip tooltip-bottom" data-tip=BiliBili><a href=https://space.bilibili.com/3493121543375815 class="btn btn-circle btn-ghost" target=_blank aria-label=BiliBili tabindex=0><svg height=1em width=1em data-icon=ri:bilibili-line class=text-xl viewBox="0 0 24 24"><use href=#ai:ri:bilibili-line></use></svg></a></div><div class="tooltip tooltip-bottom" data-tip="RSS Feed"><a href=https://tankimzeg.top/rss.xml class="btn btn-circle btn-ghost" target=_blank aria-label="RSS Feed" tabindex=0><svg height=1em width=1em data-icon=ri:rss-line class=text-xl viewBox="0 0 24 24"><use href=#ai:ri:rss-line></use></svg></a></div></div></div></div></div></div><div class="relative mb-4"><div class="w-full bg-base-100 shadow-lg rounded-xl"><div class=p-4><form method=get class="relative w-full" action=/blog/search><div class="relative w-full search-container"><input type=text aria-label=Search class="w-full input input-bordered pl-10 pr-16 py-2" name=q placeholder=Search><div class="flex items-center absolute inset-y-0 left-0 pl-3 pointer-events-none"><svg height=1em width=1em data-icon=lucide:search class="h-5 w-5"><symbol id=ai:lucide:search viewBox="0 0 24 24"><g fill=none stroke=currentColor stroke-linecap=round stroke-linejoin=round stroke-width=2><path d="m21 21l-4.34-4.34"/><circle cx=11 cy=11 r=8 /></g></symbol><use href=#ai:lucide:search></use></svg></div><button class="p-2 -translate-y-1/2 absolute right-2 top-1/2 transform" aria-label=Search type=submit><svg height=1em width=1em data-icon=lucide:arrow-right class="h-4 w-4"><symbol id=ai:lucide:arrow-right viewBox="0 0 24 24"><path d="M5 12h14m-7-7l7 7l-7 7" fill=none stroke=currentColor stroke-linecap=round stroke-linejoin=round stroke-width=2 /></symbol><use href=#ai:lucide:arrow-right></use></svg></button></div></form></div></div><script type=module>document.addEventListener("keydown",(e=>{const t=e.target;if("/"===e.key&&t&&"INPUT"!==t.tagName&&"TEXTAREA"!==t.tagName){e.preventDefault();const t=document.querySelector('input[name="q"]');t&&t.focus()}}))</script></div><div class="md:top-4 md:sticky"><div class="w-full bg-base-100 shadow-lg rounded-xl"><div class="toolbar-container p-4"><div class="items-center justify-center flex-wrap gap-4 grid grid-cols-4 lg:grid-cols-4 md:grid-cols-2"><a href=/blog/tags class="btn btn-circle hover:scale-110 bg-base-100 btn-md shadow-sm border-base-content/20" aria-label=Tag title=Tag><svg height=1em width=1em data-icon=lucide:tag class="h-5 w-5"><symbol id=ai:lucide:tag viewBox="0 0 24 24"><g fill=none stroke=currentColor stroke-linecap=round stroke-linejoin=round stroke-width=2><path d="M12.586 2.586A2 2 0 0 0 11.172 2H4a2 2 0 0 0-2 2v7.172a2 2 0 0 0 .586 1.414l8.704 8.704a2.426 2.426 0 0 0 3.42 0l6.58-6.58a2.426 2.426 0 0 0 0-3.42z"/><circle cx=7.5 cy=7.5 r=.5 fill=currentColor /></g></symbol><use href=#ai:lucide:tag></use></svg> </a><a href=/blog/categories class="btn btn-circle hover:scale-110 bg-base-100 btn-md shadow-sm border-base-content/20" aria-label=Category title=Category><svg height=1em width=1em data-icon=lucide:folder class="h-5 w-5"><symbol id=ai:lucide:folder viewBox="0 0 24 24"><path d="M20 20a2 2 0 0 0 2-2V8a2 2 0 0 0-2-2h-7.9a2 2 0 0 1-1.69-.9L9.6 3.9A2 2 0 0 0 7.93 3H4a2 2 0 0 0-2 2v13a2 2 0 0 0 2 2Z" fill=none stroke=currentColor stroke-linecap=round stroke-linejoin=round stroke-width=2 /></symbol><use href=#ai:lucide:folder></use></svg> </a><a href=/blog/archives class="btn btn-circle hover:scale-110 bg-base-100 btn-md shadow-sm border-base-content/20" aria-label=Archives title=Archives><svg height=1em width=1em data-icon=lucide:archive class="h-5 w-5"><symbol id=ai:lucide:archive viewBox="0 0 24 24"><g fill=none stroke=currentColor stroke-linecap=round stroke-linejoin=round stroke-width=2><rect height=5 rx=1 width=20 x=2 y=3 /><path d="M4 8v11a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8m-10 4h4"/></g></symbol><use href=#ai:lucide:archive></use></svg> </a><button title="Theme Toggle" aria-label="Toggle theme" class="btn btn-circle hover:scale-110 bg-base-100 btn-md shadow-sm md:border-base-content/20 sidebar-theme" data-astro-cid-mjqc4hpp data-theme-toggle id=theme-toggle-keiabxjm1><svg height=1em width=1em data-icon=lucide:sun class="h-5 w-5 theme-toggle-icon sun-icon" viewBox="0 0 24 24" data-astro-cid-mjqc4hpp=true><use href=#ai:lucide:sun></use></svg> <svg height=1em width=1em data-icon=lucide:moon class="h-5 w-5 theme-toggle-icon hidden moon-icon" viewBox="0 0 24 24" data-astro-cid-mjqc4hpp=true><use href=#ai:lucide:moon></use></svg></button><script>!function(){const e="winter",t="dracula",n=(e,t)=>{if(!e)return;const n=e.querySelector(".sun-icon"),d=e.querySelector(".moon-icon");n&&d&&(t?(n.classList.remove("hidden"),d.classList.add("hidden")):(n.classList.add("hidden"),d.classList.remove("hidden")))};document.addEventListener("astro:page-load",(()=>{const d=document.getElementById("theme-toggle-keiabxjm1");if(!d)return;const o=document.documentElement.getAttribute("data-theme");n(d,o===t),d.addEventListener("click",(()=>{const o=document.documentElement.getAttribute("data-theme")===e?t:e;d.classList.add("animate-spin-once"),document.documentElement.setAttribute("data-theme",o);const c=o===t?"dark":"light";document.documentElement.setAttribute("data-theme-type",c),localStorage.setItem("theme",o);document.querySelectorAll("[data-theme-toggle]").forEach((e=>{n(e,o===t)})),setTimeout((()=>{d.classList.remove("animate-spin-once")}),300)}))}))}()</script></div></div></div><button class="btn btn-circle hover:scale-110 bg-base-100 btn-md fixed invisible opacity-0 right-6 shadow-lg transition-all z-50 bottom-6" aria-label="Scroll to top" id=scroll-to-top><svg height=1em width=1em data-icon=material-symbols:arrow-upward-rounded class="h-5 w-5"><symbol id=ai:material-symbols:arrow-upward-rounded viewBox="0 0 24 24"><path d="m11 7.825l-4.9 4.9q-.3.3-.7.288t-.7-.313q-.275-.3-.288-.7t.288-.7l6.6-6.6q.15-.15.325-.212T12 4.425t.375.063t.325.212l6.6 6.6q.275.275.275.688t-.275.712q-.3.3-.712.3t-.713-.3L13 7.825V19q0 .425-.288.713T12 20t-.712-.288T11 19z" fill=currentColor /></symbol><use href=#ai:material-symbols:arrow-upward-rounded></use></svg></button><script type=module>document.addEventListener("astro:page-load",(()=>{const i=document.getElementById("scroll-to-top");if(i){const s=()=>{window.scrollY>300?(i.classList.remove("opacity-0","invisible"),i.classList.add("opacity-100","visible")):(i.classList.remove("opacity-100","visible"),i.classList.add("opacity-0","invisible"))};s(),window.addEventListener("scroll",s),i.addEventListener("click",(()=>{window.scrollTo({top:0,behavior:"smooth"})}))}}))</script><div class="mt-4 md:max-h-[calc(100vh-2rem)] z-10" id=toc-container data-astro-cid-ydxbofl4><div class="w-full bg-base-100 shadow-lg rounded-xl"><div class=p-4 data-astro-cid-ydxbofl4><nav class="relative max-h-[calc(100vh-200px)] overflow-y-auto scrollbar-none toc-nav" data-astro-cid-ydxbofl4><div class="shadow-sm absolute bg-primary/10 opacity-0 pointer-events-none rounded-lg z-0" id=active-indicator data-astro-cid-ydxbofl4></div><ul class="relative space-y-0 z-10" data-astro-cid-ydxbofl4><li data-astro-cid-ydxbofl4><a href=#heading-0 class="flex items-center rounded-lg px-2 py-2 relative toc-link" style=padding-left:28px data-astro-cid-ydxbofl4 data-heading-depth=2 data-heading-slug=长序列建模的问题 data-index=0><svg height=1em width=1em data-icon=tabler:chevron-right class="duration-200 absolute h-4 icon-indicator left-1 opacity-0 text-primary transition-all w-4" data-astro-cid-ydxbofl4=true><symbol id=ai:tabler:chevron-right viewBox="0 0 24 24"><path d="m9 6l6 6l-6 6" fill=none stroke=currentColor stroke-linecap=round stroke-linejoin=round stroke-width=2 /></symbol><use href=#ai:tabler:chevron-right></use></svg><span class="duration-200 link-text transition-transform" data-astro-cid-ydxbofl4>长序列建模的问题</span></a></li><li data-astro-cid-ydxbofl4><a href=#heading-1 class="flex items-center rounded-lg px-2 py-2 relative toc-link" style=padding-left:44px data-astro-cid-ydxbofl4 data-heading-depth=3 data-heading-slug=循环神经网络rnn data-index=1><svg height=1em width=1em data-icon=tabler:chevron-right class="duration-200 absolute h-4 icon-indicator left-1 opacity-0 text-primary transition-all w-4" viewBox="0 0 24 24" data-astro-cid-ydxbofl4=true><use href=#ai:tabler:chevron-right></use></svg><span class="duration-200 link-text transition-transform" data-astro-cid-ydxbofl4>循环神经网络(RNN)</span></a></li><li data-astro-cid-ydxbofl4><a href=#heading-2 class="flex items-center rounded-lg px-2 py-2 relative toc-link" style=padding-left:44px data-astro-cid-ydxbofl4 data-heading-depth=3 data-heading-slug=注意力机制的解决方法 data-index=2><svg height=1em width=1em data-icon=tabler:chevron-right class="duration-200 absolute h-4 icon-indicator left-1 opacity-0 text-primary transition-all w-4" viewBox="0 0 24 24" data-astro-cid-ydxbofl4=true><use href=#ai:tabler:chevron-right></use></svg><span class="duration-200 link-text transition-transform" data-astro-cid-ydxbofl4>注意力机制的解决方法</span></a></li><li data-astro-cid-ydxbofl4><a href=#heading-3 class="flex items-center rounded-lg px-2 py-2 relative toc-link" style=padding-left:28px data-astro-cid-ydxbofl4 data-heading-depth=2 data-heading-slug=通过注意力机制捕捉数据依赖关系 data-index=3><svg height=1em width=1em data-icon=tabler:chevron-right class="duration-200 absolute h-4 icon-indicator left-1 opacity-0 text-primary transition-all w-4" viewBox="0 0 24 24" data-astro-cid-ydxbofl4=true><use href=#ai:tabler:chevron-right></use></svg><span class="duration-200 link-text transition-transform" data-astro-cid-ydxbofl4>通过注意力机制捕捉数据依赖关系</span></a></li><li data-astro-cid-ydxbofl4><a href=#heading-4 class="flex items-center rounded-lg px-2 py-2 relative toc-link" style=padding-left:28px data-astro-cid-ydxbofl4 data-heading-depth=2 data-heading-slug=通过自注意力机制关注输入的不同部分 data-index=4><svg height=1em width=1em data-icon=tabler:chevron-right class="duration-200 absolute h-4 icon-indicator left-1 opacity-0 text-primary transition-all w-4" viewBox="0 0 24 24" data-astro-cid-ydxbofl4=true><use href=#ai:tabler:chevron-right></use></svg><span class="duration-200 link-text transition-transform" data-astro-cid-ydxbofl4>通过自注意力机制关注输入的不同部分</span></a></li><li data-astro-cid-ydxbofl4><a href=#heading-5 class="flex items-center rounded-lg px-2 py-2 relative toc-link" style=padding-left:44px data-astro-cid-ydxbofl4 data-heading-depth=3 data-heading-slug=一种不含可训练权重的简化自注意力机制 data-index=5><svg height=1em width=1em data-icon=tabler:chevron-right class="duration-200 absolute h-4 icon-indicator left-1 opacity-0 text-primary transition-all w-4" viewBox="0 0 24 24" data-astro-cid-ydxbofl4=true><use href=#ai:tabler:chevron-right></use></svg><span class="duration-200 link-text transition-transform" data-astro-cid-ydxbofl4>一种不含可训练权重的简化自注意力机制</span></a></li><li data-astro-cid-ydxbofl4><a href=#heading-6 class="flex items-center rounded-lg px-2 py-2 relative toc-link" style=padding-left:44px data-astro-cid-ydxbofl4 data-heading-depth=3 data-heading-slug=为所有输入的token计算注意力权重 data-index=6><svg height=1em width=1em data-icon=tabler:chevron-right class="duration-200 absolute h-4 icon-indicator left-1 opacity-0 text-primary transition-all w-4" viewBox="0 0 24 24" data-astro-cid-ydxbofl4=true><use href=#ai:tabler:chevron-right></use></svg><span class="duration-200 link-text transition-transform" data-astro-cid-ydxbofl4>为所有输入的token计算注意力权重</span></a></li><li data-astro-cid-ydxbofl4><a href=#heading-7 class="flex items-center rounded-lg px-2 py-2 relative toc-link" style=padding-left:28px data-astro-cid-ydxbofl4 data-heading-depth=2 data-heading-slug=实现带有可训练权重的自注意力机制 data-index=7><svg height=1em width=1em data-icon=tabler:chevron-right class="duration-200 absolute h-4 icon-indicator left-1 opacity-0 text-primary transition-all w-4" viewBox="0 0 24 24" data-astro-cid-ydxbofl4=true><use href=#ai:tabler:chevron-right></use></svg><span class="duration-200 link-text transition-transform" data-astro-cid-ydxbofl4>实现带有可训练权重的自注意力机制</span></a></li><li data-astro-cid-ydxbofl4><a href=#heading-8 class="flex items-center rounded-lg px-2 py-2 relative toc-link" style=padding-left:44px data-astro-cid-ydxbofl4 data-heading-depth=3 data-heading-slug=逐步计算注意力权重 data-index=8><svg height=1em width=1em data-icon=tabler:chevron-right class="duration-200 absolute h-4 icon-indicator left-1 opacity-0 text-primary transition-all w-4" viewBox="0 0 24 24" data-astro-cid-ydxbofl4=true><use href=#ai:tabler:chevron-right></use></svg><span class="duration-200 link-text transition-transform" data-astro-cid-ydxbofl4>逐步计算注意力权重</span></a></li><li data-astro-cid-ydxbofl4><a href=#heading-9 class="flex items-center rounded-lg px-2 py-2 relative toc-link" style=padding-left:44px data-astro-cid-ydxbofl4 data-heading-depth=3 data-heading-slug=实现一个简洁的子注意力机制python类 data-index=9><svg height=1em width=1em data-icon=tabler:chevron-right class="duration-200 absolute h-4 icon-indicator left-1 opacity-0 text-primary transition-all w-4" viewBox="0 0 24 24" data-astro-cid-ydxbofl4=true><use href=#ai:tabler:chevron-right></use></svg><span class="duration-200 link-text transition-transform" data-astro-cid-ydxbofl4>实现一个简洁的子注意力机制Python类</span></a></li><li data-astro-cid-ydxbofl4><a href=#heading-10 class="flex items-center rounded-lg px-2 py-2 relative toc-link" style=padding-left:28px data-astro-cid-ydxbofl4 data-heading-depth=2 data-heading-slug=使用因果注意力机制来屏蔽后续词 data-index=10><svg height=1em width=1em data-icon=tabler:chevron-right class="duration-200 absolute h-4 icon-indicator left-1 opacity-0 text-primary transition-all w-4" viewBox="0 0 24 24" data-astro-cid-ydxbofl4=true><use href=#ai:tabler:chevron-right></use></svg><span class="duration-200 link-text transition-transform" data-astro-cid-ydxbofl4>使用因果注意力机制来屏蔽后续词</span></a></li><li data-astro-cid-ydxbofl4><a href=#heading-11 class="flex items-center rounded-lg px-2 py-2 relative toc-link" style=padding-left:44px data-astro-cid-ydxbofl4 data-heading-depth=3 data-heading-slug=应用因果注意力掩码 data-index=11><svg height=1em width=1em data-icon=tabler:chevron-right class="duration-200 absolute h-4 icon-indicator left-1 opacity-0 text-primary transition-all w-4" viewBox="0 0 24 24" data-astro-cid-ydxbofl4=true><use href=#ai:tabler:chevron-right></use></svg><span class="duration-200 link-text transition-transform" data-astro-cid-ydxbofl4>应用因果注意力掩码</span></a></li><li data-astro-cid-ydxbofl4><a href=#heading-12 class="flex items-center rounded-lg px-2 py-2 relative toc-link" style=padding-left:44px data-astro-cid-ydxbofl4 data-heading-depth=3 data-heading-slug=使用dropout遮掩额外的注意力权重 data-index=12><svg height=1em width=1em data-icon=tabler:chevron-right class="duration-200 absolute h-4 icon-indicator left-1 opacity-0 text-primary transition-all w-4" viewBox="0 0 24 24" data-astro-cid-ydxbofl4=true><use href=#ai:tabler:chevron-right></use></svg><span class="duration-200 link-text transition-transform" data-astro-cid-ydxbofl4>使用dropout遮掩额外的注意力权重</span></a></li><li data-astro-cid-ydxbofl4><a href=#heading-13 class="flex items-center rounded-lg px-2 py-2 relative toc-link" style=padding-left:44px data-astro-cid-ydxbofl4 data-heading-depth=3 data-heading-slug=实现一个简洁的因果注意力类 data-index=13><svg height=1em width=1em data-icon=tabler:chevron-right class="duration-200 absolute h-4 icon-indicator left-1 opacity-0 text-primary transition-all w-4" viewBox="0 0 24 24" data-astro-cid-ydxbofl4=true><use href=#ai:tabler:chevron-right></use></svg><span class="duration-200 link-text transition-transform" data-astro-cid-ydxbofl4>实现一个简洁的因果注意力类</span></a></li><li data-astro-cid-ydxbofl4><a href=#heading-14 class="flex items-center rounded-lg px-2 py-2 relative toc-link" style=padding-left:28px data-astro-cid-ydxbofl4 data-heading-depth=2 data-heading-slug=从单头注意力拓展到多头注意力 data-index=14><svg height=1em width=1em data-icon=tabler:chevron-right class="duration-200 absolute h-4 icon-indicator left-1 opacity-0 text-primary transition-all w-4" viewBox="0 0 24 24" data-astro-cid-ydxbofl4=true><use href=#ai:tabler:chevron-right></use></svg><span class="duration-200 link-text transition-transform" data-astro-cid-ydxbofl4>从单头注意力拓展到多头注意力</span></a></li><li data-astro-cid-ydxbofl4><a href=#heading-15 class="flex items-center rounded-lg px-2 py-2 relative toc-link" style=padding-left:44px data-astro-cid-ydxbofl4 data-heading-depth=3 data-heading-slug=堆叠多层单头注意力 data-index=15><svg height=1em width=1em data-icon=tabler:chevron-right class="duration-200 absolute h-4 icon-indicator left-1 opacity-0 text-primary transition-all w-4" viewBox="0 0 24 24" data-astro-cid-ydxbofl4=true><use href=#ai:tabler:chevron-right></use></svg><span class="duration-200 link-text transition-transform" data-astro-cid-ydxbofl4>堆叠多层单头注意力</span></a></li><li data-astro-cid-ydxbofl4><a href=#heading-16 class="flex items-center rounded-lg px-2 py-2 relative toc-link" style=padding-left:44px data-astro-cid-ydxbofl4 data-heading-depth=3 data-heading-slug=通过权重分割实现多头注意力机制 data-index=16><svg height=1em width=1em data-icon=tabler:chevron-right class="duration-200 absolute h-4 icon-indicator left-1 opacity-0 text-primary transition-all w-4" viewBox="0 0 24 24" data-astro-cid-ydxbofl4=true><use href=#ai:tabler:chevron-right></use></svg><span class="duration-200 link-text transition-transform" data-astro-cid-ydxbofl4>通过权重分割实现多头注意力机制</span></a></li><li data-astro-cid-ydxbofl4><a href=#heading-17 class="flex items-center rounded-lg px-2 py-2 relative toc-link" style=padding-left:28px data-astro-cid-ydxbofl4 data-heading-depth=2 data-heading-slug=本章摘要 data-index=17><svg height=1em width=1em data-icon=tabler:chevron-right class="duration-200 absolute h-4 icon-indicator left-1 opacity-0 text-primary transition-all w-4" viewBox="0 0 24 24" data-astro-cid-ydxbofl4=true><use href=#ai:tabler:chevron-right></use></svg><span class="duration-200 link-text transition-transform" data-astro-cid-ydxbofl4>本章摘要</span></a></li></ul></nav></div></div></div><script type=module>document.addEventListener("astro:page-load",(()=>{const t=document.querySelector(".toc-nav"),e=document.getElementById("active-indicator"),n=Array.from(document.querySelectorAll(".toc-nav a"));if(!t||!e||0===n.length)return;const o=document.querySelectorAll("h1[id], h2[id], h3[id], h4[id], h5[id], h6[id]");if(0===o.length)return;let i=null,r=!1,s=null;const c=new Map;function a(t,n=!0){if(!t)return;const o=parseInt(t.getAttribute("data-heading-depth")||"1");e.style.transition=n?"all 0.3s cubic-bezier(0.25, 0.1, 0.25, 1)":"opacity 0.3s ease-in-out",requestAnimationFrame((()=>{e.style.top=`${t.offsetTop}px`,e.style.height=`${t.offsetHeight}px`,e.style.opacity="1",e.style.left=16*(o-1)+"px",e.style.width=`calc(100% - ${16*(o-1)}px)`}))}function u(t,e){const n=t.getBoundingClientRect(),o=e.getBoundingClientRect();return n.top>=o.top&&n.bottom<=o.bottom}function l(e){i!==e&&(i&&i.classList.remove("active"),e.classList.add("active"),i=e,a(e),u(e,t)||function(e){if(u(e,t))return;const n=e.offsetTop,o=t.clientHeight,i=Math.max(0,n-o/2+e.offsetHeight/2);t.scrollTo({top:i,behavior:"smooth"})}(e))}o.forEach(((t,e)=>{const n=t.getAttribute("id"),o=`heading-${e}`;n&&n!==o&&(t.setAttribute("id",o),c.set(n,o),document.querySelectorAll(`a[href="#${n}"]:not(.toc-nav a)`).forEach((t=>t.setAttribute("href",`#${o}`))))})),t.addEventListener("click",(t=>{const e=t.target.closest("a");if(!e)return;t.preventDefault();const n=e.getAttribute("href")||"";if(!n||!n.startsWith("#"))return;const o=n.substring(1),i=document.getElementById(o);i&&(r=!0,l(e),i.scrollIntoView({behavior:"smooth"}),history.pushState(null,"",n),setTimeout((()=>{r=!1}),1e3))})),window.addEventListener("resize",(()=>{s&&clearTimeout(s),s=window.setTimeout((()=>{i&&a(i,!1)}),100)}),{passive:!0});const d=new IntersectionObserver((t=>{if(r)return;const e=t.filter((t=>t.isIntersecting));if(0===e.length)return;let n=e[0],o=1/0;for(const t of e){const e=Math.abs(t.boundingClientRect.top);e<o&&(o=e,n=t)}const s=n.target.getAttribute("id");if(!s)return;const c=document.querySelector(`.toc-nav a[href="#${s}"]`);c&&c!==i&&l(c)}),{rootMargin:"-50px 0px -75% 0px",threshold:[0,.25]});o.forEach((t=>d.observe(t))),setTimeout((()=>{const t=window.location.hash.substring(1),e=t?document.getElementById(t):o[0];if(e){const t=e.getAttribute("id");if(!t)return;const n=document.querySelector(`.toc-nav a[href="#${t}"]`);n&&l(n)}}),200),window.addEventListener("hashchange",(()=>{if(r)return;const t=window.location.hash.substring(1);if(t){if(r=!0,document.getElementById(t)){const e=document.querySelector(`.toc-nav a[href="#${t}"]`);e&&l(e)}setTimeout((()=>{r=!1}),1e3)}}))}))</script></div></aside></div></div><button class="btn btn-circle hover:scale-110 bg-base-100 btn-md fixed invisible opacity-0 right-6 shadow-lg transition-all z-50 bottom-20 md:hidden" aria-label="Table of Contents" id=mobile-toc-button onclick=mobile_toc_modal.showModal()><svg height=1em width=1em data-icon=lucide:list class="h-5 w-5"><symbol id=ai:lucide:list viewBox="0 0 24 24"><path d="M3 5h.01M3 12h.01M3 19h.01M8 5h13M8 12h13M8 19h13" fill=none stroke=currentColor stroke-linecap=round stroke-linejoin=round stroke-width=2 /></symbol><use href=#ai:lucide:list></use></svg></button><dialog class="modal md:hidden" id=mobile_toc_modal><div class="modal-box bg-base-100 max-w-md rounded-lg shadow-xl w-10/12"><div class="flex items-center border-b border-base-200 justify-between pb-4"><h3 class=font-medium>Table of Contents</h3><form method=dialog><button class="btn btn-circle btn-ghost" aria-label="Table of Contents"><svg height=1em width=1em data-icon=lucide:x class="h-5 w-5" viewBox="0 0 24 24"><use href=#ai:lucide:x></use></svg></button></form></div><div class="max-h-[70vh] overflow-auto pt-4"><ul class="menu-sm menu"><li class=my-1><a href=#长序列建模的问题 class="flex items-center rounded-lg hover:bg-base-200 mobile-toc-link transition-color" style=margin-left:12px data-mobile-heading-id=长序列建模的问题><span class="text-sm truncate">长序列建模的问题</span></a></li><li class=my-1><a href=#循环神经网络rnn class="flex items-center rounded-lg hover:bg-base-200 mobile-toc-link transition-color" style=margin-left:24px data-mobile-heading-id=循环神经网络rnn><span class="text-sm truncate">循环神经网络(RNN)</span></a></li><li class=my-1><a href=#注意力机制的解决方法 class="flex items-center rounded-lg hover:bg-base-200 mobile-toc-link transition-color" style=margin-left:24px data-mobile-heading-id=注意力机制的解决方法><span class="text-sm truncate">注意力机制的解决方法</span></a></li><li class=my-1><a href=#通过注意力机制捕捉数据依赖关系 class="flex items-center rounded-lg hover:bg-base-200 mobile-toc-link transition-color" style=margin-left:12px data-mobile-heading-id=通过注意力机制捕捉数据依赖关系><span class="text-sm truncate">通过注意力机制捕捉数据依赖关系</span></a></li><li class=my-1><a href=#通过自注意力机制关注输入的不同部分 class="flex items-center rounded-lg hover:bg-base-200 mobile-toc-link transition-color" style=margin-left:12px data-mobile-heading-id=通过自注意力机制关注输入的不同部分><span class="text-sm truncate">通过自注意力机制关注输入的不同部分</span></a></li><li class=my-1><a href=#一种不含可训练权重的简化自注意力机制 class="flex items-center rounded-lg hover:bg-base-200 mobile-toc-link transition-color" style=margin-left:24px data-mobile-heading-id=一种不含可训练权重的简化自注意力机制><span class="text-sm truncate">一种不含可训练权重的简化自注意力机制</span></a></li><li class=my-1><a href=#为所有输入的token计算注意力权重 class="flex items-center rounded-lg hover:bg-base-200 mobile-toc-link transition-color" style=margin-left:24px data-mobile-heading-id=为所有输入的token计算注意力权重><span class="text-sm truncate">为所有输入的token计算注意力权重</span></a></li><li class=my-1><a href=#实现带有可训练权重的自注意力机制 class="flex items-center rounded-lg hover:bg-base-200 mobile-toc-link transition-color" style=margin-left:12px data-mobile-heading-id=实现带有可训练权重的自注意力机制><span class="text-sm truncate">实现带有可训练权重的自注意力机制</span></a></li><li class=my-1><a href=#逐步计算注意力权重 class="flex items-center rounded-lg hover:bg-base-200 mobile-toc-link transition-color" style=margin-left:24px data-mobile-heading-id=逐步计算注意力权重><span class="text-sm truncate">逐步计算注意力权重</span></a></li><li class=my-1><a href=#实现一个简洁的子注意力机制python类 class="flex items-center rounded-lg hover:bg-base-200 mobile-toc-link transition-color" style=margin-left:24px data-mobile-heading-id=实现一个简洁的子注意力机制python类><span class="text-sm truncate">实现一个简洁的子注意力机制Python类</span></a></li><li class=my-1><a href=#使用因果注意力机制来屏蔽后续词 class="flex items-center rounded-lg hover:bg-base-200 mobile-toc-link transition-color" style=margin-left:12px data-mobile-heading-id=使用因果注意力机制来屏蔽后续词><span class="text-sm truncate">使用因果注意力机制来屏蔽后续词</span></a></li><li class=my-1><a href=#应用因果注意力掩码 class="flex items-center rounded-lg hover:bg-base-200 mobile-toc-link transition-color" style=margin-left:24px data-mobile-heading-id=应用因果注意力掩码><span class="text-sm truncate">应用因果注意力掩码</span></a></li><li class=my-1><a href=#使用dropout遮掩额外的注意力权重 class="flex items-center rounded-lg hover:bg-base-200 mobile-toc-link transition-color" style=margin-left:24px data-mobile-heading-id=使用dropout遮掩额外的注意力权重><span class="text-sm truncate">使用dropout遮掩额外的注意力权重</span></a></li><li class=my-1><a href=#实现一个简洁的因果注意力类 class="flex items-center rounded-lg hover:bg-base-200 mobile-toc-link transition-color" style=margin-left:24px data-mobile-heading-id=实现一个简洁的因果注意力类><span class="text-sm truncate">实现一个简洁的因果注意力类</span></a></li><li class=my-1><a href=#从单头注意力拓展到多头注意力 class="flex items-center rounded-lg hover:bg-base-200 mobile-toc-link transition-color" style=margin-left:12px data-mobile-heading-id=从单头注意力拓展到多头注意力><span class="text-sm truncate">从单头注意力拓展到多头注意力</span></a></li><li class=my-1><a href=#堆叠多层单头注意力 class="flex items-center rounded-lg hover:bg-base-200 mobile-toc-link transition-color" style=margin-left:24px data-mobile-heading-id=堆叠多层单头注意力><span class="text-sm truncate">堆叠多层单头注意力</span></a></li><li class=my-1><a href=#通过权重分割实现多头注意力机制 class="flex items-center rounded-lg hover:bg-base-200 mobile-toc-link transition-color" style=margin-left:24px data-mobile-heading-id=通过权重分割实现多头注意力机制><span class="text-sm truncate">通过权重分割实现多头注意力机制</span></a></li><li class=my-1><a href=#本章摘要 class="flex items-center rounded-lg hover:bg-base-200 mobile-toc-link transition-color" style=margin-left:12px data-mobile-heading-id=本章摘要><span class="text-sm truncate">本章摘要</span></a></li></ul></div></div><form method=dialog class=modal-backdrop><button>close</button></form></dialog><script type=module>document.addEventListener("astro:page-load",(()=>{const e=document.getElementById("mobile-toc-button");if(!e)return;const t=document.querySelectorAll(".mobile-toc-link"),i=document.querySelectorAll("h1[id], h2[id], h3[id], h4[id], h5[id], h6[id]");let o=null;const l=()=>{o||(o=setTimeout((()=>{window.scrollY>300?(e.classList.remove("opacity-0","invisible"),e.classList.add("opacity-100","visible")):(e.classList.remove("opacity-100","visible"),e.classList.add("opacity-0","invisible")),t.length>0&&i.length>0&&s(),o=null}),100))},s=()=>{let e=null;for(let t=i.length-1;t>=0;t--){const o=i[t];if(o.getBoundingClientRect().top<=100){e=o.id;break}}t.forEach((t=>{t.getAttribute("data-mobile-heading-id")===e?t.classList.add("bg-primary/10","font-medium"):t.classList.remove("bg-primary/10","font-medium")}))};window.addEventListener("scroll",l,{passive:!0}),l(),document.addEventListener("astro:before-swap",(()=>{window.removeEventListener("scroll",l),o&&clearTimeout(o)}))}))</script><script>!function(){const t="dracula";document.addEventListener("astro:after-swap",(()=>{const e=localStorage.getItem("theme");if(e){document.documentElement.setAttribute("data-theme",e);const n=e===t?"dark":"light";document.documentElement.setAttribute("data-theme-type",n)}}))}()</script><script>document.addEventListener("astro:page-load",(()=>{document.querySelectorAll(".btn-copy").forEach((e=>{e.addEventListener("click",(async()=>{const o=e.closest(".frosti-code").querySelector("code").textContent,c=e.querySelector(".frosti-code-toolbar-copy-icon"),s=e.querySelector(".frosti-code-toolbar-copy-success");try{await navigator.clipboard.writeText(o),c.classList.add("hidden"),s.classList.remove("hidden"),e.classList.add("copy-success"),setTimeout((()=>{c.classList.remove("hidden"),s.classList.add("hidden"),e.classList.remove("copy-success")}),2e3)}catch(e){console.error("Failed to copy:",e)}}))}))}))</script><style>.btn-copy{position:relative;overflow:hidden}.copy-success{animation:pulse .5s ease-in-out}.frosti-code-toolbar-copy-success svg{color:#10b981}@keyframes pulse{0%{transform:scale(1)}50%{transform:scale(1.1)}100%{transform:scale(1)}}</style></body></html>