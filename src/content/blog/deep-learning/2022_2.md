---
title: "Lecture 2: What to do if my network fails to train | ML"
description: åœ‹ç«‹å°ç£å¤§å­¸ æå®æ¯…æ©Ÿå™¨å­¸ç¿’2022æ˜¥è¯¾ç¨‹2
pubDate: 2026 01 31
categories:
  - tech
tags:
  - æ·±åº¦å­¦ä¹ 
---

## Preparation 1: æœºå™¨å­¦ä¹ ä»»åŠ¡æ”»ç•¥

[æ©Ÿå™¨å­¸ç¿’ä»»å‹™æ”»ç•¥](https://speech.ee.ntu.edu.tw/~hylee/ml/ml2021-course-data/overfit-v6.pdf)

### MLçš„æ¡†æ¶

è®­ç»ƒæ•°æ®ï¼š${(\mathbf{x_{1}},\hat{y_{1}}),(\mathbf{x_{2}},\hat{y_{2}}),\dots(\mathbf{x_{n}},\hat{y_{n}})}$

Step 1: function with unknown
$y=f_{\mathbf{\theta}}(\mathbf{x})$

Step 2: define loss from training data
$L(\mathbf{\theta})$

Step 3: optimization
$\mathbf{\theta}^*=arg \min_{\theta}L$

æµ‹è¯•æ•°æ®ï¼š${\mathbf{x_{n+1}},\mathbf{x_{n+2}},\dots,\mathbf{x_{n+m}}}$

$y=f_{\mathbf{\theta}^*}(\mathbf{x})$
å¾—åˆ°${y_{n+1},y_{n+2},\dots,y_{n+m}}$ => ä¸Šä¼ Kaggle

### å¦‚ä½•å°†æ¨¡å‹å˜å¥½ï¼Ÿ

![](./attachments/Leature2-1770288188991.png)

é¦–å…ˆï¼Œæ£€æŸ¥training data lossã€‚

å¦‚æœlossæ¯”è¾ƒå¤§ï¼ˆå·¦è¾¹çš„åˆ†æ”¯ï¼‰ï¼Œæœ‰ä¸¤ä¸ªå¯èƒ½ï¼š

- model bias: æ¨¡å‹å¤ªç®€å•
- optimization: ä¼˜åŒ–åšå¾—ä¸å¥½

#### Model Bias

å¯ä»¥è®©losså˜ä½çš„functionä¸åœ¨ä½ çš„æ¨¡å‹å¯ä»¥æè¿°çš„èŒƒå›´å†…~

![](./attachments/Leature2-1770288478225.png)

è§£å†³æ–¹æ³•ï¼šé‡æ–°è®¾è®¡æ¨¡å‹ï¼Œä½¿å…¶æ›´åŠ çµæ´»ã€‚ä¾‹å¦‚å¢åŠ å‚æ•°ã€å±‚æ•°ã€æ¿€æ´»å‡½æ•°ç­‰ã€‚

#### Optimization Issue

å±€éƒ¨æœ€ä¼˜é—®é¢˜

![](./attachments/Leature2-1770288694774.png)

> å¦‚ä½•åˆ¤æ–­æ˜¯å“ªä¸€ç§æƒ…å†µï¼Ÿ
> è®¾ç½®ä¸åŒçš„æ¨¡å‹è¿›è¡Œæ¯”è¾ƒã€‚å¯ä»¥å…ˆè·‘æ¯”è¾ƒå°çš„ç½‘ç»œæ¨¡å‹ï¼Œæ˜¯æ¯”è¾ƒå®¹æ˜“ä¼˜åŒ–çš„ã€‚å¦‚æœæ·±çš„æ¨¡å‹æ•ˆæœåè€Œä¸å¦‚å°æ¨¡å‹çš„loss ï¼Œå°±è¯´æ˜æ˜¯ä¼˜åŒ–çš„é—®é¢˜ã€‚
> ![](./attachments/Leature2-1770289119939.png)

å‡è®¾ç°åœ¨training dataçš„losså·²ç»å˜å°äº†ï¼Œå†æ¥çœ‹testing data çš„Lossï¼ˆå³è¾¹çš„åˆ†æ”¯ï¼‰ã€‚

#### Overfitting: è¿‡æ‹Ÿåˆ

training losså°ï¼Œä½†æ˜¯testing losså¤§ï¼Œå¯ä»¥ç†è§£ä¸ºæ¨¡å‹èƒŒç­”æ¡ˆã€‚é‡åˆ°æ²¡è§è¿‡çš„é¢˜ç›®å°±ä¸ä¼šäº†ã€‚

![](./attachments/Leature2-1770300397357.png)

è§£å†³çš„åŠæ³•ä¹‹ä¸€å°±æ˜¯å¢åŠ è®­ç»ƒæ•°æ®ã€‚ä½†æ˜¯åœ¨ä½œä¸šé‡Œé¢æ— æ³•å®ç°è¿™ä¸€ç‚¹ã€‚

å¦ä¸€ä¸ªè§£å†³åŠæ³•æ˜¯æ•°æ®å¢å¹¿ï¼ˆdata augmentationï¼‰

å¦ä¸€ä¸ªè§£å†³åŠæ³•æ˜¯ä¸è¦è®©æ¨¡å‹å¤ªå¤æ‚ï¼Œæœ‰å¤ªé«˜çš„è‡ªç”±åº¦ã€‚ç»™æ¨¡å‹å¢åŠ é™åˆ¶ï¼š

- Less parameters, sharing parameters
- Less features
- Early stopping
- Regulation
- Dropout

æ‰€ä»¥å•Šï¼Œé€‰æ‹©æ¨¡å‹çš„å¤æ‚åº¦éœ€è¦è¾¾åˆ°å¹³è¡¡ã€‚

#### Cross Validation: äº¤å‰éªŒè¯

ä¸ºäº†é¿å…è¿™ç±»è¿‡æ‹Ÿåˆçš„é—®é¢˜ï¼Œéœ€è¦åˆç†åœ°åˆ†å‰²æ•°æ®ã€‚

![](./attachments/Leature2-1770301873193.png)

éªŒè¯é›†ä¸å‚ä¸è®­ç»ƒã€‚æ‰€ä»¥æœ€åˆç†åœ°åšæ³•æ˜¯æ ¹æ®éªŒè¯é›†æŒ‘é€‰æ¨¡å‹ã€‚å¦‚æœæ ¹æ®public testing setå»æŒ‘é€‰æ¨¡å‹ï¼Œåˆæœ‰è¿‡æ‹Ÿåˆåœ°é£é™©ã€‚é™¤æ­¤ä¹‹å¤–ï¼Œä¸èƒ½é€šè¿‡è¯•ç§å­çš„æ–¹æ³•å»é™ä½åŒä¸€ä¸ªæ¨¡å‹çš„éªŒè¯é›†lossï¼ŒåŒæ ·æœ‰å‚æ•°éšæœºåˆ°éªŒè¯é›†çš„é£é™©ã€‚

ç°åœ¨å­¦ç•Œæœ‰çš„ä¸ºäº†æ‰€è°“å¯å¤ç°æ€§å›ºå®šäº†éšæœºç§å­å»æï¼Œæ€ä¹ˆé™ä½è¿™ç§æƒ…å†µä¸‹çš„å¯æ“ä½œæ€§å‘¢ï¼Ÿæœ¬æ¥åº”è¯¥éšæœºåˆ†ï¼Œæ—¢ç„¶ä¸èƒ½éšæœºåˆ†äº†ï¼Œé‚£å°±å¤šåˆ†å‡ æ¬¡ï¼Œæ¯æ¬¡é‡‡ç”¨ä¸åŒçš„éƒ¨åˆ†å»éªŒè¯ï¼Œè¿™å°±æ˜¯NæŠ˜äº¤å‰éªŒè¯ï¼ˆN-fold Validationï¼‰

![](./attachments/Leature2-1770302472623.png)

ç›®å‰æˆ‘è¿˜æ²¡çœ‹åˆ°å…¬å¼€è®ºæ–‡é‡‡ç”¨è¿™ç§ä¼˜ç§€çš„æ–¹å¼ï¼Œå¯èƒ½æ˜¯å¤§å®¶é»˜è®¤ç•™ä¸‹çš„ä¸€å—é®ç¾å¸ƒå§ğŸ¤­

#### Mismatch

è®­ç»ƒæ•°æ®å’Œæµ‹è¯•æ•°æ®çš„åˆ†å¸ƒä»æ ¹æœ¬ä¸Šå°±ä¸ä¸€è‡´ï¼Œå‡ºç°äº†åå¸¸æƒ…å†µ

> è¿™èŠ‚è¯¾å¾ˆé‡è¦ï¼è®²äº†å¾ˆå¤šå®ç”¨ä¸”é‡è¦çš„æ¦‚å¿µï¼Œæˆ‘å¼„æ¸…æ¥šäº†å¤šå¹´æ¥æ²¡æœ‰å®Œå…¨æŒæ¡çš„çŸ¥è¯†ã€‚

## Preparation 2: å±€éƒ¨æœ€å°å€¼ä¸éç‚¹

[When gradient is small â€¦](https://speech.ee.ntu.edu.tw/~hylee/ml/ml2021-course-data/small-gradient-v7.pdf)

æœ‰æ—¶å€™ä¼šå‘ç°å•Šï¼ŒLossä¼˜åŒ–ç€å°±ä¸å†ä¸‹é™äº†ï¼Œåå¤éœ‡è¡ï¼Œä½†æ˜¯å´è¿˜ä¾ç„¶æ²¡æœ‰è¾¾åˆ°ç†æƒ³çš„æ•ˆæœã€‚è¿™ä¸ªæ—¶å€™æ¢¯åº¦ä¸‹é™åˆ°äº†0ï¼Œå¯èƒ½æ˜¯è¾¾åˆ°äº†å±€éƒ¨æœ€å°å€¼æˆ–éç‚¹ã€‚

æ¢¯åº¦ä¸‹é™åˆ°0ï¼Œè¿™ä¸¤ç§ç‚¹éƒ½æ˜¯é©»ç‚¹ã€‚

### Math Warning

![](./attachments/Leature2-1770304510241.png)

åœ¨é©»ç‚¹å¤„ï¼Œ$g=0$ï¼Œé©»ç‚¹å±äºå“ªä¸€ç§ç±»å‹å°±å–å†³äºè¿™ä¸ªé»‘å¡çŸ©é˜µäº†ã€‚

![](./attachments/Leature2-1770345752245.png)

è®¡ç®—é»‘å¡çŸ©é˜µ$\mathbf{H}$çš„ç‰¹å¾å€¼å’Œç‰¹å¾å‘é‡ï¼Œæ‰¾åˆ°è´Ÿç‰¹å¾å€¼å’Œå¯¹åº”çš„ç‰¹å¾å‘é‡ï¼Œåœ¨è¯¥ç‰¹å¾å‘é‡çš„æ–¹å‘ä¸Šï¼Œå‡½æ•°$L(\mathbf{\theta})$å°±ä¼šå‘é‡ï¼Œæ²¿ç€è¯¥æ–¹å‘å°±å¯ä»¥ç»§ç»­ä¸‹é™æŸå¤±å‡½æ•°ã€‚

### Saddle Point VS Local Minima

å½“å‚æ•°æ•°é‡è¶Šå¤§ï¼ŒçŸ©é˜µç»´åº¦è¶Šé«˜æ—¶ï¼Œé©»ç‚¹æ˜¯çœŸæ­£çš„local minimaå…¶å®å¾ˆå°‘è§ï¼Œ**å¤§å¤šæ•°æƒ…å†µéƒ½æ˜¯éç‚¹**ï¼Œä¹Ÿå°±æ˜¯èƒ½æ‰¾åˆ°ç»§ç»­ä¼˜åŒ–çš„æ–¹å‘ã€‚

## Preparation 3: æ‰¹æ¬¡ä¸åŠ¨é‡

### Batch

![](./attachments/Leature2-1770346634601.png)

å®é™…æƒ…å†µçš„è®­ç»ƒè¿‡ç¨‹å°†æ•°æ®åˆ†ä¸ºå¤šä¸ªbatchã€‚æ¨¡å‹æ¯ç»è¿‡ä¸€ä¸ªæ‰¹æ¬¡å°±ä¼šæ›´æ–°ä¸€æ¬¡å‚æ•°ã€‚éå†æ‰€æœ‰æ‰¹æ¬¡åä¸ºç»è¿‡ä¸€ä¸ªEpochã€‚ç„¶åæ‰“ä¹±ï¼Œå†è¿›è¡Œä»¥ä¸Šè¿‡ç¨‹ã€‚

é‚£ä¹ˆæ‰¹æ¬¡çš„å¤§å°æœ‰ä»€ä¹ˆå½±å“å‘¢ï¼Ÿä»å¤šä¸ªè§’åº¦æ¥åˆ†æäº†è¿™ä¸ªé—®é¢˜ï¼š

#### å‚æ•°æ›´æ–°

![](./attachments/Leature2-1770347185056.png)

batch_size å°ï¼Œå‚æ•°æ›´æ–°æ˜¾å¾—ä¸ç¨³å®š

#### è¿ç®—æ—¶é—´

batch_size å¤§ï¼Œæœ‰åˆ©äºå……åˆ†åˆ©ç”¨GPUçš„å¹¶è¡Œè®¡ç®—èƒ½åŠ›

![](./attachments/Leature2-1770347298407.png)

#### è®­ç»ƒæ•ˆæœ

![](./attachments/Leature2-1770347529287.png)

batch_sizeè¶Šå°ï¼Œè®­ç»ƒçš„å‡†ç¡®ç‡è¶Šå¥½ã€‚è¿™æ˜¯ä¸ºä»€ä¹ˆå‘¢ï¼Ÿ
å› ä¸ºå‚æ•°éœ‡è¡å¾—æ¯”è¾ƒæ¿€çƒˆï¼Œå®¹æ˜“è·³å‡ºé©»ç‚¹ã€‚
![](./attachments/Leature2-1770347866799.png)

å¯¹äºFull Batchçš„ä¸€ä¸ªEpoché‡Œé¢ï¼Œå‚æ•°æ²¡æœ‰å˜åŒ–ï¼ŒæŸå¤±å‡½æ•°æ˜¯å›ºå®šçš„ï¼›å¯¹äºSmall Batchï¼Œå°±å¯èƒ½ç»§ç»­è®­ç»ƒã€‚

#### æµ‹è¯•æ•ˆæœ

Large Batchå°±ç®—è°ƒå¤§å­¦ä¹ ç‡ï¼Œè¿˜æ˜¯å®¹æ˜“äº§ç”Ÿå…³é”®å‚æ•°çš„ä¾èµ–ï¼Œé²æ£’æ€§ä¸å¤Ÿå¥½ã€‚ï¼ˆOverfittingï¼‰
![](./attachments/Leature2-1770348576130.png)

è™½ç„¶æå°å€¼éƒ½å·®ä¸å¤šï¼Œä½†äº¦æœ‰å¥½ååŒºåˆ«ã€‚è¿™ç§Sharp Minimaçš„æ³›åŒ–æ€§å°±ä¸å¥½ï¼Œè€ŒFlat Minimaå°±ç®—æµ‹è¯•ä¸è®­ç»ƒæœ‰åˆ†å¸ƒåå·®ï¼Œè¿˜æ˜¯å¯ä»¥è¾¾åˆ°æ¯”è¾ƒå¥½çš„æ•ˆæœã€‚

#### å°ç»“

batch_sizeæ˜¯ä¸€ä¸ªéœ€è¦è‡ªå·±æƒè¡¡çš„è¶…å‚æ•°

| æ€§è´¨             | å°æ‰¹æ¬¡ | å¤§æ‰¹æ¬¡ |
| -------------- | --- | --- |
| æ›´æ–°é€Ÿåº¦<br>ï¼ˆæ²¡æœ‰å¹¶è¡Œï¼‰ | å¿«   | æ…¢   |
| æ›´æ–°é€Ÿåº¦<br>ï¼ˆå¹¶è¡Œï¼‰   | å·®ä¸å¤š | å·®ä¸å¤š |
| ä¸€è½®è¿­ä»£çš„æ—¶é—´        | æ…¢   | å¿«   |
| æ¢¯åº¦             | å˜ˆæ‚  | ç¨³å®š  |
| ä¼˜åŒ–             | æ›´å¥½  | æ›´å  |
| æ³›åŒ–æ€§            | æ›´å¥½  | æ›´å  |

### Momentum

æˆ‘ç†è§£ä¸ºä¸ç›´æ¥æ›´æ–°é€Ÿåº¦ï¼Œè€Œæ˜¯æ›´æ–°åŠ é€Ÿåº¦ï¼Œé€Ÿåº¦ä¸Šè¿˜ä¿ç•™ä¸€å®šæƒ¯æ€§ã€‚ç”±äºè®¡ç®—é«˜ç»´çŸ©é˜µçš„ç‰¹å¾å€¼å’Œç‰¹å¾å‘é‡çš„è®¡ç®—é‡å¤ªå¤§ï¼Œè¿™æ˜¯å¦ä¸€ç§å¯ä»¥é¿å…é™·å…¥é©»ç‚¹åå‡ºä¸æ¥çš„è§£å†³åŠæ³•ã€‚

![](./attachments/Leature2-1770349553391.png)

## Preparation 4: è‡ªé€‚åº”å­¦ä¹ ç‡

[Adaptive Learning Rate](https://speech.ee.ntu.edu.tw/~hylee/ml/ml2021-course-data/optimizer_v4.pdf)
å‘ç°Lossæ²¡æœ‰å†ä¸‹é™ï¼Œå¾€å¾€**ä¸æ˜¯**å› ä¸ºæ¢¯åº¦ä¸‹é™åˆ°é©»ç‚¹ã€‚é€šè¿‡åˆ†ææ¢¯åº¦ï¼Œå‘ç°è¿™ä¸ªæ—¶å€™æ¢¯åº¦ä¸æ˜¯0ã€‚

éœ€è¦æ›´å¥½çš„æ¢¯åº¦ä¸‹é™ç­–ç•¥ã€‚

![](./attachments/Leature2-1770364906856.png)

å›ºå®šçš„learning rateï¼Œæ— æ³•é€‚åº”æ•´ä¸ªä»»åŠ¡çš„ä¸åŒé˜¶æ®µã€‚

learning rateéœ€è¦æ ¹æ®æ¢¯åº¦çš„å¤§å°åšåŠ¨æ€çš„è°ƒæ•´ã€‚æ¢¯åº¦å¤§çš„æ—¶å€™å‡å°æ­¥å¹…ï¼›æ¢¯åº¦å°çš„æ—¶å€™å¢å¤§æ­¥å¹…ã€‚

åŸæ¥çš„æ›´æ–°æ–¹æ³•
$$
\theta_{i}^{t+1} \gets \theta_{i}^{t}-\eta g_{i}^{t}
$$
å…¶ä¸­
$$
g_{i}^t=\frac{\partial L}{\partial \theta_{i}}|_{\theta=\theta^t}
$$
ç°åœ¨$\eta$å˜æˆåŠ¨æ€è°ƒæ•´çš„
$$
\theta_{i}^{t+1} \gets \theta_{i}^{t}-\frac{\eta}{\sigma_{i}^t} g_{i}^{t}
$$

æˆ–è€…ç®€è®°ä¸º
$$
\theta^{t+1} \gets \theta^{t}-\frac{\eta}{\sigma^t} g^{t}
$$

è¿™ä¸ª$\sigma$å°±ä¸$g$æ­£ç›¸å…³ã€‚æ ¹æ®$\eta$ä¸$g$çš„å…³ç³»ï¼Œäº§ç”Ÿäº†å¤šç§ä¸åŒçš„æ›´æ–°ç­–ç•¥ï¼š

### Root Mean Square

å¹³æ–¹å¹³å‡
$$
\sigma^{0}=\sqrt{ (g^{0})^{2} }
$$
$$
\sigma^{1}=\sqrt{ \frac{1}{2}[(g^{0})^{2}+(g^{1})^{2}] }
$$
$$
\sigma^{2}=\sqrt{ \frac{1}{3}[(g^{0})^{2}+(g^{1})^{2}+(g^{2})^{2}] }
$$
å³
$$
\sigma^t = \sqrt{ \frac{1}{t+1}\sum_{i=0}^t (g^i)^{2} }
$$

æ­¤ç­–ç•¥ä¸º `Adagrad` æ‰€é‡‡ç”¨ã€‚

### RMSProp

$$
\sigma^{0}=\sqrt{ (g^{0})^{2} }
$$

$$
\sigma^{1} = \sqrt{ \alpha(\sigma^{0})^{2}+(1-\alpha(g^{1})^{2}) }
$$

$$
\sigma^{2} = \sqrt{ \alpha(\sigma^{1})^{2}+(1-\alpha(g^{2})^{2}) }
$$

å³

$$
\sigma^{t} = \sqrt{ \alpha(\sigma^{t-1})^{2}+(1-\alpha(g^{t})^{2}) }
$$

å…¶ä¸­$0<\alpha<1$æ˜¯ä¸€ä¸ªè¶…å‚æ•°

ç›¸æ¯”äºåˆšæ‰çš„å¹³æ–¹å¹³å‡ï¼ŒRMSProp å¯ä»¥é€šè¿‡æŠŠ$\alpha$è®¾ç½®å°ä¸€ç‚¹æ¥å¢å¼ºæ•æ„Ÿæ€§ã€‚

`Adam` é‡‡ç”¨RMSProp + Momentumã€‚

### å­¦ä¹ ç‡è°ƒåº¦å™¨

ç”±äºæ¢¯åº¦é•¿æœŸå¾ˆå°ï¼Œç´¯ç§¯ä¹‹ä¸‹æ­¥å¹…è¶Šæ¥è¶Šå¤§ï¼Œå¾ˆå®¹æ˜“åˆå†²è¿‡å¤´äº†ã€‚è™½ç„¶æœ€åè¿˜æ˜¯èƒ½è´Ÿåé¦ˆè°ƒèŠ‚å›æ¥ï¼Œä½†æ˜¯æ•ˆç‡å°±é™ä½äº†ã€‚

æˆ‘ä»¬è§‚å¯Ÿåˆ°ï¼Œéšç€è®­ç»ƒçš„è¿›è¡Œï¼Œå‚æ•°è¶Šæ¥è¶Šæ¥è¿‘æå€¼ç‚¹ï¼Œå­¦ä¹ ç‡æ€»ä½“ä¸Šè¿˜æ˜¯è¦å‘ˆç°ä¸€ä¸ªé™ä½çš„è¶‹åŠ¿ã€‚

$$
\theta^{t+1} \gets \theta^{t} - \frac{\eta^{t}}{\sigma^{t}}g^{t}
$$

å› æ­¤å¼•å…¥ Learning Rate Decayç­–ç•¥ï¼š

![](./attachments/Lecture2-1770368448033.png)

å°±å¯ä»¥ç¼“è§£è¿™ä¸ªé—®é¢˜ã€‚

Warn upç­–ç•¥ï¼š

![](./attachments/Lecture2-1770368905747.png)

åœ¨ä¸€å¼€å§‹è®­ç»ƒçš„æ—¶å€™ï¼Œå¯¹äº$\sigma^{t}$çš„è¯„ä¼°ä¸å¤ªç¨³å®šï¼Œå°±ä¸èƒ½è®©æ­¥å¹…èµ°å¾—å¤ªå¤§ã€‚

### å°ç»“

è‡³æ­¤ï¼Œæˆ‘ä»¬å°†Optimizationä»ä¸€å¼€å§‹ç®€å•çš„æ¢¯åº¦ä¸‹é™å˜æˆç°åœ¨è¿™æ ·çš„ç‰ˆæœ¬ï¼š

$$
\theta^{t+1} \gets \theta^{t} - \frac{\eta^{t}}{\sigma^{t}}m^{t}
$$

![](./attachments/Lecture2-1770369201757.png)

$m$å’Œ$\sigma$éƒ½å’Œä¹‹å‰çš„æ¢¯åº¦$g$æœ‰å…³ï¼Œä½†ä¸€ä¸ªæ˜¯æ§åˆ¶æ–¹å‘çš„çŸ¢é‡ï¼Œä¸€ä¸ªæ˜¯æ§åˆ¶å¤§å°çš„æ ‡é‡ã€‚

é’ˆå¯¹$\eta$,$\sigma$,$m$é‡‡ç”¨ä¸åŒç­–ç•¥ï¼Œå°±å½¢æˆäº†ç°åœ¨çš„ä¼˜åŒ–å™¨çš„åŸºæœ¬ç®—æ³•ã€‚

## Preparation 5: æŸå¤±å‡½æ•°

[Logistic Regression](https://speech.ee.ntu.edu.tw/~hylee/ml/ml2021-course-data/classification_v2.pdf)

ä¸åŒçš„æŸå¤±å‡½æ•°ï¼Œä¹Ÿä¼šå½±å“è®­ç»ƒçš„éš¾æ˜“ç¨‹åº¦ã€‚è¿™èŠ‚è¯¾ç¨‹åšäº†ä¸€äº›é“ºå«ï¼Œè®²äº†æœºå™¨å­¦ä¹ ä¸­çš„å¤šåˆ†ç±»ä»»åŠ¡ã€‚é€šè¿‡å¤šåˆ†ç±»ä»»åŠ¡ä¸­æŸå¤±å‡½æ•°çš„é€‰æ‹©æ¥æ­ç¤ºè¿™ä¸€é“ç†ã€‚

åˆ†ç±»ä»»åŠ¡å°±æ˜¯ç¦»æ•£å›å½’ä»»åŠ¡ï¼Œæ ‡ç­¾è¢«è½¬æ¢ä¸º**ç‹¬çƒ­ç¼–ç ï¼ˆone-hot vectorï¼‰**ï¼Œä»¥ä¿è¯çº¿æ€§æ— å…³æ€§ã€‚

ä¾‹å¦‚
$$
\begin{bmatrix}
1\\0\\0
\end{bmatrix},
\begin{bmatrix}
0\\1\\0
\end{bmatrix},
\begin{bmatrix}
 0\\0\\1
\end{bmatrix}
$$

![](./attachments/Lecture2-1770431243878.png)

å…³äºäº¤å‰ç†µæŸå¤±å‡½æ•°è®¾è®¡ï¼Œè·Ÿæå¤§ä¼¼ç„¶ä¼°è®¡æœ‰å¾ˆå¤§å…³ç³»ã€‚ä½†åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ç ”ç©¶çš„æ˜¯å®ƒåœ¨åšOptimizationæ—¶è·ŸMSEçš„åŒºåˆ«ã€‚

`softmax`å‡½æ•°ç»å¸¸ç”¨åœ¨æ¦‚ç‡ç›¸å…³çš„ä»»åŠ¡ä¸­ï¼Œå®ƒå°†å„ä¸ªè¾“å‡ºèŠ‚ç‚¹çš„è¾“å‡ºå€¼èŒƒå›´æ˜ å°„åˆ°$[0, 1]$ï¼Œå¹¶ä¸”çº¦æŸå„ä¸ªè¾“å‡ºèŠ‚ç‚¹çš„è¾“å‡ºå€¼çš„å’Œä¸º1ã€‚

$$
\text{softmax}(y_{i})=\frac{\exp(y_{i})}{\sum \exp(y_{j})}
$$

è¿™æ˜¯æœ¬èŠ‚è¯¾ç¨‹æœ€åä¸€å¼ PPTï¼Œä¸¾äº†ä¸€ä¸ªä¾‹å­ï¼š

![](./attachments/Lecture2-1770431895530.png)

å‡è®¾ç¥ç»ç½‘ç»œçš„è¾“å‡ºä¸­$y_{3}$å¾ˆå°ï¼Œç»è¿‡ `softmax` åæ¥è¿‘äº0ï¼Œä»è€Œlossä¸»è¦è·Ÿ$y_{1},y_{2}$æœ‰å…³ï¼š

$$
\text{MSE}(\mathbf{y})=(1-y_{1}')^{2}+y_{2}'^{2}
$$

$$
\text{CrossEntropy}(\mathbf{y})=-\ln y_{1}'
$$

è¿™é‡Œå¹»ç¯ç‰‡ç›´æ¥å±•ç¤ºäº†ä¸¤ç§æŸå¤±å‡½æ•°çš„æ¢¯åº¦çš„åŒºåˆ«ï¼Œå¹¶æŒ‡å‡ºåœ¨æŸå¤±è¾ƒå¤§ï¼ˆ$y_{1}$å°ï¼Œ$y_{2}$å¤§ï¼‰çš„æƒ…å†µä¸‹ï¼Œ**MSEå‡½æ•°æ¢¯åº¦å¾ˆå°ï¼Œè€Œäº¤å‰ç†µæŸå¤±å‡½æ•°åè€Œæœ‰å¾ˆå¤§çš„æ¢¯åº¦**ï¼Œä»è€Œè§£é‡Šäº†åœ¨é€»è¾‘å›å½’ä»»åŠ¡ä¸­é‡‡ç”¨äº¤å‰ç†µæŸå¤±å‡½æ•°ç”±äºMSEçš„åŸå› ï¼Œè¿›è€Œå°è¯äº†æŸå¤±å‡½æ•°ä¹Ÿå¯¹è®­ç»ƒè¿‡ç¨‹æœ‰å½±å“ï¼ŒçœŸå®é†é†çŒé¡¶å•Š~

è¯¾ç¨‹æ²¡æœ‰è§£é‡Šæ¢¯åº¦ä¸åŒçš„åŸå› ï¼Œè¿™é‡Œæˆ‘åšä¸€ä¸ªç®€å•è¯æ˜ã€‚

åˆ†åˆ«è®¡ç®—å…¶æ¢¯åº¦ï¼š

$$
\nabla \text{MSE}=\left((2y_{1}'-2)\frac{ \partial y_{1}' }{ \partial y_{1} } ,2y_{2}'\frac{ \partial y_{2}' }{ \partial y_{2} } \right)=2\left( -(1-\alpha)^{2}\alpha, (1-\alpha)\alpha\right)
$$

å…¶ä¸­

$$
\alpha=\frac{\exp(y_{1})}{\exp(y_{1})+\exp(y_{2})}
$$
$$
1-\alpha=\frac{\exp(y_{2})}{\exp(y_{1})+\exp(y_{2})}
$$

å½“$y_{1}$å°ï¼Œ$y_{2}$å¤§çš„æ—¶å€™ï¼Œå¯ä»¥è®¤ä¸º$\alpha \to 0$ï¼Œè€Œåšå•†å¯ä»¥è¯æ˜$(1-\alpha)^{2}\alpha, (1-\alpha)\alpha$éƒ½æ˜¯$\alpha$çš„ç­‰ä»·æ— ç©·å°ï¼Œæ‰€ä»¥ä¹Ÿéƒ½æ¥è¿‘0.

$$
\nabla \text{CrossEntropy}=\left( -\frac{1}{y_{1}'}\frac{ \partial y_{1}' }{ \partial y_{1} } ,0 \right)=\left( -\frac{\exp(y_{2})}{\exp(y_{1})+\exp(y_{2})},0 \right)
$$
åˆ™æ¥è¿‘1.

## åˆ†ç±»å™¨ï¼šæµ…è°ˆæœºå™¨å­¦ä¹ åŸç†

[å¯¶å¯å¤¢ã€æ•¸ç¢¼å¯¶è²åˆ†é¡å™¨](https://speech.ee.ntu.edu.tw/~hylee/ml/ml2022-course-data/theory%20(v7).pdf)

> è¿™èŠ‚è¯¾ç¨‹å¥½åƒæ˜¯åœ¨å°†æ•°æ®é›†é‡‡æ ·çš„äº‹â€¦â€¦å°½é‡ç†è®ºåŒ–ï¼Œä½†è¿˜æ˜¯æœ‰ç‚¹ç‰µå¼ºâ€¦â€¦

### å¸¦æœ‰æœªçŸ¥å‚æ•°çš„å‡½æ•°

åˆ†ç±»ä»»åŠ¡å¯ä»¥ç†è§£ä¸ºæ‰¾åˆ°ä¸€ä¸ªé˜ˆå€¼ï¼Œå¤§äºè¿™ä¸ªé˜ˆå€¼çš„æ˜¯ä¸€ç±»ï¼Œå°äºè¿™ä¸ªé˜ˆå€¼çš„æ˜¯å¦ä¸€ç±»ã€‚è®­ç»ƒå°±æ˜¯éœ€è¦åœ¨é˜ˆå€¼çš„å–å€¼èŒƒå›´$\mathcal{H}$å†…æ‰¾åˆ°èƒ½ä½¿å‡†ç¡®ç‡è¾¾åˆ°æœ€å¤§çš„é˜ˆå€¼ã€‚å–å€¼èŒƒå›´çš„å¤§å°$\left|\mathcal{H}\right|$å°±æ˜¯è¿™ä¸ªæ¨¡å‹çš„å¤æ‚ç¨‹åº¦ã€‚

$$
f_{h}(\mathbf{x})=\begin{cases}
\text{A} &\text{if }e(\mathbf{x})\ge h \\\text{B} & \text{if }e(\mathbf{x})\lt h
\end{cases}
$$

### æŸå¤±

ç»™å®šä¸€ä¸ªæ•°æ®é›†$$\mathcal{D}=\{ (x_{1},\hat{y}_{1}),(x_{2},\hat{y}_{2}),\dots,(x_{n},\hat{y}_{n}) \}$$

åœ¨é˜ˆå€¼$h$ä¸‹çš„æŸå¤±å¯ä»¥è¡¨ç¤ºä¸º
$$
L(h,\mathcal{D})=\frac{1}{N}\sum_{i=1}^{N} l(h,x_{i},\hat{y}_{i})
$$

è¯¯åˆ†ç±»ç‡ä¸­ï¼Œ
$$
l(h,x_{i},\hat{y}_{i})
$$

å¯å–ä¸º
$$
I(f_{h}(x_{i})\ne \hat{y}_{i})=\begin{cases}
1 & \text{if }f_{h}(x_{i})\ne \hat{y}_{i} \\
0 & \text{if }f_{h}(x_{i}) = \hat{y}_{i}
\end{cases}
$$

ç”±äºè¿™ä¸ªå‡½æ•°ä¸è¿ç»­ï¼Œä¸ºäº†æ¢¯åº¦ä¸‹é™ï¼Œå®é™…è®­ç»ƒä¸­è®¾ä¸ºäº¤å‰ç†µæŸå¤±å‡½æ•°ã€‚

### æ•°æ®é›†é‡‡æ ·

**ç†æƒ³**æƒ…å†µæ˜¯ç”¨æ‰€æœ‰æ•°æ®æ¥è®­ç»ƒï¼š
$$
h_{all}=arg \min_{h}L(h,\mathcal{D}_{all})
$$

**ç°å®**ä¸Šåªèƒ½æ‰¾åˆ°éƒ¨åˆ†æ•°æ®æ¥è®­ç»ƒï¼š
$$
h_{train}=arg \min_{h}L(h,\mathcal{
D_{train}
})
$$

æˆ‘ä»¬å¸Œæœ›$L(h,\mathcal{D}_{all})$å’Œ$L(h,\mathcal{D}_{train})$è¶³å¤Ÿæ¥è¿‘ï¼š

$$
L(h,\mathcal{D}_{all})-L(h,\mathcal{D}_{all})\le\delta
$$
å…¶å¿…è¦æ¡ä»¶æ˜¯
$$
\forall h \in \mathcal{H}, \left | L(h,\mathcal{D}_{train}) - L(h,\mathcal{D}_{all})\right | \le \frac{\delta}{2}
$$

ä»¥ä¸‹è¯æ˜:
ç”±
$$
\begin{align}
L(h_{train},\mathcal{D}_{all}) &\le L(h_{train},\mathcal{D}_{all})+\frac{\delta}{2}  \\
&\le L(h_{all},\mathcal{D}_{train})+\frac{\delta}{2} \\
&\le L(h_{all},\mathcal{D}_{all})+\frac{\delta}{2}+\frac{\delta}{2} \\
&=L(h_{all},\mathcal{D}_{all})+\delta
\end{align}
$$

å¦$\varepsilon=\frac{\delta}{2}$ï¼Œæˆ‘ä»¬å°†æ»¡è¶³
$$
\forall h \in \mathcal{H}, \left | L(h,\mathcal{D}_{train}) - L(h,\mathcal{D}_{all})\right | \le \varepsilon
$$

çš„$\mathcal{D}_{train}$ç§°ä¸ºå¥½çš„é‡‡æ ·ã€‚å¦åˆ™å°±æ˜¯åçš„é‡‡æ ·ã€‚

é‚£ä¹ˆå‘ç”Ÿåçš„é‡‡æ ·çš„æ¦‚ç‡æ˜¯å¤šå°‘å‘¢ï¼Ÿå¦‚ä½•é¿å…åçš„è®­ç»ƒé›†ï¼Ÿ

åçš„è®­ç»ƒé›†$\mathcal{D}_{train}$å°±æ˜¯
$$
\exists h\in \mathcal{H}, \left | L(h,\mathcal{D}_{train}) - L(h,\mathcal{D}_{all})\right | \le \varepsilon
$$

![](./attachments/Lecture2-1770519066152.png)

$$
\begin{align}
P(\mathcal{D}_{train} \text{ is bad})&=P\left( \bigcup_{h \in \mathcal{H}}\mathcal{D}_{train} \text{ is bad due to }h \right) \\
&\le \sum_{h\in \mathcal{H}}P(\mathcal{D}_{train} \text{ is bad due to }h)
\end{align}
$$

>è¿™æ˜¯ä¸€ä¸ªå¾ˆè¿‡åˆ†çš„æ”¾ç¼©ï¼Œæˆ‘ä¸æ˜¯å¾ˆè®¤å¯ï¼Œä½†æœ€ç»ˆé€šè¿‡æ»¡è¶³æŸäº›å–å€¼ä»ç„¶å¯ä»¥ä½¿å…¶å°äº1ã€‚

éœå¤«ä¸ä¸ç­‰å¼ï¼ˆHoeffdingâ€™s Inequality):

$$
P(\mathcal{D}_{train} \text{ is bad due to }h) \le 2\exp(-2N\varepsilon^{2})
$$

å…¶ä¸­$N$æ˜¯$\mathcal{D}_{train}$ä¸­æ ·æœ¬çš„æ•°é‡ã€‚

é‚£ä¹ˆ

$$
\begin{align}
P(\mathcal{D}_{train} \text{ is bad})
&\le \sum_{h\in \mathcal{H}}P(\mathcal{D}_{train} \text{ is bad due to }h)  \\
&\le  \sum_{h\in \mathcal{H}}2\exp(-2N\varepsilon^{2}) \\
&=|\mathcal{H}|\cdot 2\exp(-2N\varepsilon^{2})
\end{align}
$$

æ‰€ä»¥ï¼Œ
$$
P(\mathcal{D}_{train} \text{ is bad})\le \delta
$$

çš„å¿…è¦æ¡ä»¶å°±æ˜¯

$$
N \ge \frac{\log\left( \frac{2|\mathcal{H}|}{\delta} \right)}{2\varepsilon^{2}}
$$

> æˆ‘æ„Ÿè§‰è¿™åªæ˜¯ä¸€ç§å°½é‡çš„è§£é‡Šå’Œç†è®ºåŒ–ï¼Œå®é™…ä¸Šçš„ç¥ç»ç½‘ç»œæ¨¡å‹å‚æ•°ç©ºé—´$\mathcal{H}$éå¸¸å¤§ï¼ˆVC-dimensionï¼‰ï¼Œæ€ä¹ˆå¯èƒ½å¾—åˆ°é‚£ä¹ˆå¤§çš„æ ·æœ¬å˜›ğŸ˜…

å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œè®­ç»ƒæ•°æ®æ— æ³•è¿›ä¸€æ­¥å¢å¤§ï¼Œé‚£ä¹ˆé‡‡ç”¨å¤§çš„$\mathcal{H}$å¯ä»¥æŠŠ$L(h_{all},\mathcal{D}_{all})$é™ä¸‹æ¥ï¼Œä½†æ„å‘³ç€$L(h_{train},\mathcal{D}_{all})$è·Ÿ$L(h_{all},\mathcal{D}_{all})$çš„å·®è·å¤§ï¼›åä¹‹ï¼Œ$L(h_{train},\mathcal{D}_{all})$è·Ÿ$L(h_{all},\mathcal{D}_{all})$å·®è·å°ï¼Œä½†æ¨¡å‹æ•ˆæœä¸å¥½ï¼Œ$L(h_{all},\mathcal{D}_{all})$æœ¬æ¥å°±æ¯”è¾ƒå¤§ã€‚

> é±¼å’Œç†ŠæŒå¯ä»¥å…¼å¾—å—ï¼Ÿ

## HW2: Phoneme Classification

[Machine Learning HW2](https://speech.ee.ntu.edu.tw/~hylee/ml/ml2022-course-data/hw2_slides2022.pdf)

### ä»»åŠ¡ä»‹ç»

è¿™æ˜¯ä¸€æ¬¡è¯­éŸ³è¯†åˆ«ä»»åŠ¡ï¼Œæ ¹æ®éŸ³é¢‘ææ–™è®­ç»ƒå¹¶é¢„æµ‹éŸ³ç´ ï¼ˆphonemeï¼‰ã€‚éŸ³ç´ æ˜¯è¯­éŸ³çš„åŸºæœ¬å•ä½ï¼Œç±»ä¼¼äºå­—æ¯åœ¨æ–‡å­—ä¸­çš„ä½œç”¨ã€‚

### æ•°æ®é¢„å¤„ç†

æ¯25mséŸ³é¢‘åˆ†å‰²ä¸ºä¸€å¸§ï¼Œä»åŸå§‹æ³¢å½¢ä¸­æå–39ç»´çš„MFCCç‰¹å¾ï¼Œè¿™éƒ¨åˆ†å·¥ä½œå·²ç»ç”±åŠ©æ•™åšå¥½äº†ï¼

### æ•°æ®é›†

æ•°æ®é›†æ˜¯[LibriSpeech](https://www.openslr.org/12/)(subset of train-clean-100)

- Training: 4268é¢„å¤„ç†éŸ³é¢‘ç‰¹å¾åŠå…¶å¯¹åº”çš„æ ‡ç­¾ï¼ˆæ€»å…± 2644158 å¸§ï¼‰
- Testing: 1078é¢„å¤„ç†éŸ³é¢‘ç‰¹å¾ï¼ˆæ€»å…± 646268 å¸§ï¼‰
- Label: 41ä¸ªç±»åˆ«çš„æ ‡ç­¾ï¼Œæ¯ä¸ªç±»åˆ«å¯¹åº”ä¸€ä¸ªéŸ³ç´ ã€‚

### æ•°æ®æ ¼å¼

```
libriphone/
    - train_split.txt (train metadata)
    - train_labels.txt (train labels)
    - test_split.txt (test metadata)
    - feat/
        - train/
        - test/
```

- `train_split.txt` å’Œ `test_split.txt` åŒ…å«äº†æ¯ä¸ªè®­ç»ƒå’Œæµ‹è¯•éŸ³é¢‘çš„æ–‡ä»¶åï¼ˆåœ¨ `feat/train` å’Œ `feat/test` ä¸‹ï¼‰ã€‚
- `train_labels.txt` åŒ…å«äº†æ¯ä¸ªéŸ³é¢‘æ–‡ä»¶çš„æ ‡ç­¾ä¿¡æ¯ã€‚
- `feat/train/` å’Œ `feat/test/` ç›®å½•ä¸‹åˆ†åˆ«å­˜å‚¨äº†è®­ç»ƒå’Œæµ‹è¯•éŸ³é¢‘çš„MFCCç‰¹å¾æ–‡ä»¶ï¼Œæ¯ä¸ªæ–‡ä»¶æœ‰å¤šä¸ªéŸ³é¢‘æ ·æœ¬ï¼ˆå½¢çŠ¶ä¸º[seq_len, 39]ï¼‰ã€‚

### ä»»åŠ¡è¦æ±‚

æ°´å¹³ | å‡†ç¡®ç‡
---|---
simple | 0.45797 (sample code)
medium | 0.69747 (concat n frames, add layers)
strong | 0.75028 (concat n, batchnorm, dropout, add layers)
boss |  0.82324 (sequence-labeling(using RNN))

### Simple Baseline

ç›´æ¥è¿è¡Œ[ç¤ºä¾‹ä»£ç ](https://github.com/virginiakm1988/ML2022-Spring/blob/main/HW02/HW02.ipynb)

![](./attachments/Lecture2-1770541585613.png)

### Medium Baseline

è¿æ¥`n_frame=19`ï¼Œå¢åŠ åˆ°ä¸‰å±‚`BasicBlock`ç½‘ç»œã€‚
å­¦ä¹ ç‡è°ƒåº¦å™¨ä½¿ç”¨ä½™å¼¦é€€ç«ï¼š

```python
scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(
Â  Â  optimizer, T_0=10, T_mult=2, eta_min=1e-6)
```

![](./attachments/Lecture2-1770541710754.png)

### Strong Baseline

åŒæ—¶ï¼Œæ¯ä¸€å±‚å¢åŠ `batchnorm`å’Œ`dropout`ï¼š

```python
class BasicBlock(nn.Module):
    def __init__(self, input_dim, output_dim):
        super(BasicBlock, self).__init__()

        self.block = nn.Sequential(
            nn.Linear(input_dim, output_dim),
            nn.ReLU(),
            nn.BatchNorm1d(output_dim),
            nn.Dropout(0.2)
        )

    def forward(self, x):
        x = self.block(x)
        return x
```

æ³¨æ„åˆ°æˆ‘ä»¬è¿æ¥äº†19å¸§ï¼Œæœ¬èº«ç‰¹å¾å°±è¾¾åˆ°äº†39\*19ï¼Œå› æ­¤å¢åŠ `hidden_dim`ã€‚

```python
# data prarameters
concat_nframes = 19             # the number of frames to concat with, n must be odd (total 2k+1 = n frames)
train_ratio = 0.8               # the ratio of data used for training, the rest will be used for validation

# training parameters
seed = 0                        # random seed
batch_size = 512                # batch size
num_epoch = 50                   # the number of training epoch
learning_rate = 0.0001          # learning rate
model_path = './model.ckpt'     # the path where the checkpoint will be saved

# model parameters
input_dim = 39 * concat_nframes # the input dim of the model, you should not change the value
hidden_layers = 3               # the number of hidden layers
hidden_dim = 1024                # the hidden dim
```

æ­¤æ—¶ç»è¿‡30ä¸ªepochçš„è®­ç»ƒåå°±æ²¡ä»€ä¹ˆè¿›æ­¥äº†ï¼Œç”šè‡³å¥½åƒè¿‡æ‹Ÿåˆäº†â€¦â€¦æ‰€ä»¥è®­ç»ƒåˆ°60epochçš„æ—¶å€™æˆ‘å°±èµ¶ç´§ææ‰äº†ã€‚

```log
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4134/4134 [00:37<00:00, 110.83it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1031/1031 [00:05<00:00, 184.43it/s]
[028/080] Train Acc: 0.806332 Loss: 0.572783 | Val Acc: 0.749010 loss: 0.830200
saving model with acc 0.749
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4134/4134 [00:37<00:00, 111.51it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1031/1031 [00:05<00:00, 195.24it/s]
[029/080] Train Acc: 0.807738 Loss: 0.568986 | Val Acc: 0.749124 loss: 0.831292
saving model with acc 0.749
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4134/4134 [00:37<00:00, 110.04it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1031/1031 [00:05<00:00, 193.80it/s]
[030/080] Train Acc: 0.808039 Loss: 0.567000 | Val Acc: 0.749021 loss: 0.831178
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4134/4134 [00:36<00:00, 111.94it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1031/1031 [00:05<00:00, 186.39it/s]
[031/080] Train Acc: 0.785248 Loss: 0.640657 | Val Acc: 0.741230 loss: 0.842659
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4134/4134 [00:37<00:00, 111.52it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1031/1031 [00:05<00:00, 190.29it/s]
[032/080] Train Acc: 0.784388 Loss: 0.642653 | Val Acc: 0.741867 loss: 0.843050
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4134/4134 [00:36<00:00, 111.89it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1031/1031 [00:05<00:00, 189.24it/s]
...
...
[055/080] Train Acc: 0.829193 Loss: 0.495767 | Val Acc: 0.747598 loss: 0.870931
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4134/4134 [00:37<00:00, 111.11it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1031/1031 [00:05<00:00, 196.11it/s]
[056/080] Train Acc: 0.831027 Loss: 0.490183 | Val Acc: 0.748087 loss: 0.870781
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4134/4134 [00:37<00:00, 111.36it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1031/1031 [00:05<00:00, 175.55it/s]
[057/080] Train Acc: 0.832389 Loss: 0.485255 | Val Acc: 0.747782 loss: 0.871408
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4134/4134 [00:37<00:00, 110.37it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1031/1031 [00:05<00:00, 201.41it/s]
[058/080] Train Acc: 0.833472 Loss: 0.481230 | Val Acc: 0.748790 loss: 0.869592
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4134/4134 [00:37<00:00, 111.45it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1031/1031 [00:05<00:00, 177.09it/s]
```

![](./attachments/Lecture2-1770541856202.png)

### Boss Baseline

ä»¥ä¸Šæˆ‘ä»¬ç”¨çš„éƒ½æ˜¯å¾ˆç®€å•çš„è¿æ¥ç¥ç»ç½‘ç»œï¼ˆå°±æ˜¯ä¸€å †çŸ©é˜µä¹˜æ³•ç½¢äº†ï¼‰ç„¶ååœ¨strong baselineé‡Œé¢åŠ å…¥äº†batchnormå’Œdropoutã€‚åƒè¿™ç§æ—¶åºåˆ†ç±»é—®é¢˜å•Šï¼Œæœ‰ç‚¹åƒæˆ‘è®ºæ–‡è®¨è®ºçš„é‚£ä¸ªé—®é¢˜ã€‚å› æ­¤æ—¶åºåˆ†ç±»ä»»åŠ¡æˆ‘è¿˜æ˜¯äº†è§£è¿‡å‡ ç§æ·±åº¦å­¦ä¹ ç®—æ³•çš„ï¼Œä¾‹å¦‚LSTMå’Œè¿™é‡Œæç¤ºçš„RNNã€‚ä¸ºäº†è¾¾åˆ°boss baselineï¼Œéœ€è¦æœè¿™ä¸ªæ–¹å‘åšæ”¹è¿›ã€‚

æˆ‘é¦–å…ˆå°è¯•äº†RNNï¼Œ10ä¸ªepochä»¥å†…å°±ä¸å†å¢é•¿äº†ï¼Œæ”¶æ•›å¾ˆå¿«ä½†æ˜¯æ•ˆæœæ²¡æœ‰åˆšæ‰å¥½ã€‚

![](./attachments/Lecture2-1770698853460.png)

è¿™äº›RNNï¼ŒLSTMéƒ½æ˜¯ç”¨æœ€åä¸€æ­¥çš„è¾“å‡ºï¼Œä½†æ˜¯æˆ‘ä»¬çš„é¢„æµ‹å¸§åœ¨ä¸­é—´ï¼Œæˆ‘ä»¥ä¸ºè¿™æ ·ä¸å¤ªå¥½ï¼Œå°±å»æ”¹ç½‘ç»œã€æŸå¤±çš„ç®—æ³•ï¼Œç»“æœæ•ˆæœä¸å ªå…¥ç›®ï¼Œæ”¹æˆä¸‰ç»´æ•°æ®è¿˜æŠ˜è…¾äº†ä¸€å¤©ã€‚åæ¥æ‰¾åˆ°äº†åˆ«äººçš„å®è·µï¼Œæ‰æ”¹å›æ¥äº†ï¼Œå¯ä»¥å¤§èƒ†å¢åŠ LSTMçš„å±‚æ•°ï¼ŒLSTMçš„è¾“å‡ºå¯ä»¥å†æ”¾åˆ°å…¨è¿æ¥å±‚é‡Œé¢â€¦â€¦

`concat_nframes = 23`ï¼Œ
å†å¤§12.7GBç³»ç»ŸRAMå°±è£…ä¸ä¸‹äº†â€¦â€¦

```log
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4381/4381 [01:54<00:00, 38.25it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 785/785 [00:06<00:00, 120.76it/s]
[001/015] Train Acc: 0.655396 Loss: 1.119341 | Val Acc: 0.674556 loss: 1.045279
saving model with acc 0.675
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4381/4381 [01:59<00:00, 36.80it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 785/785 [00:07<00:00, 111.12it/s]
[002/015] Train Acc: 0.735927 Loss: 0.832867 | Val Acc: 0.690507 loss: 1.017421
saving model with acc 0.691
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4381/4381 [01:59<00:00, 36.76it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 785/785 [00:06<00:00, 112.56it/s]
[003/015] Train Acc: 0.782831 Loss: 0.673428 | Val Acc: 0.691520 loss: 1.049682
saving model with acc 0.692
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4381/4381 [01:59<00:00, 36.72it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 785/785 [00:06<00:00, 118.46it/s]
[004/015] Train Acc: 0.825613 Loss: 0.531372 | Val Acc: 0.689239 loss: 1.141714
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4381/4381 [01:59<00:00, 36.76it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 785/785 [00:07<00:00, 111.97it/s]
[005/015] Train Acc: 0.867342 Loss: 0.397408 | Val Acc: 0.686937 loss: 1.269593
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4381/4381 [01:58<00:00, 36.82it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 785/785 [00:07<00:00, 111.64it/s]
[006/015] Train Acc: 0.906736 Loss: 0.273127 | Val Acc: 0.686579 loss: 1.448474
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4381/4381 [01:59<00:00, 36.80it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 785/785 [00:06<00:00, 119.00it/s]
[007/015] Train Acc: 0.942374 Loss: 0.165881 | Val Acc: 0.684778 loss: 1.703792
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4381/4381 [01:59<00:00, 36.80it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 785/785 [00:06<00:00, 117.48it/s]
[008/015] Train Acc: 0.970304 Loss: 0.084912 | Val Acc: 0.685219 loss: 2.109468
```

é‡‡ç”¨åŒå‘LSTM+MLPå·²ç»æ˜¯ä¸¥é‡è¿‡æ‹Ÿåˆäº†ã€‚çœ‹äº†ä¸€ä¸‹åˆ«äººçš„å®è·µï¼Œconcat_frameè°ƒåˆ°61ï¼Œè¿™å·²ç»ä¸æ˜¯å…è´¹ç‰ˆColabèƒ½æ‰¿å—çš„ï¼Œé‚ä½œç½¢ã€‚è€Œä¸”LSTMçš„è®­ç»ƒæ¯”RNNæ›´è€—æ—¶ï¼Œæ¯å¤©çš„å…è´¹é¢åº¦å¯èƒ½åªèƒ½è·‘ä¸¤æ¬¡å®éªŒï¼Œéå¸¸æŠ˜ç£¨ï¼Œæ•ˆæœä¹Ÿä¸Šä¸å»ã€‚

æ¨¡å‹æ¶æ„å¤§è‡´å¦‚ä¸‹ï¼š

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class BasicBlock(nn.Module):
    def __init__(self, input_dim, output_dim):
        super(BasicBlock, self).__init__()

        self.block = nn.Sequential(
            nn.Linear(input_dim, output_dim),
            nn.BatchNorm1d(output_dim),
            nn.ReLU(),
            nn.Dropout(0.2),
        )

    def forward(self, x):
        x = self.block(x)
        return x

class SimpleRNN(nn.Module):
    def __init__(self, feature_dim=39, hidden_dim=512, hidden_layers=3) -> None:
        super(SimpleRNN, self).__init__()
        self.feature_dim = feature_dim
        self.rnn = nn.RNN(feature_dim, hidden_dim, num_layers=hidden_layers, 
                          batch_first=True, dropout=0.25)

    def forward(self, x):
        # x: (batch_size, concat_n * feature_dim)
        x = x.view(x.size(0), -1, self.feature_dim)  # (batch_size, concat_nframes, feature_dim)
        out, _ = self.rnn(x)  # out: (batch_size, seq_len, hidden_size)
        # å–åºåˆ—æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„è¾“å‡ºä½œä¸ºæ¨¡å‹çš„è¾“å‡º
        out = out[:, -1, :]   # (batch_size, hidden_size)
        return out
    
class SimpleLSTM(nn.Module):
    def __init__(self, feature_dim=39, hidden_dim=512, hidden_layers=4) -> None:
        super(SimpleLSTM, self).__init__()
        self.feature_dim = feature_dim
        self.rnn = nn.LSTM(feature_dim, hidden_dim, num_layers=hidden_layers,
                           batch_first=True, dropout=0.25)

    def forward(self, x):
        # x: (batch_size, concat_nframes * feature_dim)
        x = x.view(x.size(0), -1, self.feature_dim)  # (batch_size, concat_nframes, feature_dim)
        out, _ = self.rnn(x)  # out: (batch_size, seq_len, hidden_dim)
        # å–åºåˆ—æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„è¾“å‡ºä½œä¸ºæ¨¡å‹çš„è¾“å‡º
        out = out[:, -1, :]   # (batch_size, hidden_size)
        return out

class Classifier(nn.Module):
    def __init__(self, input_dim, output_dim=41, hidden_layers=1, hidden_dim=256):
        super(Classifier, self).__init__()
        self.rnn_hidden_dim = 768
        self.rnn = SimpleRNN(
            feature_dim=input_dim, hidden_dim=self.rnn_hidden_dim
        )
        self.lstm = SimpleLSTM(
            feature_dim=input_dim, hidden_dim=self.rnn_hidden_dim
        )
        # åæ¥å…¨è¿æ¥å±‚
        self.fc = nn.Sequential(
            # ä¿®æ”¹æˆ 2 * self.rnn_hidden_size çš„åŸå› æ˜¯å› ä¸ºLSTM()ä¸­çš„bidirectionalè®¾ç½®ä¸ºäº†Trueï¼Œè¿™è¡¨ç¤ºä½¿ç”¨Biï¼ˆåŒå‘ï¼‰LSTMæ¨¡å‹ï¼Œæ‰€ä»¥éœ€è¦ä¿®æ”¹è¾“å…¥ç»´åº¦ä»¥åŒ¹é…
            BasicBlock(self.rnn_hidden_dim, hidden_dim),
            *[BasicBlock(hidden_dim, hidden_dim) for _ in range(hidden_layers)],
            nn.Linear(hidden_dim, output_dim)
        )

    def forward(self, x):
        # x: (batch_size, seq_len, rnn_input_dim=feature_dim)
        # x = self.lstm(x)  # (batch_size, seq_len, rnn_hidden_dim)
        x = self.rnn(x)  # (batch_size, seq_len, rnn_hidden_dim)

        # é€šè¿‡å…¨è¿æ¥å±‚å¾—åˆ° logits
        logits = self.fc(x)  # (batch_size, num_classes)
        return logits
```

### å‚è€ƒèµ„æ–™

[ã€æå®æ¯…æœºå™¨å­¦ä¹ HW2ã€‘-CSDNåšå®¢](https://blog.csdn.net/detemination_/article/details/127194301)

[Boss Baselineæ€è·¯è§£æ](https://github.com/Aaricis/Hung-yi-Lee-ML2022/blob/main/HW2/HW02_boss.ipynb)

### å°ç»“

æœ¬æ¬¡ä½œä¸šæ•°æ®é›†è¾ƒå¤§ï¼ŒRAMå†…å­˜æœ‰ç‚¹ä¸å¤ªå¤Ÿç”¨ã€‚è¿è¡Œè®­ç»ƒå’Œæµ‹è¯•ä»£ç ä¹‹åå³ä½¿è°ƒç”¨äº† `gc.collect()` æ²¡æœ‰ä½œç”¨ï¼Œå†…å­˜å ç”¨ä¾ç„¶æ²¡æœ‰ä¸‹é™ï¼Œç„¶åcolabå°±å˜çš„å¾ˆå¡ï¼Œä¸‹è½½ `pred.csv` éƒ½ä¸ç¨³å®šï¼Œä½“éªŒä¸ä½³ã€‚åŒæ—¶ï¼Œè®­ç»ƒæ—¶é—´ä¹Ÿæ¯”ä¸Šæ¬¡ä½œä¸šå¤šäº†å¾ˆå¤šã€‚è¿™ä¸ªè¿æ¥19å¸§çš„è¶…å‚æ•°æˆ‘æ˜¯ç›´æ¥çœ‹åˆ«äººçš„å®è·µï¼Œåœ¨å…è´¹é¢åº¦çš„æ¡ä»¶ä¸‹ä¸€å¤©å†…æ— æ³•å¤šè¯•å‡ ä¸ªè¶…å‚æ•°ã€‚ä½†æ˜¯ç›¸æ¯”äºHW01è¿™æ¬¡çš„æ–¹å‘å€’æ˜¯å¾ˆæ˜ç¡®ï¼Œä¸åƒHW01æ²¡æœ‰ç¡®åˆ‡çš„æ”¹è¿›æ–¹å‘ï¼Œä¸€ç›´åœ¨è°ƒè¶…å‚æ•°ã€‚è€Œä¸”HW02ä¸€æ”¹è¿›å°±èƒ½çœ‹åˆ°æˆæ•ˆï¼Œå…¶å®æœ€è®©äººå®¹æ˜“å´©æºƒçš„æ˜¯HW01ç»å¸¸æ”¹äº†å‚æ•°æ•ˆæœä¸å‡åé™ã€‚

> æˆ‘çš„notebook: [HW02.ipynb](https://colab.research.google.com/drive/1UO6pYlGJ1Rs5iKpfd6NdOezxmsTWdbZS)
