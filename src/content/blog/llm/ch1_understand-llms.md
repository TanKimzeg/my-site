---
title: 理解大预言模型 | LLM
description: 从零构建大语言模型第1章:本章介绍了Transformer架构和GPT-like LLM
pubDate: 2025 09 20
categories:
  - tech
tags:
  - llm
---

## 介绍Transformer架构

![](https://skindhu.github.io/Build-A-Large-Language-Model-CN/Image/chapter1/figure1.4.png)

> 为什么编码器的Embeddings能作为解码器的输入来翻译?解码器不是用来预测下一个词的吗?

>我在学习编译原理的时候想过用用于机器翻译任务,这是DeepSeek的回答:

---

这是一个非常棒的问题，它触及了自然语言处理（NLP）发展的核心脉络。你的直觉很对，早期的机器翻译（MT）和编译原理的前端（词法分析、语法分析）确实共享了非常相似的思想——**基于规则和符号的精确处理**。

然而，我们之所以没有用这种“机翻思想”来制作现代大语言模型（LLM），是因为这两种范式在哲学和技术上存在着根本性的差异。简单来说，**编译原理是“确定性”的，而自然语言是“概率性”的**。

下面我们从几个关键维度进行详细对比，来解释为什么后者不适用：

---

### 1. 核心哲学：规则 vs. 统计

*   **编译原理前端（传统机翻思想）**：
    *   **方法**：基于一套**精确、预定义、手工编写**的规则。
    *   **过程**：输入源代码 -> **词法分析器（Lexer）** 根据正则规则切分成Token -> **语法分析器（Parser）** 根据上下文无关文法（CFG）等规则构建**确定的**语法树（AST）-> 进行语义分析。
    *   **特点**：这是一个**确定性的过程**。同一段代码，每次解析得到的AST都完全一样。规则是二元对错的，没有“好像可能大概”这种模糊概念。

*   **大语言模型（LLM）**：
    *   **方法**：基于从海量数据中学习到的**统计概率**。
    *   **过程**：模型通过学习万亿级别的词汇共现关系，形成一个**高维概率模型**。当它收到一个提示（Prompt）时，它不是在“解析”句子，而是在计算“**在给定上文的情况下，下一个最可能出现的词或字是什么**”，并依此逐个生成（自回归）。
    *   **特点**：这是一个**概率性的过程**。同一个提示，每次生成的结果都可能略有不同。它的输出是一个概率分布，选择哪个词存在随机性。

### 2. 处理歧义的能力

这是两种范式最根本的区别。

*   **编译原理**：要求语言**无歧义**。设计编程语言的首要目标就是消除歧义，确保每个合法程序只有一种解析方式。如果代码有歧义（语法错误或设计缺陷），编译就会失败。
*   **自然语言**：**充满歧义**。这是核心特征。
    *   **词法歧义**：“苹果”指的是水果还是公司？
    *   **语法歧义**：“咬死了猎人的狗” 是狗被咬死了，还是狗咬死了猎人？
    *   **语义歧义**：“她的头发很长” 是字面意思，还是比喻她见识短？

    **基于规则的系统** 处理歧义需要编写极其复杂且会相互冲突的规则，覆盖 corner case 的成本极高，最终会陷入“规则爆炸”的困境。
    **基于统计的LLM** 通过上下文来解决歧义。它通过计算“咬死了”、“猎人的”、“狗”这些词在一起出现的概率，来判断最可能的语义关系。它接受模糊性，并给出一个“最可能”的答案。

### 3.  robustness（鲁棒性）与容错性

*   **编译原理**：非常**脆弱**。哪怕只缺少一个分号 `;`，或者错了一个括号，整个解析过程就会立即失败，并抛出一个语法错误。
*   **大语言模型**：非常**鲁棒**。即使你的输入有错别字、语法不通顺、中英文混杂，LLM 依然能**大概率**理解你的意图并给出合理的回答。因为它见过海量不规范的网络文本，它学习的是语义的“模糊映射”，而不是严格的“符号逻辑”。

### 4. 创造性与泛化能力

*   **编译原理**：**没有创造性**。它只能处理符合预定义规则的情况。无法生成全新的、超出规则范围的代码或表达。它的能力上限在规则编写完成时就已经被锁死了。
*   **大语言模型**：**具有涌现的创造性**。它可以写诗、编故事、进行哲学思辨、解决从未见过的新问题（In-context Learning）。这种能力来自于它从数据中学习到的极其复杂的模式，而不是显式的规则。它可以生成训练数据中不存在但又合情合理的表达。

### 历史的视角：这其实正是NLP走过的路

你的问题其实完美地描述了自然语言处理领域的历史演变：

1.  **1950s-1980s：规则主义时代**
    *   人们最初的想法和你完全一样，试图用编写语法规则（类似编译原理）的方法让机器理解和翻译语言。这被称为**基于规则的机器翻译（RBMT）**。
    *   **结果**：失败告终。系统非常脆弱，无法处理复杂多变的真实语言，开发和维护成本巨大。

2.  **1990s-2010s：统计时代**
    *   人们转向了数据驱动的统计方法，例如**基于统计的机器翻译（SBTM）**。它不再依赖语言学家编写规则，而是通过分析大量双语语料库，学习翻译的概率模型。
    *   **结果**：取得了巨大成功（比如早期的谷歌翻译），但模型仍然较为简单，需要复杂的特征工程。

3.  **2010s-至今：神经网络与LLM时代**
    *   **深度学习**和**Transformer架构**的出现，使得我们可以构建极其庞大的神经网络（LLM），它们本质上是最极致的统计模型，能够捕获语言中更微妙、更复杂的概率关系。
    *   **结果**：带来了革命性的突破，模型的能力实现了质的飞跃。

---

### 总结：为什么不能用？

| 特性 | 编译原理前端（传统机翻思想） | 大语言模型（LLM） | 结论 |
| :--- | :--- | :--- | :--- |
| **哲学** | 基于**规则**与**符号**的确定性推理 | 基于**数据**与**统计**的概率性生成 | 根本范式不同 |
| **歧义处理** | 要求**无歧义**，否则失败 | 接受歧义，给出**最可能**解释 | LLM能处理自然语言的核心特征 |
| **鲁棒性** | **脆弱**，严格容错 | **强壮**，容忍错误和不规范输入 | LLM适用于真实世界的不完美语言 |
| **创造性** | **无**，能力受规则限制 | **有**，可涌现出新能力 | LLM能完成开放域任务 |
| **开发成本** | 依赖专家**手工编写**规则，成本高 | 依赖**算力和数据**，规则自动学习 | LLM的扩展性远超人工系统 |

因此，不是“不能”用，而是历史已经证明，那种方法在处理开放域、充满歧义的自然语言时是一条**效率低下且能力有限**的路径。大语言模型的**概率生成范式**是远比**规则符号范式**更适合处理自然语言的技术路径。

最终，编译原理教会了我们如何让机器**精确地服从**，而大语言模型则是在尝试让机器**模糊地理解**。两者适用于完全不同的领域。

---

Transformer的编码器演化成了BERT,而解码器演化成了GPT:

![](https://skindhu.github.io/Build-A-Large-Language-Model-CN/Image/chapter1/figure1.5.png)

## 利用大型数据集
GPT-3 的预训练费用估计为 460 万美元,使用了几千亿token的数据集来训练.所以要训练出"有智慧"的GPT是需要耗费大量资源的,不只是算法的挤牙膏.

有许多经过预训练的开源LLM，可以作为通用工具来撰写、提取和编辑不在训练数据中的文本。此外，LLM可以在相对较小的数据集上进行微调，以减少所需的计算资源并提高特定任务的性能。

像 GPT-3 这样的模型架构明显大于原始的 Transformer 模型。例如，原始的 Transformer 将编码器和解码器块重复了六次，而 GPT-3 具有 96 层 Transformer，总共有 1750 亿个参数。

![](https://skindhu.github.io/Build-A-Large-Language-Model-CN/Image/chapter1/figure1.8.png)

模型能够执行未明确训练的任务被称为“涌现行为”。这种能力不是通过明确的训练获得的，而是模型接触大量多语言数据和多样化上下文后自然而然涌现的结果。这展示了这些大语言模型的优势和能力.

## 构建大语言模型
在本书,我们将从零开始编码一个GPT-like LLM.

构建LLM需要的步骤:
![](https://skindhu.github.io/Build-A-Large-Language-Model-CN/Image/chapter1/figure1.9.png)

首先，我们将学习基本的数据预处理步骤，并编写 LLM 核心的注意力机制代码。

接下来，在第二阶段，我们将学习如何编写代码并预训练一个类似 GPT 的 LLM，能够生成新的文本。同时，我们还会介绍评估 LLM 的基本原理，这对开发强大的 NLP（自然语言处理）系统至关重要。

请注意，从头开始预训练一个 LLM 是一项庞大的工程，对于类似 GPT 的模型，计算成本可能高达数千到数百万美元。因此，第二阶段的重点是进行教学目的的训练，使用小型数据集。此外，本书还将提供关于如何加载公开可用的模型权重的代码示例。

最后，在第三阶段，我们将使用一个预训练好的 LLM，对其进行微调，使其能够执行指令，例如回答查询或进行文本分类——这些是在许多现实世界应用和研究中最常见的任务。

## **本章摘要**
- LLM 已经彻底改变了自然语言处理的领域，之前自然语言处理主要依赖于显式的规则系统和较为简单的统计方法。LLM 的出现引入了新的深度学习驱动的方法，推动了对人类语言的理解、生成和翻译的进步。
- 现代 LLM 的训练通常分为两个主要步骤：
    - 首先，它们在一个大型未标注的文本语料库中进行预训练，通过使用句子中下一个单词的预测作为“标签”。
    - 这些模型接下来会在一个较小的、有标签的目标数据集上进行微调，以遵循指令或执行分类任务。
- LLM 基于Transformer架构。Transformer架构的核心理念是注意力机制，这使得 LLM 在逐字生成输出时，能够选择性地访问整个输入序列。
- 原始的Transformer架构由一个用于解析文本的编码器和一个用于生成文本的解码器组成。
- 生成文本和执行指令的 LLM，例如 GPT-3 和 ChatGPT，仅实现解码器模块，这使得架构更加简化。
- 由数十亿个单词构成的大型数据集对预训练 LLM 至关重要。在本书中，我们将实现并在小型数据集上训练 LLM，以便用于教学，同时也会探讨如何加载公开可用的模型权重。
- 类似 GPT 的模型的普遍预训练任务是预测句子中的下一个单词，但这些 LLM 显示出了“涌现”特性，例如具备分类、翻译或文本总结的能力。
- 一旦 LLM 完成预训练，得到的基础模型就可以更高效地微调，以应对各种下游任务。
- 在自定义数据集上微调过的 LLM 能够在特定任务上超越通用 LLM。
